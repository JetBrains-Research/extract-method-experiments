{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, LSTM, Flatten, Dense, BatchNormalization\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Input and Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function takes in preprocessed data (classes balanced, duplicates removed, NOT normalized)\n",
    "The standard scaler has to be exported so it is important to normalize the data here\n",
    "\n",
    "Change the paths in trainFull and TestFull to desired preprocessedData\n",
    "\n",
    "\"\"\"\n",
    "def getPreprocessedData():\n",
    "    \n",
    "    #Concatenate positive and negative samples\n",
    "    trainFull = pd.read_csv(\"PreprocessedData/trainFull.csv\", index_col = 0)\n",
    "    testFull = pd.read_csv(\"PreprocessedData/testFull.csv\", index_col = 0)\n",
    "    \n",
    "    #Randomize samples\n",
    "    trainFull = trainFull.sample(len(trainFull))\n",
    "    testFull = testFull.sample(len(testFull))\n",
    "\n",
    "    #Randomize samples\n",
    "    trainFull = trainFull.sample(len(trainFull))\n",
    "    testFull = testFull.sample(len(testFull))\n",
    "\n",
    "    #Reduce features and extract labels\n",
    "    trainX = trainFull.iloc[:,:-1]\n",
    "    trainY = trainFull.iloc[:,-1]\n",
    "    testX = testFull.iloc[:,:-1]\n",
    "    testY = testFull.iloc[:,-1]\n",
    "\n",
    "    #Reshape data to 3D for CNN\n",
    "    trainX = trainX.to_numpy()[..., None]\n",
    "    trainY = trainY.to_numpy()[..., None]\n",
    "    testX = testX.to_numpy()[..., None]\n",
    "    testY = testY.to_numpy()[..., None]\n",
    "\n",
    "    return trainX, trainY, testX, testY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This method contains the architecture for the CNN used.\n",
    "It uses tensorflow sequential as the basis to crease the model.\n",
    "\n",
    "@param numConvFilters: number of convolutional filters in the 2nd hidden layer. We recommend >32 to upscale.\n",
    "@param dropout: fraction of nodes dropped out of the Max Pooling layer.\n",
    "@param numDenseNodes: number of dense nodes in the feed forward layer.\n",
    "\n",
    "Recommended:\n",
    "numConvFilters = 242\n",
    "dropout = .215\n",
    "numDenseNodes = 190\n",
    "\n",
    "\"\"\"\n",
    "def newModel(numConvFilters = 242, dropout = .215, numDenseNodes = 190):\n",
    "    #Create new sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    #Add 1D Convolutional layers\n",
    "    model.add(BatchNormalization())\n",
    "    model.add((Conv1D(filters=32, kernel_size=3, activation='relu')))\n",
    "    model.add((Conv1D(filters=numConvFilters, kernel_size=3, activation='relu')))\n",
    "    #Add Max Pooling layer\n",
    "    model.add((MaxPooling1D(pool_size=2)))\n",
    "    #Apply dropout\n",
    "    model.add(Dropout(dropout))\n",
    "    #Flatten model\n",
    "    model.add(Flatten())\n",
    "    #Add fully connected dense layer\n",
    "    model.add(Dense(numDenseNodes, activation='sigmoid'))\n",
    "    #Add output node\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    #Define optimizer\n",
    "    adam = tf.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    #Compile model\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC(curve='PR'),                                                                                         tf.keras.metrics.PrecisionAtRecall(0.8)])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-26 10:36:02.137617: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-26 10:36:02.971068: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "758/758 [==============================] - 23s 27ms/step - loss: 0.6809 - auc: 0.5780 - precision_at_recall: 0.5346\n",
      "Epoch 2/100\n",
      "758/758 [==============================] - 17s 22ms/step - loss: 0.6571 - auc: 0.6318 - precision_at_recall: 0.5684\n",
      "Epoch 3/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.6458 - auc: 0.6524 - precision_at_recall: 0.5809\n",
      "Epoch 4/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.6405 - auc: 0.6598 - precision_at_recall: 0.5881\n",
      "Epoch 5/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.6346 - auc: 0.6682 - precision_at_recall: 0.5938\n",
      "Epoch 6/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.6333 - auc: 0.6679 - precision_at_recall: 0.5960\n",
      "Epoch 7/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.6292 - auc: 0.6755 - precision_at_recall: 0.6010\n",
      "Epoch 8/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.6229 - auc: 0.6878 - precision_at_recall: 0.6059\n",
      "Epoch 9/100\n",
      "758/758 [==============================] - 18s 23ms/step - loss: 0.6150 - auc: 0.7006 - precision_at_recall: 0.6166\n",
      "Epoch 10/100\n",
      "758/758 [==============================] - 17s 22ms/step - loss: 0.6107 - auc: 0.7006 - precision_at_recall: 0.6126\n",
      "Epoch 11/100\n",
      "758/758 [==============================] - 17s 23ms/step - loss: 0.6039 - auc: 0.7122 - precision_at_recall: 0.6250\n",
      "Epoch 12/100\n",
      "758/758 [==============================] - 17s 22ms/step - loss: 0.6002 - auc: 0.7137 - precision_at_recall: 0.6303\n",
      "Epoch 13/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.5950 - auc: 0.7218 - precision_at_recall: 0.6337\n",
      "Epoch 14/100\n",
      "758/758 [==============================] - 16s 22ms/step - loss: 0.5936 - auc: 0.7266 - precision_at_recall: 0.6361\n",
      "Epoch 15/100\n",
      "758/758 [==============================] - 17s 22ms/step - loss: 0.5894 - auc: 0.7270 - precision_at_recall: 0.6430\n",
      "Epoch 16/100\n",
      "758/758 [==============================] - 17s 23ms/step - loss: 0.5869 - auc: 0.7330 - precision_at_recall: 0.6382\n",
      "Epoch 17/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.5838 - auc: 0.7338 - precision_at_recall: 0.6512\n",
      "Epoch 18/100\n",
      "758/758 [==============================] - 17s 22ms/step - loss: 0.5815 - auc: 0.7397 - precision_at_recall: 0.6491\n",
      "Epoch 19/100\n",
      "758/758 [==============================] - 15s 20ms/step - loss: 0.5795 - auc: 0.7437 - precision_at_recall: 0.6445\n",
      "Epoch 20/100\n",
      "758/758 [==============================] - 17s 23ms/step - loss: 0.5776 - auc: 0.7447 - precision_at_recall: 0.6530\n",
      "Epoch 21/100\n",
      "758/758 [==============================] - 24s 32ms/step - loss: 0.5694 - auc: 0.7520 - precision_at_recall: 0.6565\n",
      "Epoch 22/100\n",
      "758/758 [==============================] - 20s 27ms/step - loss: 0.5715 - auc: 0.7537 - precision_at_recall: 0.6600\n",
      "Epoch 23/100\n",
      "758/758 [==============================] - 19s 25ms/step - loss: 0.5672 - auc: 0.7564 - precision_at_recall: 0.6634\n",
      "Epoch 24/100\n",
      "758/758 [==============================] - 19s 25ms/step - loss: 0.5611 - auc: 0.7636 - precision_at_recall: 0.6638\n",
      "Epoch 25/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.5598 - auc: 0.7700 - precision_at_recall: 0.6648\n",
      "Epoch 26/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.5560 - auc: 0.7741 - precision_at_recall: 0.6701\n",
      "Epoch 27/100\n",
      "758/758 [==============================] - 15s 20ms/step - loss: 0.5503 - auc: 0.7828 - precision_at_recall: 0.6748\n",
      "Epoch 28/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.5486 - auc: 0.7817 - precision_at_recall: 0.6730\n",
      "Epoch 29/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.5422 - auc: 0.7903 - precision_at_recall: 0.6815\n",
      "Epoch 30/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.5390 - auc: 0.7967 - precision_at_recall: 0.6822\n",
      "Epoch 31/100\n",
      "758/758 [==============================] - 15s 20ms/step - loss: 0.5361 - auc: 0.7987 - precision_at_recall: 0.6798\n",
      "Epoch 32/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.5280 - auc: 0.8074 - precision_at_recall: 0.6954\n",
      "Epoch 33/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.5264 - auc: 0.8103 - precision_at_recall: 0.6972\n",
      "Epoch 34/100\n",
      "758/758 [==============================] - 17s 22ms/step - loss: 0.5206 - auc: 0.8161 - precision_at_recall: 0.6944\n",
      "Epoch 35/100\n",
      "758/758 [==============================] - 36s 47ms/step - loss: 0.5125 - auc: 0.8230 - precision_at_recall: 0.7081\n",
      "Epoch 36/100\n",
      "758/758 [==============================] - 25s 33ms/step - loss: 0.5167 - auc: 0.8180 - precision_at_recall: 0.7034\n",
      "Epoch 37/100\n",
      "758/758 [==============================] - 21s 27ms/step - loss: 0.5083 - auc: 0.8274 - precision_at_recall: 0.7075\n",
      "Epoch 38/100\n",
      "758/758 [==============================] - 19s 25ms/step - loss: 0.5061 - auc: 0.8276 - precision_at_recall: 0.7106\n",
      "Epoch 39/100\n",
      "758/758 [==============================] - 18s 24ms/step - loss: 0.4963 - auc: 0.8347 - precision_at_recall: 0.7226\n",
      "Epoch 40/100\n",
      "758/758 [==============================] - 17s 23ms/step - loss: 0.4979 - auc: 0.8349 - precision_at_recall: 0.7236\n",
      "Epoch 41/100\n",
      "758/758 [==============================] - 16s 22ms/step - loss: 0.4905 - auc: 0.8403 - precision_at_recall: 0.7272\n",
      "Epoch 42/100\n",
      "758/758 [==============================] - 19s 25ms/step - loss: 0.4847 - auc: 0.8443 - precision_at_recall: 0.7293\n",
      "Epoch 43/100\n",
      "758/758 [==============================] - 17s 22ms/step - loss: 0.4789 - auc: 0.8477 - precision_at_recall: 0.7359 0s - loss: 0.4778 - auc: 0.8480 - precision_at_rec\n",
      "Epoch 44/100\n",
      "758/758 [==============================] - 19s 25ms/step - loss: 0.4757 - auc: 0.8504 - precision_at_recall: 0.7393\n",
      "Epoch 45/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.4687 - auc: 0.8552 - precision_at_recall: 0.7450\n",
      "Epoch 46/100\n",
      "758/758 [==============================] - 18s 24ms/step - loss: 0.4679 - auc: 0.8581 - precision_at_recall: 0.7486\n",
      "Epoch 47/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.4644 - auc: 0.8590 - precision_at_recall: 0.7449\n",
      "Epoch 48/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.4627 - auc: 0.8610 - precision_at_recall: 0.7517\n",
      "Epoch 49/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.4562 - auc: 0.8659 - precision_at_recall: 0.7536\n",
      "Epoch 50/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.4554 - auc: 0.8657 - precision_at_recall: 0.7574\n",
      "Epoch 51/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.4490 - auc: 0.8690 - precision_at_recall: 0.7635\n",
      "Epoch 52/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.4445 - auc: 0.8735 - precision_at_recall: 0.7648\n",
      "Epoch 53/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.4422 - auc: 0.8734 - precision_at_recall: 0.7735\n",
      "Epoch 54/100\n",
      "758/758 [==============================] - 20s 26ms/step - loss: 0.4381 - auc: 0.8754 - precision_at_recall: 0.7764\n",
      "Epoch 55/100\n",
      "758/758 [==============================] - 18s 24ms/step - loss: 0.4352 - auc: 0.8784 - precision_at_recall: 0.7713\n",
      "Epoch 56/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.4266 - auc: 0.8827 - precision_at_recall: 0.7800\n",
      "Epoch 57/100\n",
      "758/758 [==============================] - 16s 22ms/step - loss: 0.4267 - auc: 0.8815 - precision_at_recall: 0.7812\n",
      "Epoch 58/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.4230 - auc: 0.8855 - precision_at_recall: 0.7883\n",
      "Epoch 59/100\n",
      "758/758 [==============================] - 17s 22ms/step - loss: 0.4200 - auc: 0.8859 - precision_at_recall: 0.7877\n",
      "Epoch 60/100\n",
      "758/758 [==============================] - 17s 22ms/step - loss: 0.4194 - auc: 0.8864 - precision_at_recall: 0.7899\n",
      "Epoch 61/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.4150 - auc: 0.8895 - precision_at_recall: 0.7898\n",
      "Epoch 62/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.4133 - auc: 0.8876 - precision_at_recall: 0.7890\n",
      "Epoch 63/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.4126 - auc: 0.8909 - precision_at_recall: 0.7886\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "758/758 [==============================] - 16s 20ms/step - loss: 0.4093 - auc: 0.8925 - precision_at_recall: 0.7967\n",
      "Epoch 65/100\n",
      "758/758 [==============================] - 15s 20ms/step - loss: 0.4070 - auc: 0.8945 - precision_at_recall: 0.8003\n",
      "Epoch 66/100\n",
      "758/758 [==============================] - 15s 20ms/step - loss: 0.4023 - auc: 0.8965 - precision_at_recall: 0.8007\n",
      "Epoch 67/100\n",
      "758/758 [==============================] - 15s 20ms/step - loss: 0.4003 - auc: 0.8974 - precision_at_recall: 0.8035\n",
      "Epoch 68/100\n",
      "758/758 [==============================] - 19s 26ms/step - loss: 0.3977 - auc: 0.8997 - precision_at_recall: 0.8065\n",
      "Epoch 69/100\n",
      "758/758 [==============================] - 20s 26ms/step - loss: 0.3929 - auc: 0.9021 - precision_at_recall: 0.8098\n",
      "Epoch 70/100\n",
      "758/758 [==============================] - 17s 22ms/step - loss: 0.3922 - auc: 0.9021 - precision_at_recall: 0.8126\n",
      "Epoch 71/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.3895 - auc: 0.9043 - precision_at_recall: 0.8182\n",
      "Epoch 72/100\n",
      "758/758 [==============================] - 23s 30ms/step - loss: 0.3888 - auc: 0.9039 - precision_at_recall: 0.8154\n",
      "Epoch 73/100\n",
      "758/758 [==============================] - 23s 30ms/step - loss: 0.3861 - auc: 0.9058 - precision_at_recall: 0.8228\n",
      "Epoch 74/100\n",
      "758/758 [==============================] - 25s 34ms/step - loss: 0.3839 - auc: 0.9062 - precision_at_recall: 0.8203\n",
      "Epoch 75/100\n",
      "758/758 [==============================] - 21s 28ms/step - loss: 0.3800 - auc: 0.9089 - precision_at_recall: 0.8237\n",
      "Epoch 76/100\n",
      "758/758 [==============================] - 21s 28ms/step - loss: 0.3811 - auc: 0.9073 - precision_at_recall: 0.8208\n",
      "Epoch 77/100\n",
      "758/758 [==============================] - 24s 32ms/step - loss: 0.3785 - auc: 0.9089 - precision_at_recall: 0.8216 0s - loss: 0.3791 - auc: 0.9087 - precision_at\n",
      "Epoch 78/100\n",
      "758/758 [==============================] - 25s 33ms/step - loss: 0.3722 - auc: 0.9121 - precision_at_recall: 0.8309\n",
      "Epoch 79/100\n",
      "758/758 [==============================] - 24s 32ms/step - loss: 0.3749 - auc: 0.9107 - precision_at_recall: 0.8255\n",
      "Epoch 80/100\n",
      "758/758 [==============================] - 29s 38ms/step - loss: 0.3729 - auc: 0.9122 - precision_at_recall: 0.8302\n",
      "Epoch 81/100\n",
      "758/758 [==============================] - 29s 38ms/step - loss: 0.3649 - auc: 0.9156 - precision_at_recall: 0.8323 0s - loss: 0.3656 - auc: 0.9154 - precision_at_r\n",
      "Epoch 82/100\n",
      "758/758 [==============================] - 28s 38ms/step - loss: 0.3666 - auc: 0.9164 - precision_at_recall: 0.8353\n",
      "Epoch 83/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.3609 - auc: 0.9178 - precision_at_recall: 0.8410\n",
      "Epoch 84/100\n",
      "758/758 [==============================] - 16s 21ms/step - loss: 0.3627 - auc: 0.9171 - precision_at_recall: 0.8377\n",
      "Epoch 85/100\n",
      "758/758 [==============================] - 17s 22ms/step - loss: 0.3592 - auc: 0.9192 - precision_at_recall: 0.8412\n",
      "Epoch 86/100\n",
      "758/758 [==============================] - 21s 27ms/step - loss: 0.3572 - auc: 0.9199 - precision_at_recall: 0.8458\n",
      "Epoch 87/100\n",
      "758/758 [==============================] - 26s 34ms/step - loss: 0.3530 - auc: 0.9220 - precision_at_recall: 0.8523\n",
      "Epoch 88/100\n",
      "758/758 [==============================] - 34s 45ms/step - loss: 0.3545 - auc: 0.9213 - precision_at_recall: 0.8459\n",
      "Epoch 89/100\n",
      "758/758 [==============================] - 24s 32ms/step - loss: 0.3484 - auc: 0.9241 - precision_at_recall: 0.8484\n",
      "Epoch 90/100\n",
      "758/758 [==============================] - 31s 41ms/step - loss: 0.3556 - auc: 0.9204 - precision_at_recall: 0.8456\n",
      "Epoch 91/100\n",
      "758/758 [==============================] - 23s 31ms/step - loss: 0.3521 - auc: 0.9216 - precision_at_recall: 0.8494\n",
      "Epoch 92/100\n",
      "758/758 [==============================] - 18s 24ms/step - loss: 0.3449 - auc: 0.9262 - precision_at_recall: 0.8570\n",
      "Epoch 93/100\n",
      "758/758 [==============================] - 19s 25ms/step - loss: 0.3407 - auc: 0.9278 - precision_at_recall: 0.8595\n",
      "Epoch 94/100\n",
      "758/758 [==============================] - 18s 23ms/step - loss: 0.3385 - auc: 0.9290 - precision_at_recall: 0.8628\n",
      "Epoch 95/100\n",
      "758/758 [==============================] - 20s 26ms/step - loss: 0.3356 - auc: 0.9293 - precision_at_recall: 0.8646\n",
      "Epoch 96/100\n",
      "758/758 [==============================] - 20s 27ms/step - loss: 0.3336 - auc: 0.9302 - precision_at_recall: 0.8653\n",
      "Epoch 97/100\n",
      "758/758 [==============================] - 19s 26ms/step - loss: 0.3297 - auc: 0.9330 - precision_at_recall: 0.8691\n",
      "Epoch 98/100\n",
      "758/758 [==============================] - 22s 29ms/step - loss: 0.3320 - auc: 0.9317 - precision_at_recall: 0.8696\n",
      "Epoch 99/100\n",
      "758/758 [==============================] - 19s 25ms/step - loss: 0.3329 - auc: 0.9320 - precision_at_recall: 0.8740\n",
      "Epoch 100/100\n",
      "758/758 [==============================] - 22s 29ms/step - loss: 0.3256 - auc: 0.9352 - precision_at_recall: 0.8757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb77e9732b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX, trainY, testX, testY = getPreprocessedData()\n",
    "model = newModel()\n",
    "model.fit(trainX, trainY, epochs = 100, batch_size=20, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation and Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create confusion Matrix\n",
    "def confusionMatrix(model, testX, testY):\n",
    "    pred = model.predict(testX)\n",
    "    predy = []\n",
    "    for i in range(0,len(pred)):\n",
    "        if pred[i] >= .5:\n",
    "            predy.append(1)\n",
    "        else:\n",
    "            predy.append(0)\n",
    "        \n",
    "    print(confusion_matrix(testY,predy))\n",
    "    \n",
    "#Calculates our model metrics\n",
    "def modelMetrics(testX, testY, model):\n",
    "    y_scores = model.predict(testX)\n",
    "    precision, recall, thresholds = precision_recall_curve(testY, y_scores)\n",
    "    rec80 = np.max(recall[precision >= .8])\n",
    "    print(\"Recall at 80% Precision: \" +str(rec80))\n",
    "    aucPR = auc(recall, precision)\n",
    "    print(\"PR-AUC: \" + str(aucPR))\n",
    "    \n",
    "    return rec80, aucPR, \n",
    "\n",
    "#plots a PR curve\n",
    "def PRplot(testX, testY, model):\n",
    "    y_scores = model.predict(testX)\n",
    "    precision, recall, thresholds = precision_recall_curve(testY, y_scores)\n",
    "    plt.plot(recall, precision)\n",
    "    plt.title(\"PR curve\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall at 80% Precision: 0.8717678100263853\n",
      "PR-AUC: 0.9108506722715662\n",
      "[[1540  355]\n",
      " [ 302 1593]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "modelMetrics(testX, testY, model)\n",
    "confusionMatrix(model, testX, testY)\n",
    "PRplot(testX, testY, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
