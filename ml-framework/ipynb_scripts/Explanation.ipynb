{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "895e7170",
   "metadata": {},
   "source": [
    "### Experiments pipeline description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ba5e33",
   "metadata": {},
   "source": [
    "#### Stage 1. Data mining\n",
    "\n",
    "The data is split into two populations: `positive` and `negative` samples, natures of mining of which are different.\n",
    "\n",
    "* positive samples are obtained by going through a list of repositories (20 most starred Java apache repos), and running RefactoringMiner tool to get instances of ExtractMethodRefactorings from authors' commits. For each instance a range of metrics is then computed (78) and written to csv file   \n",
    "\n",
    "* negative samples are obtained by going through a list of repositories (20 most starred Java apache repos), each Java file is then opened, and each method is considered that is then split into statements with PsiMiner. From the list of statements, then all combinations of consequent statements of lengths from 1 to n, where n - number of statements within the method. Then, each combination is considered as a fragment of code, for which the same list of metrics is computed and its Haas score is measured\n",
    "\n",
    "Haas score, is a scoring function inspired by Haas et al work [cite something] and can be written as \n",
    "$$Score(F) = LengthScore(F) + AreaScore(F) + DepthScore(F)$$\n",
    "And each component is defined as\n",
    "$$\n",
    "\\begin{align}\n",
    "&LengthScore(F) = min(0.1 \\cdot min(Length(F), Length(F^C)), 3) \\\\\n",
    "&AreaScore(F) = \\frac{2 \\cdot Depth(M)}{Area(M)} \\cdot min(Area(M) - Area(F), Area(M) - Area(F^C)) \\\\\n",
    "&DepthScore(F) = min(Depth(M) - Depth(F), Depth(M) - Depth(F^C)) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$M$ - method, $F$ - fragment of code, $F^C$ - fragment's complement w/r to method, or *remainder* \n",
    "\n",
    "$Length(F)$ - line-count of fragment of code, i.e. number of rows.\n",
    "\n",
    "$Depth(F)$ - nesting depth of code, i.e. row-wise maximal nesting level within fragment.\n",
    "\n",
    "$Area(F)$ - nesting area of code, i.e. row-wise sum of nesting levels within fragment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3a1ab0",
   "metadata": {},
   "source": [
    "#### Stage 2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b97186",
   "metadata": {},
   "source": [
    "Once the scripts were finished, overall about 16 thousand `positive` and 4 million `negative` samples were obtained. In order to reduce the stress on soon to-be-trained models it was decided to make a smaller subsample of negative observations that accounted for 1 million elements. \n",
    "\n",
    "After that, 80\\% of both sets were left for training and validation stages, and 20% were left for the testing stage. \n",
    "\n",
    "(80\\% from positive set, and 80\\% from negative set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73cd5b9",
   "metadata": {},
   "source": [
    "#### Stage 3. Training procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2716e697",
   "metadata": {},
   "source": [
    "Before each models training, two preprocessing tasks were always performed on the dataset.\n",
    "\n",
    "1. Negative samples Haas scores' of which were too high (higher than 50th percentile, or 95th, depending on the experiment) were dropped from the training set. Haas scores were also dropped from the dataset (not a feature for a model to know)\n",
    "2. Training data X scaled by MinMax strategy feature-wise, i.e. $X = \\frac{X - min(X)}{max(X) - min(X)}$, to ensure distribution of features from 0 to 1. Implemented via `sklearn.preprocessing.MinMaxScaler` and incorporated into the model through sklearn Pipelines\n",
    "\n",
    "After that, each model was trained with two thresholds for Haas-Score drop (50th, and 95th). \n",
    "\n",
    "Overall training procedure was conducted with use of `sklearn.model_selection.GridSearchCV` to find optimal hyperparameters from the given *grid* and get CrossValidation scores at the same time. As metric to optimize, the Precision-Recall Area Under Curve was chosen (PR AUC). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bede3c1f",
   "metadata": {},
   "source": [
    "#### Stage 4. Testing procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bd77f4",
   "metadata": {},
   "source": [
    "Each trained model was tested on the same dataset, that was built during stage 2, and 3 *metrics* were under consideration during the tests. \n",
    "\n",
    "1. Precision Recall Curve as a whole\n",
    "2. Area under the curve\n",
    "3. Recall at points Precision=0.9, and Precision=0.8 \n",
    "\n",
    "Overall, this results are yet to be computed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f32fac3",
   "metadata": {},
   "source": [
    "### Overall information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f0847b",
   "metadata": {},
   "source": [
    "#### Repository list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5279ae79",
   "metadata": {},
   "source": [
    "1.  https://github.com/apache/dubbo\n",
    "2.  https://github.com/apache/kafka\n",
    "3.  https://github.com/apache/skywalking\n",
    "4.  https://github.com/apache/flink\n",
    "5.  https://github.com/apache/rocketmq\n",
    "6.  https://github.com/apache/shardingsphere\n",
    "7.  https://github.com/apache/hadoop\n",
    "8.  https://github.com/apache/druid\n",
    "9.  https://github.com/apache/zookeeper\n",
    "10.  https://github.com/apache/pulsar\n",
    "11.  https://github.com/apache/shardingsphere-elasticjob\n",
    "12.  https://github.com/apache/cassandra\n",
    "13.  https://github.com/apache/storm\n",
    "14.  https://github.com/apache/tomcat\n",
    "15.  https://github.com/apache/jmeter\n",
    "16.  https://github.com/apache/zeppelin\n",
    "17.  https://github.com/apache/incubator-shenyu\n",
    "18.  https://github.com/apache/beam\n",
    "19.  https://github.com/apache/groovy\n",
    "20.  https://github.com/apache/hbase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a00e476",
   "metadata": {},
   "source": [
    "#### Models used\n",
    "\n",
    "1. `sklearn.ensemble.RandomForestClassifier` - RandomForest\n",
    "2. `sklearn.ensemble.BaggingClassifier` of `sklearn.svm.SVC` - Ensemble of support vector machines\n",
    "3. `sklearn.naive_bayes.GaussianNB` - Gaussian Naive Bayes\n",
    "4. `sklearn.neural_network.MLPClassifier` - MultiLayer Perceptron\n",
    "5. `sklearn.ensemble.GradientBoostingClassifier` - Ensemble of Decision Trees with Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb3c204",
   "metadata": {},
   "source": [
    "#### Models' Inference\n",
    "\n",
    "Models were trained with sklearn, Python's module and saved in PMML format via [sklearn2pmml](https://github.com/jpmml/sklearn2pmml), then used in Java through [pmml4s library](https://github.com/autodeployai/pmml4s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef08a16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
