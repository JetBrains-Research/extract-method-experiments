COMMIT ID: 9cfcf5c9e48cf00fbab6b8b134cf315c29069cd1
URL: https://github.com/apache/incubator-iceberg/commit/9cfcf5c9e48cf00fbab6b8b134cf315c29069cd1
DESCRIPTION: Extract Method	private fromJsonNode(node JsonNode) : StreamingOffset extracted from package fromJson(json String) : StreamingOffset in class org.apache.iceberg.spark.source.StreamingOffset
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/9cfcf5c9e48cf00fbab6b8b134cf315c29069cd1/spark3/src/main/java/org/apache/iceberg/spark/source/StreamingOffset.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/9cfcf5c9e48cf00fbab6b8b134cf315c29069cd1/spark3/src/main/java/org/apache/iceberg/spark/source/StreamingOffset.java#L140
DIRECTLY EXTRACTED OPERATION:
    // The version of StreamingOffset. The offset was created with a version number
    // used to validate when deserializing from json string.
    int version = JsonUtil.getInt(VERSION, node);
    Preconditions.checkArgument(version == CURR_VERSION,
        "This version of Iceberg source only supports version %s. Version %s is not supported.",
        CURR_VERSION, version);

    long snapshotId = JsonUtil.getLong(SNAPSHOT_ID, node);
    int position = JsonUtil.getInt(POSITION, node);
    boolean shouldScanAllFiles = JsonUtil.getBool(SCAN_ALL_FILES, node);

    return new StreamingOffset(snapshotId, position, shouldScanAllFiles);
  }
}
PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 647
FRAGMENT LINE AVG SIZE: 46.214285714285715
DEPTHS:
1 2 2 2 2 2 2 2 2 2 2 2 1 0 
AREA: 24
AVG DEPTH: 1.7142857142857142
NUMBER OF LINES IN FRAGMENT: 14
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 01393a06c284175edab75de34f48b2bfbd606081
URL: https://github.com/apache/incubator-iceberg/commit/01393a06c284175edab75de34f48b2bfbd606081
DESCRIPTION: Extract Method	public serializeToBytes(obj Object, confSerializer Function<Configuration,SerializableSupplier<Configuration>>) : byte[] extracted from public serializeToBytes(obj Object) : byte[] in class org.apache.iceberg.util.SerializationUtil
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/01393a06c284175edab75de34f48b2bfbd606081/core/src/main/java/org/apache/iceberg/util/SerializationUtil.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/01393a06c284175edab75de34f48b2bfbd606081/core/src/main/java/org/apache/iceberg/util/SerializationUtil.java#L50
DIRECTLY EXTRACTED OPERATION:
   * Serialize an object to bytes. If the object implements {@link HadoopConfigurable}, the confSerializer will be used
   * to serialize Hadoop configuration used by the object.
   * @param obj object to serialize
   * @param confSerializer serializer for the Hadoop configuration
   * @return serialized bytes
   */
  public static byte[] serializeToBytes(Object obj,
                                        Function<Configuration, SerializableSupplier<Configuration>> confSerializer) {
    if (obj instanceof HadoopConfigurable) {
      ((HadoopConfigurable) obj).serializeConfWith(confSerializer);
    }

    try (ByteArrayOutputStream baos = new ByteArrayOutputStream();
         ObjectOutputStream oos = new ObjectOutputStream(baos)) {
      oos.writeObject(obj);
      return baos.toByteArray();
    } catch (IOException e) {
      throw new UncheckedIOException("Failed to serialize object", e);
    }
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 915
FRAGMENT LINE AVG SIZE: 43.57142857142857
DEPTHS:
0 1 1 1 1 1 1 1 2 3 2 2 2 2 3 3 3 3 2 1 1 
AREA: 36
AVG DEPTH: 1.7142857142857142
NUMBER OF LINES IN FRAGMENT: 21
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 0e4f0f0bf5644faa372b96abd7df25fd8930ddc1
URL: https://github.com/apache/incubator-iceberg/commit/0e4f0f0bf5644faa372b96abd7df25fd8930ddc1
DESCRIPTION: Extract Method	package withWriterVersion(version WriterVersion) : WriteBuilder extracted from public build() : FileAppender<D> in class org.apache.iceberg.parquet.Parquet.WriteBuilder
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/0e4f0f0bf5644faa372b96abd7df25fd8930ddc1/parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/0e4f0f0bf5644faa372b96abd7df25fd8930ddc1/parquet/src/main/java/org/apache/iceberg/parquet/Parquet.java#L197
DIRECTLY EXTRACTED OPERATION:
    WriteBuilder withWriterVersion(WriterVersion version) {
      this.writerVersion = version;
      return this;
    }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 122
FRAGMENT LINE AVG SIZE: 24.4
DEPTHS:
2 3 3 2 2 
AREA: 12
AVG DEPTH: 2.4
NUMBER OF LINES IN FRAGMENT: 5
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: a0d6cdf8a76e29d07ce0d5889b7ee9a2335bde1f
URL: https://github.com/apache/incubator-iceberg/commit/a0d6cdf8a76e29d07ce0d5889b7ee9a2335bde1f
DESCRIPTION: Extract Method	public actualRowSet(table Table, snapshotId Long, columns String...) : StructLikeSet extracted from public actualRowSet(table Table, columns String...) : StructLikeSet in class org.apache.iceberg.flink.SimpleDataUtil
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/a0d6cdf8a76e29d07ce0d5889b7ee9a2335bde1f/flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/a0d6cdf8a76e29d07ce0d5889b7ee9a2335bde1f/flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java#L215
DIRECTLY EXTRACTED OPERATION:
    table.refresh();
    StructLikeSet set = StructLikeSet.create(table.schema().asStruct());
    try (CloseableIterable<Record> reader = IcebergGenerics
        .read(table)
        .useSnapshot(snapshotId == null ? table.currentSnapshot().snapshotId() : snapshotId)
        .select(columns)
        .build()) {
      reader.forEach(set::add);
    }
    return set;
  }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 372
FRAGMENT LINE AVG SIZE: 31.0
DEPTHS:
1 2 2 2 2 2 2 3 2 2 1 1 
AREA: 22
AVG DEPTH: 1.8333333333333333
NUMBER OF LINES IN FRAGMENT: 12
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 49d8ade09b24fce3b95450f7bd1ba1cb517261b0
URL: https://github.com/apache/incubator-iceberg/commit/49d8ade09b24fce3b95450f7bd1ba1cb517261b0
DESCRIPTION: Extract Method	private toOutputRows(result RewriteManifests.Result) : InternalRow[] extracted from public call(args InternalRow) : InternalRow[] in class org.apache.iceberg.spark.procedures.RewriteManifestsProcedure
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/49d8ade09b24fce3b95450f7bd1ba1cb517261b0/spark3/src/main/java/org/apache/iceberg/spark/procedures/RewriteManifestsProcedure.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/49d8ade09b24fce3b95450f7bd1ba1cb517261b0/spark3/src/main/java/org/apache/iceberg/spark/procedures/RewriteManifestsProcedure.java#L96
DIRECTLY EXTRACTED OPERATION:
    int rewrittenManifestsCount = Iterables.size(result.rewrittenManifests());
    int addedManifestsCount = Iterables.size(result.addedManifests());
    InternalRow row = newInternalRow(rewrittenManifestsCount, addedManifestsCount);
    return new InternalRow[]{row};
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 274
FRAGMENT LINE AVG SIZE: 45.666666666666664
DEPTHS:
1 2 2 2 1 1 
AREA: 9
AVG DEPTH: 1.5
NUMBER OF LINES IN FRAGMENT: 6
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: db8248c16e99c435ff7eed8fa86bc3913af2756a
URL: https://github.com/apache/incubator-iceberg/commit/db8248c16e99c435ff7eed8fa86bc3913af2756a
DESCRIPTION: Extract Method	private setCustomCatalogProperties(catalogName String, warehouseLocation String) : void extracted from public testLoadTableFromCatalog() : void in class org.apache.iceberg.mr.TestCatalogs
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/db8248c16e99c435ff7eed8fa86bc3913af2756a/mr/src/test/java/org/apache/iceberg/mr/TestCatalogs.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/db8248c16e99c435ff7eed8fa86bc3913af2756a/mr/src/test/java/org/apache/iceberg/mr/TestCatalogs.java#L293
DIRECTLY EXTRACTED OPERATION:
    conf.set(String.format(InputFormatConfig.CATALOG_WAREHOUSE_TEMPLATE, catalogName), warehouseLocation);
    conf.set(String.format(InputFormatConfig.CATALOG_CLASS_TEMPLATE, catalogName), CustomHadoopCatalog.class.getName());
    conf.set(String.format(InputFormatConfig.CATALOG_TYPE_TEMPLATE, catalogName), "custom");
    conf.set(InputFormatConfig.CATALOG_NAME, catalogName);
  }
}
PARAMS COUNT: 2
IS VOID METHOD: true
FRAGMENT LENGTH: 386
FRAGMENT LINE AVG SIZE: 64.33333333333333
DEPTHS:
1 2 2 2 1 0 
AREA: 8
AVG DEPTH: 1.3333333333333333
NUMBER OF LINES IN FRAGMENT: 6
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private setCustomCatalogProperties(catalogName String, warehouseLocation String) : void extracted from public testCreateDropTableToCatalog() : void in class org.apache.iceberg.mr.TestCatalogs
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/db8248c16e99c435ff7eed8fa86bc3913af2756a/mr/src/test/java/org/apache/iceberg/mr/TestCatalogs.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/db8248c16e99c435ff7eed8fa86bc3913af2756a/mr/src/test/java/org/apache/iceberg/mr/TestCatalogs.java#L293
DIRECTLY EXTRACTED OPERATION:
    conf.set(String.format(InputFormatConfig.CATALOG_WAREHOUSE_TEMPLATE, catalogName), warehouseLocation);
    conf.set(String.format(InputFormatConfig.CATALOG_CLASS_TEMPLATE, catalogName), CustomHadoopCatalog.class.getName());
    conf.set(String.format(InputFormatConfig.CATALOG_TYPE_TEMPLATE, catalogName), "custom");
    conf.set(InputFormatConfig.CATALOG_NAME, catalogName);
  }
}
PARAMS COUNT: 2
IS VOID METHOD: true
FRAGMENT LENGTH: 386
FRAGMENT LINE AVG SIZE: 64.33333333333333
DEPTHS:
1 2 2 2 1 0 
AREA: 8
AVG DEPTH: 1.3333333333333333
NUMBER OF LINES IN FRAGMENT: 6
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	package shell(configs Map<String,String>) : TestHiveShell extracted from package shell() : TestHiveShell in class org.apache.iceberg.mr.hive.HiveIcebergStorageHandlerTestUtils
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/db8248c16e99c435ff7eed8fa86bc3913af2756a/mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerTestUtils.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/db8248c16e99c435ff7eed8fa86bc3913af2756a/mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerTestUtils.java#L67
DIRECTLY EXTRACTED OPERATION:
    TestHiveShell shell = new TestHiveShell();
    shell.setHiveConfValue("hive.notification.event.poll.interval", "-1");
    shell.setHiveConfValue("hive.tez.exec.print.summary", "true");
    configs.forEach((k, v) -> shell.setHiveConfValue(k, v));
    // We would like to make sure that ORC reading overrides this config, so reading Iceberg tables could work in
    // systems (like Hive 3.2 and higher) where this value is set to true explicitly.
    shell.setHiveConfValue(OrcConf.FORCE_POSITIONAL_EVOLUTION.getHiveConfName(), "true");
    shell.start();
    return shell;
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 582
FRAGMENT LINE AVG SIZE: 52.90909090909091
DEPTHS:
1 2 2 2 2 2 2 2 2 1 1 
AREA: 19
AVG DEPTH: 1.7272727272727273
NUMBER OF LINES IN FRAGMENT: 11
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	package testTables(shell TestHiveShell, testTableType TestTables.TestTableType, temp TemporaryFolder, catalogName String) : TestTables extracted from package testTables(shell TestHiveShell, testTableType TestTables.TestTableType, temp TemporaryFolder) : TestTables in class org.apache.iceberg.mr.hive.HiveIcebergStorageHandlerTestUtils
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/db8248c16e99c435ff7eed8fa86bc3913af2756a/mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerTestUtils.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/db8248c16e99c435ff7eed8fa86bc3913af2756a/mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerTestUtils.java#L84
DIRECTLY EXTRACTED OPERATION:
                               String catalogName) throws IOException {
    return testTableType.instance(shell.metastore().hiveConf(), temp, catalogName);
  }

PARAMS COUNT: 4
IS VOID METHOD: false
FRAGMENT LENGTH: 161
FRAGMENT LINE AVG SIZE: 40.25
DEPTHS:
1 2 1 1 
AREA: 5
AVG DEPTH: 1.25
NUMBER OF LINES IN FRAGMENT: 4
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public propertiesForCreateTableSQL(tableProperties Map<String,String>) : String extracted from public createHiveTableSQL(identifier TableIdentifier, tableProps Map<String,String>) : String in class org.apache.iceberg.mr.hive.TestTables
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/db8248c16e99c435ff7eed8fa86bc3913af2756a/mr/src/test/java/org/apache/iceberg/mr/hive/TestTables.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/db8248c16e99c435ff7eed8fa86bc3913af2756a/mr/src/test/java/org/apache/iceberg/mr/hive/TestTables.java#L109
DIRECTLY EXTRACTED OPERATION:
   * The table properties string needed for the CREATE TABLE ... commands,
   * like "TBLPROPERTIES('iceberg.catalog'='mycatalog')
   * @return
   */
  public String propertiesForCreateTableSQL(Map<String, String> tableProperties) {
    Map<String, String> properties = new HashMap<>(tableProperties);
    properties.putIfAbsent(InputFormatConfig.CATALOG_NAME, catalog);
    String props = properties.entrySet().stream()
            .map(entry -> String.format("'%s'='%s'", entry.getKey(), entry.getValue()))
            .collect(Collectors.joining(","));
    return " TBLPROPERTIES (" + props + ")";
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 606
FRAGMENT LINE AVG SIZE: 46.61538461538461
DEPTHS:
0 1 1 1 1 2 2 2 2 2 2 1 1 
AREA: 18
AVG DEPTH: 1.3846153846153846
NUMBER OF LINES IN FRAGMENT: 13
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: ae2f7da9b712b1d4047e01506c46756b32ae0542
URL: https://github.com/apache/incubator-iceberg/commit/ae2f7da9b712b1d4047e01506c46756b32ae0542
DESCRIPTION: Extract Method	private renameAndBackupSourceTable() : void extracted from private doExecute() : Long in class org.apache.iceberg.actions.Spark3MigrateAction
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/ae2f7da9b712b1d4047e01506c46756b32ae0542/spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/ae2f7da9b712b1d4047e01506c46756b32ae0542/spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java#L136
DIRECTLY EXTRACTED OPERATION:
    try {
      LOG.info("Renaming {} as {} for backup", sourceTableIdent(), backupIdent);
      destCatalog().renameTable(sourceTableIdent(), backupIdent);

    } catch (org.apache.spark.sql.catalyst.analysis.NoSuchTableException e) {
      throw new NoSuchTableException("Cannot find source table %s", sourceTableIdent());

    } catch (org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException e) {
      throw new AlreadyExistsException(
          "Cannot rename %s as %s for backup. The backup table already exists.",
          sourceTableIdent(), backupIdent);
    }
  }

IS VOID METHOD: true
FRAGMENT LENGTH: 587
FRAGMENT LINE AVG SIZE: 41.92857142857143
DEPTHS:
2 3 3 3 3 3 3 3 3 3 3 2 1 1 
AREA: 36
AVG DEPTH: 2.5714285714285716
NUMBER OF LINES IN FRAGMENT: 14
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private restoreSourceTable() : void extracted from private doExecute() : Long in class org.apache.iceberg.actions.Spark3MigrateAction
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/ae2f7da9b712b1d4047e01506c46756b32ae0542/spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/ae2f7da9b712b1d4047e01506c46756b32ae0542/spark3/src/main/java/org/apache/iceberg/actions/Spark3MigrateAction.java#L151
DIRECTLY EXTRACTED OPERATION:
    try {
      LOG.info("Restoring {} from {}", sourceTableIdent(), backupIdent);
      destCatalog().renameTable(backupIdent, sourceTableIdent());

    } catch (org.apache.spark.sql.catalyst.analysis.NoSuchTableException e) {
      LOG.error("Cannot restore the original table, the backup table {} cannot be found", backupIdent, e);

    } catch (org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException e) {
      LOG.error("Cannot restore the original table, a table with the original name exists. " +
          "Use the backup table {} to restore the original table manually.", backupIdent, e);
    }
  }
}
IS VOID METHOD: true
FRAGMENT LENGTH: 622
FRAGMENT LINE AVG SIZE: 47.84615384615385
DEPTHS:
2 3 3 3 3 3 3 3 3 3 2 1 0 
AREA: 32
AVG DEPTH: 2.4615384615384617
NUMBER OF LINES IN FRAGMENT: 13
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: d1510340eaff68d88a2e8194d58e7e493af02bcc
URL: https://github.com/apache/incubator-iceberg/commit/d1510340eaff68d88a2e8194d58e7e493af02bcc
DESCRIPTION: Extract Method	private dataFiles(executor ExecutorService, location String, jobContext JobContext, io FileIO, throwOnFailure boolean) : Collection<DataFile> extracted from public commitJob(originalContext JobContext) : void in class org.apache.iceberg.mr.hive.HiveIcebergOutputCommitter
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/d1510340eaff68d88a2e8194d58e7e493af02bcc/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/d1510340eaff68d88a2e8194d58e7e493af02bcc/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java#L341
DIRECTLY EXTRACTED OPERATION:
   * Get the committed data files for this table and job.
   * @param executor The executor used for reading the forCommit files parallel
   * @param location The location of the table
   * @param jobContext The job context
   * @param io The FileIO used for reading a files generated for commit
   * @param throwOnFailure If <code>true</code> then it throws an exception on failure
   * @return The list of the committed data files
   */
  private static Collection<DataFile> dataFiles(ExecutorService executor, String location, JobContext jobContext,
      FileIO io, boolean throwOnFailure) {
    JobConf conf = jobContext.getJobConf();
    // If there are reducers, then every reducer will generate a result file.
    // If this is a map only task, then every mapper will generate a result file.
    int expectedFiles = conf.getNumReduceTasks() > 0 ? conf.getNumReduceTasks() : conf.getNumMapTasks();

    Collection<DataFile> dataFiles = new ConcurrentLinkedQueue<>();

    // Reading the committed files. The assumption here is that the taskIds are generated in sequential order
    // starting from 0.
    Tasks.range(expectedFiles)
        .throwFailureWhenFinished(throwOnFailure)
        .executeWith(executor)
        .retry(3)
        .run(taskId -> {
          String taskFileName = generateFileForCommitLocation(location, conf, jobContext.getJobID(), taskId);
          dataFiles.addAll(Arrays.asList(readFileForCommit(taskFileName, io)));
        });

    return dataFiles;
  }

PARAMS COUNT: 5
IS VOID METHOD: false
FRAGMENT LENGTH: 1494
FRAGMENT LINE AVG SIZE: 48.193548387096776
DEPTHS:
0 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 2 2 2 1 1 
AREA: 51
AVG DEPTH: 1.6451612903225807
NUMBER OF LINES IN FRAGMENT: 31
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private commitTable(io FileIO, executor ExecutorService, jobContext JobContext, name String, location String) : void extracted from public commitJob(originalContext JobContext) : void in class org.apache.iceberg.mr.hive.HiveIcebergOutputCommitter
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/d1510340eaff68d88a2e8194d58e7e493af02bcc/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/d1510340eaff68d88a2e8194d58e7e493af02bcc/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergOutputCommitter.java#L241
DIRECTLY EXTRACTED OPERATION:
   * Collects the additions to a single table and adds/commits the new files to the Iceberg table.
   * @param io The io to read the forCommit files
   * @param executor The executor used to read the forCommit files
   * @param jobContext The job context
   * @param name The name of the table used for loading from the catalog
   * @param location The location of the table used for loading from the catalog
   */
  private void commitTable(FileIO io, ExecutorService executor, JobContext jobContext, String name, String location) {
    JobConf conf = jobContext.getJobConf();
    Properties catalogProperties = new Properties();
    catalogProperties.put(Catalogs.NAME, name);
    catalogProperties.put(Catalogs.LOCATION, location);
    Table table = Catalogs.loadTable(conf, catalogProperties);

    long startTime = System.currentTimeMillis();
    LOG.info("Committing job has started for table: {}, using location: {}",
        table, generateJobLocation(location, conf, jobContext.getJobID()));

    Collection<DataFile> dataFiles = dataFiles(executor, location, jobContext, io, true);

    if (dataFiles.size() > 0) {
      // Appending data files to the table
      AppendFiles append = table.newAppend();
      dataFiles.forEach(append::appendFile);
      append.commit();
      LOG.info("Commit took {} ms for table: {} with {} file(s)", System.currentTimeMillis() - startTime, table,
          dataFiles.size());
      LOG.debug("Added files {}", dataFiles);
    } else {
      LOG.info("Commit took {} ms for table: {} with no new files", System.currentTimeMillis() - startTime, table);
    }
  }

PARAMS COUNT: 5
IS VOID METHOD: true
FRAGMENT LENGTH: 1610
FRAGMENT LINE AVG SIZE: 48.78787878787879
DEPTHS:
0 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 2 1 1 
AREA: 64
AVG DEPTH: 1.9393939393939394
NUMBER OF LINES IN FRAGMENT: 33
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: f0a6b717dbf662caa9c762e72c47715a12625647
URL: https://github.com/apache/incubator-iceberg/commit/f0a6b717dbf662caa9c762e72c47715a12625647
DESCRIPTION: Extract Method	public importSparkTable(spark SparkSession, sourceTableIdent TableIdentifier, targetTable Table, stagingDir String, partitionFilter Map<String,String>) : void extracted from public importSparkTable(spark SparkSession, sourceTableIdent TableIdentifier, targetTable Table, stagingDir String) : void in class org.apache.iceberg.spark.SparkTableUtil
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/f0a6b717dbf662caa9c762e72c47715a12625647/spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/f0a6b717dbf662caa9c762e72c47715a12625647/spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java#L496
DIRECTLY EXTRACTED OPERATION:
   * Import files from an existing Spark table to an Iceberg table.
   *
   * The import uses the Spark session to get table metadata. It assumes no
   * operation is going on the original and target table and thus is not
   * thread-safe.
   *
   * @param spark a Spark session
   * @param sourceTableIdent an identifier of the source Spark table
   * @param targetTable an Iceberg table where to import the data
   * @param stagingDir a staging directory to store temporary manifest files
   * @param partitionFilter only import partitions whose values match those in the map, can be partially defined
   */
  public static void importSparkTable(SparkSession spark, TableIdentifier sourceTableIdent, Table targetTable,
                                      String stagingDir, Map<String, String> partitionFilter) {
    SessionCatalog catalog = spark.sessionState().catalog();

    String db = sourceTableIdent.database().nonEmpty() ?
        sourceTableIdent.database().get() :
        catalog.getCurrentDatabase();
    TableIdentifier sourceTableIdentWithDB = new TableIdentifier(sourceTableIdent.table(), Some.apply(db));

    if (!catalog.tableExists(sourceTableIdentWithDB)) {
      throw new org.apache.iceberg.exceptions.NoSuchTableException("Table %s does not exist", sourceTableIdentWithDB);
    }

    try {
      PartitionSpec spec = SparkSchemaUtil.specForTable(spark, sourceTableIdentWithDB.unquotedString());

      if (spec == PartitionSpec.unpartitioned()) {
        importUnpartitionedSparkTable(spark, sourceTableIdentWithDB, targetTable);
      } else {
        List<SparkPartition> sourceTablePartitions = getPartitions(spark, sourceTableIdent);
        Preconditions.checkArgument(!sourceTablePartitions.isEmpty(),
            "Cannot find any partitions in table %s", sourceTableIdent);
        List<SparkPartition> filteredPartitions = filterPartitions(sourceTablePartitions, partitionFilter);
        Preconditions.checkArgument(!filteredPartitions.isEmpty(),
            "Cannot find any partitions which match the given filter. Partition filter is %s",
            MAP_JOINER.join(partitionFilter));
        importSparkPartitions(spark, filteredPartitions, targetTable, spec, stagingDir);
      }
    } catch (AnalysisException e) {
      throw SparkExceptionUtil.toUncheckedException(
          e, "Unable to get partition spec for table: %s", sourceTableIdentWithDB);
    }
  }

FRAGMENT LENGTH: 2408
FRAGMENT LINE AVG SIZE: 52.34782608695652
DEPTHS:
0 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 3 2 2 2 3 3 3 4 4 4 4 4 4 4 4 4 4 3 3 3 3 2 1 1 
AREA: 103
AVG DEPTH: 2.239130434782609
NUMBER OF LINES IN FRAGMENT: 46
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 713136df34a65df4306a7b98c67c4b6d11cc65ad
URL: https://github.com/apache/incubator-iceberg/commit/713136df34a65df4306a7b98c67c4b6d11cc65ad
DESCRIPTION: Extract Method	private applyEqDeletes() : List<Predicate<T>> extracted from private applyEqDeletes(records CloseableIterable<T>) : CloseableIterable<T> in class org.apache.iceberg.data.DeleteFilter
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/713136df34a65df4306a7b98c67c4b6d11cc65ad/data/src/main/java/org/apache/iceberg/data/DeleteFilter.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/713136df34a65df4306a7b98c67c4b6d11cc65ad/data/src/main/java/org/apache/iceberg/data/DeleteFilter.java#L115
DIRECTLY EXTRACTED OPERATION:
    List<Predicate<T>> isInDeleteSets = Lists.newArrayList();
    if (eqDeletes.isEmpty()) {
      return isInDeleteSets;
    }

    Multimap<Set<Integer>, DeleteFile> filesByDeleteIds = Multimaps.newMultimap(Maps.newHashMap(), Lists::newArrayList);
    for (DeleteFile delete : eqDeletes) {
      filesByDeleteIds.put(Sets.newHashSet(delete.equalityFieldIds()), delete);
    }

    for (Map.Entry<Set<Integer>, Collection<DeleteFile>> entry : filesByDeleteIds.asMap().entrySet()) {
      Set<Integer> ids = entry.getKey();
      Iterable<DeleteFile> deletes = entry.getValue();

      Schema deleteSchema = TypeUtil.select(requiredSchema, ids);

      // a projection to select and reorder fields of the file schema to match the delete rows
      StructProjection projectRow = StructProjection.create(requiredSchema, deleteSchema);

      Iterable<CloseableIterable<Record>> deleteRecords = Iterables.transform(deletes,
          delete -> openDeletes(delete, deleteSchema));
      StructLikeSet deleteSet = Deletes.toEqualitySet(
          // copy the delete records because they will be held in a set
          CloseableIterable.transform(CloseableIterable.concat(deleteRecords), Record::copy),
          deleteSchema.asStruct());

      Predicate<T> isInDeleteSet = record -> deleteSet.contains(projectRow.wrap(asStructLike(record)));
      isInDeleteSets.add(isInDeleteSet);
    }

    return isInDeleteSets;
  }

IS VOID METHOD: false
FRAGMENT LENGTH: 1419
FRAGMENT LINE AVG SIZE: 43.0
DEPTHS:
1 2 3 2 2 2 2 3 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 1 1 
AREA: 82
AVG DEPTH: 2.484848484848485
NUMBER OF LINES IN FRAGMENT: 33
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 343104c8e40bbcacbf6297445cc9dfc82769afe8
URL: https://github.com/apache/incubator-iceberg/commit/343104c8e40bbcacbf6297445cc9dfc82769afe8
DESCRIPTION: Extract Method	public convertRowDataToRow(rowDataList List<RowData>, rowType RowType) : List<Row> extracted from public readRows(inputFormat FlinkInputFormat, rowType RowType) : List<Row> in class org.apache.iceberg.flink.TestHelpers
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/343104c8e40bbcacbf6297445cc9dfc82769afe8/flink/src/test/java/org/apache/iceberg/flink/TestHelpers.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/343104c8e40bbcacbf6297445cc9dfc82769afe8/flink/src/test/java/org/apache/iceberg/flink/TestHelpers.java#L92
DIRECTLY EXTRACTED OPERATION:
    DataStructureConverter<Object, Object> converter = DataStructureConverters.getConverter(
        TypeConversions.fromLogicalToDataType(rowType));
    return rowDataList.stream()
        .map(converter::toExternal)
        .map(Row.class::cast)
        .collect(Collectors.toList());
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 292
FRAGMENT LINE AVG SIZE: 36.5
DEPTHS:
1 2 2 2 2 2 1 1 
AREA: 13
AVG DEPTH: 1.625
NUMBER OF LINES IN FRAGMENT: 8
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 255e9525c5bfeb7b57e49a2d96178ebe615c5e16
URL: https://github.com/apache/incubator-iceberg/commit/255e9525c5bfeb7b57e49a2d96178ebe615c5e16
DESCRIPTION: Extract Method	private toJson(struct Types.StructType, schemaId Integer, generator JsonGenerator) : void extracted from package toJson(struct Types.StructType, generator JsonGenerator) : void in class org.apache.iceberg.SchemaParser
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/255e9525c5bfeb7b57e49a2d96178ebe615c5e16/core/src/main/java/org/apache/iceberg/SchemaParser.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/255e9525c5bfeb7b57e49a2d96178ebe615c5e16/core/src/main/java/org/apache/iceberg/SchemaParser.java#L65
DIRECTLY EXTRACTED OPERATION:
    generator.writeStartObject();

    generator.writeStringField(TYPE, STRUCT);
    if (schemaId != null) {
      generator.writeNumberField(SCHEMA_ID, schemaId);
    }

    generator.writeArrayFieldStart(FIELDS);
    for (Types.NestedField field : struct.fields()) {
      generator.writeStartObject();
      generator.writeNumberField(ID, field.fieldId());
      generator.writeStringField(NAME, field.name());
      generator.writeBooleanField(REQUIRED, field.isRequired());
      generator.writeFieldName(TYPE);
      toJson(field.type(), generator);
      if (field.doc() != null) {
        generator.writeStringField(DOC, field.doc());
      }
      generator.writeEndObject();
    }
    generator.writeEndArray();

    generator.writeEndObject();
  }

PARAMS COUNT: 3
IS VOID METHOD: true
FRAGMENT LENGTH: 760
FRAGMENT LINE AVG SIZE: 30.4
DEPTHS:
1 2 2 2 3 2 2 2 2 3 3 3 3 3 3 3 4 3 3 2 2 2 2 1 1 
AREA: 59
AVG DEPTH: 2.36
NUMBER OF LINES IN FRAGMENT: 25
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 4f9edccda324f5ae5be45dfc666a682bfb0c14c7
URL: https://github.com/apache/incubator-iceberg/commit/4f9edccda324f5ae5be45dfc666a682bfb0c14c7
DESCRIPTION: Extract Method	protected isConnectionException(exc Exception) : boolean extracted from public run(action Action<R,C,E>) : R in class org.apache.iceberg.hive.ClientPool
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/4f9edccda324f5ae5be45dfc666a682bfb0c14c7/hive-metastore/src/main/java/org/apache/iceberg/hive/ClientPool.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/4f9edccda324f5ae5be45dfc666a682bfb0c14c7/hive-metastore/src/main/java/org/apache/iceberg/hive/ClientPool.java#L79
DIRECTLY EXTRACTED OPERATION:
    return reconnectExc.isInstance(exc);
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 46
FRAGMENT LINE AVG SIZE: 15.333333333333334
DEPTHS:
1 1 1 
AREA: 3
AVG DEPTH: 1.0
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 8a84715205631a17668ec8d63a791a25fb511992
URL: https://github.com/apache/incubator-iceberg/commit/8a84715205631a17668ec8d63a791a25fb511992
DESCRIPTION: Extract Method	private newFile(recordCount long, partition StructLike) : DataFile extracted from private newFile(recordCount long) : DataFile in class org.apache.iceberg.TestManifestWriter
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/8a84715205631a17668ec8d63a791a25fb511992/core/src/test/java/org/apache/iceberg/TestManifestWriter.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/8a84715205631a17668ec8d63a791a25fb511992/core/src/test/java/org/apache/iceberg/TestManifestWriter.java#L94
DIRECTLY EXTRACTED OPERATION:
    String fileName = UUID.randomUUID().toString();
    DataFiles.Builder builder = DataFiles.builder(SPEC)
        .withPath("data_bucket=0/" + fileName + ".parquet")
        .withFileSizeInBytes(1024)
        .withRecordCount(recordCount);
    if (partition != null) {
      builder.withPartition(partition);
    }
    return builder.build();
  }
}
PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 351
FRAGMENT LINE AVG SIZE: 31.90909090909091
DEPTHS:
1 2 2 2 2 2 3 2 2 1 0 
AREA: 19
AVG DEPTH: 1.7272727272727273
NUMBER OF LINES IN FRAGMENT: 11
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: a42df9e2d3d837346c3ba1eaaa57eb9c209706e6
URL: https://github.com/apache/incubator-iceberg/commit/a42df9e2d3d837346c3ba1eaaa57eb9c209706e6
DESCRIPTION: Extract Method	private writeRecords(taskNum int, attemptNum int, commitTasks boolean, abortTasks boolean, conf JobConf, committer OutputCommitter) : List<Record> extracted from private writeRecords(taskNum int, attemptNum int, commitTasks boolean, abortTasks boolean, conf JobConf) : List<Record> in class org.apache.iceberg.mr.hive.TestHiveIcebergOutputCommitter
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/a42df9e2d3d837346c3ba1eaaa57eb9c209706e6/mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergOutputCommitter.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/a42df9e2d3d837346c3ba1eaaa57eb9c209706e6/mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergOutputCommitter.java#L237
DIRECTLY EXTRACTED OPERATION:
   * Write random records to the given table using separate {@link HiveIcebergOutputCommitter} and
   * a separate {@link HiveIcebergRecordWriter} for every task.
   * @param taskNum The number of tasks in the job handled by the committer
   * @param attemptNum The id used for attempt number generation
   * @param commitTasks If <code>true</code> the tasks will be committed
   * @param abortTasks If <code>true</code> the tasks will be aborted - needed so we can simulate no commit/no abort
   *                   situation
   * @param conf The job configuration
   * @param committer The output committer that should be used for committing/aborting the tasks
   * @return The random generated records which were appended to the table
   * @throws IOException Propagating {@link HiveIcebergRecordWriter} exceptions
   */
  private List<Record> writeRecords(int taskNum, int attemptNum, boolean commitTasks, boolean abortTasks,
                                    JobConf conf, OutputCommitter committer) throws IOException {
    List<Record> expected = new ArrayList<>(RECORD_NUM * taskNum);

    FileIO io = HiveIcebergStorageHandler.io(conf);
    LocationProvider location = HiveIcebergStorageHandler.location(conf);
    EncryptionManager encryption = HiveIcebergStorageHandler.encryption(conf);
    Schema schema = HiveIcebergStorageHandler.schema(conf);
    PartitionSpec spec = HiveIcebergStorageHandler.spec(conf);

    for (int i = 0; i < taskNum; ++i) {
      List<Record> records = TestHelper.generateRandomRecords(schema, RECORD_NUM, i + attemptNum);
      TaskAttemptID taskId = new TaskAttemptID(JOB_ID.getJtIdentifier(), JOB_ID.getId(), TaskType.MAP, i, attemptNum);
      OutputFileFactory outputFileFactory =
          new OutputFileFactory(spec, FileFormat.PARQUET, location, io, encryption, taskId.getTaskID().getId(),
              attemptNum, QUERY_ID + "-" + JOB_ID);
      HiveIcebergRecordWriter testWriter = new HiveIcebergRecordWriter(schema, spec, FileFormat.PARQUET,
          new GenericAppenderFactory(schema), outputFileFactory, io, TARGET_FILE_SIZE, taskId);

      Container<Record> container = new Container<>();

      for (Record record : records) {
        container.set(record);
        testWriter.write(container);
      }

      testWriter.close(false);
      if (commitTasks) {
        committer.commitTask(new TaskAttemptContextImpl(conf, taskId));
        expected.addAll(records);
      } else if (abortTasks) {
        committer.abortTask(new TaskAttemptContextImpl(conf, taskId));
      }
    }

    return expected;
  }

PARAMS COUNT: 6
IS VOID METHOD: false
FRAGMENT LENGTH: 2569
FRAGMENT LINE AVG SIZE: 51.38
DEPTHS:
0 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 4 4 3 3 3 3 4 4 4 4 3 2 2 2 1 1 
AREA: 111
AVG DEPTH: 2.22
NUMBER OF LINES IN FRAGMENT: 50
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: eca7dc38a46df1919059c1b2d303588d9bc6d8bc
URL: https://github.com/apache/incubator-iceberg/commit/eca7dc38a46df1919059c1b2d303588d9bc6d8bc
DESCRIPTION: Extract Method	protected createAndInitTable(schema String, jsonData String) : void extracted from protected createAndInitTable(schema String) : void in class org.apache.iceberg.spark.extensions.SparkRowLevelOperationsTestBase
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/eca7dc38a46df1919059c1b2d303588d9bc6d8bc/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkRowLevelOperationsTestBase.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/eca7dc38a46df1919059c1b2d303588d9bc6d8bc/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkRowLevelOperationsTestBase.java#L118
DIRECTLY EXTRACTED OPERATION:
    sql("CREATE TABLE %s (%s) USING iceberg", tableName, schema);
    initTable();

    if (jsonData != null) {
      try {
        Dataset<Row> ds = toDS(schema, jsonData);
        ds.writeTo(tableName).append();
      } catch (NoSuchTableException e) {
        throw new RuntimeException("Failed to write data", e);
      }
    }
  }

PARAMS COUNT: 2
IS VOID METHOD: true
FRAGMENT LENGTH: 337
FRAGMENT LINE AVG SIZE: 25.923076923076923
DEPTHS:
1 2 2 2 3 4 4 4 4 3 2 1 1 
AREA: 33
AVG DEPTH: 2.5384615384615383
NUMBER OF LINES IN FRAGMENT: 13
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private toDS(schema String, jsonData String) : Dataset<Row> extracted from protected createOrReplaceView(name String, schema String, jsonData String) : void in class org.apache.iceberg.spark.extensions.SparkRowLevelOperationsTestBase
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/eca7dc38a46df1919059c1b2d303588d9bc6d8bc/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkRowLevelOperationsTestBase.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/eca7dc38a46df1919059c1b2d303588d9bc6d8bc/spark3-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkRowLevelOperationsTestBase.java#L141
DIRECTLY EXTRACTED OPERATION:
    List<String> jsonRows = Arrays.stream(jsonData.split("\n"))
        .filter(str -> str.trim().length() > 0)
        .collect(Collectors.toList());
    Dataset<String> jsonDS = spark.createDataset(jsonRows, Encoders.STRING());

    if (schema != null) {
      return spark.read().schema(schema).json(jsonDS);
    } else {
      return spark.read().json(jsonDS);
    }
  }
}
PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 377
FRAGMENT LINE AVG SIZE: 31.416666666666668
DEPTHS:
1 2 2 2 2 2 3 3 3 2 1 0 
AREA: 23
AVG DEPTH: 1.9166666666666667
NUMBER OF LINES IN FRAGMENT: 12
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: c75ac359c1de6bf9fd4894b40009c5c42d2fee9d
URL: https://github.com/apache/incubator-iceberg/commit/c75ac359c1de6bf9fd4894b40009c5c42d2fee9d
DESCRIPTION: Extract Method	protected warehouseRoot() : String extracted from public FlinkCatalogTestBase(catalogName String, baseNamespace Namespace) in class org.apache.iceberg.flink.FlinkCatalogTestBase
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/c75ac359c1de6bf9fd4894b40009c5c42d2fee9d/flink/src/test/java/org/apache/iceberg/flink/FlinkCatalogTestBase.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/c75ac359c1de6bf9fd4894b40009c5c42d2fee9d/flink/src/test/java/org/apache/iceberg/flink/FlinkCatalogTestBase.java#L114
DIRECTLY EXTRACTED OPERATION:
    if (isHadoopCatalog) {
      return hadoopWarehouse.getRoot().getAbsolutePath();
    } else {
      return hiveWarehouse.getRoot().getAbsolutePath();
    }
  }

IS VOID METHOD: false
FRAGMENT LENGTH: 165
FRAGMENT LINE AVG SIZE: 23.571428571428573
DEPTHS:
2 3 3 3 2 1 1 
AREA: 15
AVG DEPTH: 2.142857142857143
NUMBER OF LINES IN FRAGMENT: 7
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 01c954ebf9442933b4cc580cc029d2aedd008e79
URL: https://github.com/apache/incubator-iceberg/commit/01c954ebf9442933b4cc580cc029d2aedd008e79
DESCRIPTION: Extract Method	public convert(fieldSchemas List<FieldSchema>, autoConvert boolean) : Schema extracted from public convert(fieldSchemas List<FieldSchema>) : Schema in class org.apache.iceberg.hive.HiveSchemaUtil
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/01c954ebf9442933b4cc580cc029d2aedd008e79/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveSchemaUtil.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/01c954ebf9442933b4cc580cc029d2aedd008e79/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveSchemaUtil.java#L60
DIRECTLY EXTRACTED OPERATION:
   * Converts a Hive schema (list of FieldSchema objects) to an Iceberg schema.
   * @param fieldSchemas The list of the columns
   * @param autoConvert If <code>true</code> then TINYINT and SMALLINT is converted to INTEGER and VARCHAR and CHAR is
   *                    converted to STRING. Otherwise if these types are used in the Hive schema then exception is
   *                    thrown.
   * @return An equivalent Iceberg Schema
   */
  public static Schema convert(List<FieldSchema> fieldSchemas, boolean autoConvert) {
    List<String> names = new ArrayList<>(fieldSchemas.size());
    List<TypeInfo> typeInfos = new ArrayList<>(fieldSchemas.size());

    for (FieldSchema col : fieldSchemas) {
      names.add(col.getName());
      typeInfos.add(TypeInfoUtils.getTypeInfoFromTypeString(col.getType()));
    }

    return HiveSchemaConverter.convert(names, typeInfos, autoConvert);
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 898
FRAGMENT LINE AVG SIZE: 47.26315789473684
DEPTHS:
0 1 1 1 1 1 1 1 2 2 2 2 3 3 2 2 2 1 1 
AREA: 29
AVG DEPTH: 1.5263157894736843
NUMBER OF LINES IN FRAGMENT: 19
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public convert(names List<String>, types List<TypeInfo>, autoConvert boolean) : Schema extracted from public convert(names List<String>, types List<TypeInfo>) : Schema in class org.apache.iceberg.hive.HiveSchemaUtil
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/01c954ebf9442933b4cc580cc029d2aedd008e79/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveSchemaUtil.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/01c954ebf9442933b4cc580cc029d2aedd008e79/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveSchemaUtil.java#L103
DIRECTLY EXTRACTED OPERATION:
   * Converts the Hive list of column names and column types to an Iceberg schema.
   * @param names The list of the Hive column names
   * @param types The list of the Hive column types
   * @param autoConvert If <code>true</code> then TINYINT and SMALLINT is converted to INTEGER and VARCHAR and CHAR is
   *                    converted to STRING. Otherwise if these types are used in the Hive schema then exception is
   *                    thrown.
   * @return The Iceberg schema
   */
  public static Schema convert(List<String> names, List<TypeInfo> types, boolean autoConvert) {
    return HiveSchemaConverter.convert(names, types, autoConvert);
  }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 660
FRAGMENT LINE AVG SIZE: 55.0
DEPTHS:
0 1 1 1 1 1 1 1 1 2 1 1 
AREA: 12
AVG DEPTH: 1.0
NUMBER OF LINES IN FRAGMENT: 12
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 3a7ac73601e958bae656c9e2185307fe11d7735c
URL: https://github.com/apache/incubator-iceberg/commit/3a7ac73601e958bae656c9e2185307fe11d7735c
DESCRIPTION: Extract Method	package overlayTableProperties(configuration Configuration, tableDesc TableDesc, map Map<String,String>) : void extracted from public configureInputJobProperties(tableDesc TableDesc, map Map<String,String>) : void in class org.apache.iceberg.mr.hive.HiveIcebergStorageHandler
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/3a7ac73601e958bae656c9e2185307fe11d7735c/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandler.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/3a7ac73601e958bae656c9e2185307fe11d7735c/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandler.java#L194
DIRECTLY EXTRACTED OPERATION:
   * Stores the serializable table data in the configuration.
   * Currently the following is handled:
   * <ul>
   *   <li>- Table - in case the table is serializable</li>
   *   <li>- Location</li>
   *   <li>- Schema</li>
   *   <li>- Partition specification</li>
   *   <li>- FileIO for handling table files</li>
   *   <li>- Location provider used for file generation</li>
   *   <li>- Encryption manager for encryption handling</li>
   * </ul>
   * @param configuration The configuration storing the catalog information
   * @param tableDesc The table which we want to store to the configuration
   * @param map The map of the configuration properties which we append with the serialized data
   */
  @VisibleForTesting
  static void overlayTableProperties(Configuration configuration, TableDesc tableDesc, Map<String, String> map) {
    Properties props = tableDesc.getProperties();
    Table table = Catalogs.loadTable(configuration, props);
    String schemaJson = SchemaParser.toJson(table.schema());

    Maps.fromProperties(props).entrySet().stream()
        .filter(entry -> !map.containsKey(entry.getKey())) // map overrides tableDesc properties
        .forEach(entry -> map.put(entry.getKey(), entry.getValue()));

    map.put(InputFormatConfig.TABLE_IDENTIFIER, props.getProperty(Catalogs.NAME));
    map.put(InputFormatConfig.TABLE_LOCATION, table.location());
    map.put(InputFormatConfig.TABLE_SCHEMA, schemaJson);
    map.put(InputFormatConfig.PARTITION_SPEC, PartitionSpecParser.toJson(table.spec()));
    String formatString = PropertyUtil.propertyAsString(table.properties(), DEFAULT_FILE_FORMAT,
        DEFAULT_FILE_FORMAT_DEFAULT);
    map.put(InputFormatConfig.WRITE_FILE_FORMAT, formatString.toUpperCase(Locale.ENGLISH));
    map.put(InputFormatConfig.WRITE_TARGET_FILE_SIZE,
        table.properties().getOrDefault(WRITE_TARGET_FILE_SIZE_BYTES,
            String.valueOf(WRITE_TARGET_FILE_SIZE_BYTES_DEFAULT)));

    if (table instanceof Serializable) {
      map.put(InputFormatConfig.SERIALIZED_TABLE, SerializationUtil.serializeToBase64(table));
    }

    map.put(InputFormatConfig.FILE_IO, SerializationUtil.serializeToBase64(table.io()));
    map.put(InputFormatConfig.LOCATION_PROVIDER, SerializationUtil.serializeToBase64(table.locationProvider()));
    map.put(InputFormatConfig.ENCRYPTION_MANAGER, SerializationUtil.serializeToBase64(table.encryption()));
    // We need to remove this otherwise the job.xml will be invalid as column comments are separated with '\0' and
    // the serialization utils fail to serialize this character
    map.remove("columns.comments");

    // save schema into table props as well to avoid repeatedly hitting the HMS during serde initializations
    // this is an exception to the interface documentation, but it's a safe operation to add this property
    props.put(InputFormatConfig.TABLE_SCHEMA, schemaJson);
  }
}
PARAMS COUNT: 3
IS VOID METHOD: true
FRAGMENT LENGTH: 2895
FRAGMENT LINE AVG SIZE: 55.67307692307692
DEPTHS:
0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 1 0 
AREA: 84
AVG DEPTH: 1.6153846153846154
NUMBER OF LINES IN FRAGMENT: 52
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 14331c4e5f61e14cdc527a567f5dafc7fd95c3e7
URL: https://github.com/apache/incubator-iceberg/commit/14331c4e5f61e14cdc527a567f5dafc7fd95c3e7
DESCRIPTION: Extract Method	protected exec(query String, args Object...) : TableResult extracted from protected sql(query String, args Object...) : List<Object[]> in class org.apache.iceberg.flink.FlinkTestBase
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/14331c4e5f61e14cdc527a567f5dafc7fd95c3e7/flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/14331c4e5f61e14cdc527a567f5dafc7fd95c3e7/flink/src/test/java/org/apache/iceberg/flink/FlinkTestBase.java#L79
DIRECTLY EXTRACTED OPERATION:
    return exec(getTableEnv(), query, args);
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 50
FRAGMENT LINE AVG SIZE: 16.666666666666668
DEPTHS:
1 1 1 
AREA: 3
AVG DEPTH: 1.0
NUMBER OF LINES IN FRAGMENT: 3
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 01fca3d0a3c5653fe5a4a2a88c29aeaffca33f1d
URL: https://github.com/apache/incubator-iceberg/commit/01fca3d0a3c5653fe5a4a2a88c29aeaffca33f1d
DESCRIPTION: Extract Method	private isTableDir(path Path) : boolean extracted from public listTables(namespace Namespace) : List<TableIdentifier> in class org.apache.iceberg.hadoop.HadoopCatalog
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/01fca3d0a3c5653fe5a4a2a88c29aeaffca33f1d/core/src/main/java/org/apache/iceberg/hadoop/HadoopCatalog.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/01fca3d0a3c5653fe5a4a2a88c29aeaffca33f1d/core/src/main/java/org/apache/iceberg/hadoop/HadoopCatalog.java#L178
DIRECTLY EXTRACTED OPERATION:
    Path metadataPath = new Path(path, "metadata");
    // Only the path which contains metadata is the path for table, otherwise it could be
    // still a namespace.
    try {
      return fs.listStatus(metadataPath, TABLE_FILTER).length >= 1;
    } catch (FileNotFoundException e) {
      return false;
    } catch (IOException e) {
      if (shouldSuppressPermissionError(e)) {
        LOG.warn("Unable to list metadata directory {}: {}", metadataPath, e);
        return false;
      } else {
        throw new UncheckedIOException(e);
      }
    }
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 560
FRAGMENT LINE AVG SIZE: 32.94117647058823
DEPTHS:
1 2 2 2 3 3 3 3 3 4 4 4 4 3 2 1 1 
AREA: 45
AVG DEPTH: 2.6470588235294117
NUMBER OF LINES IN FRAGMENT: 17
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private isTableDir(path Path) : boolean extracted from private isNamespace(path Path) : boolean in class org.apache.iceberg.hadoop.HadoopCatalog
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/01fca3d0a3c5653fe5a4a2a88c29aeaffca33f1d/core/src/main/java/org/apache/iceberg/hadoop/HadoopCatalog.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/01fca3d0a3c5653fe5a4a2a88c29aeaffca33f1d/core/src/main/java/org/apache/iceberg/hadoop/HadoopCatalog.java#L178
DIRECTLY EXTRACTED OPERATION:
    Path metadataPath = new Path(path, "metadata");
    // Only the path which contains metadata is the path for table, otherwise it could be
    // still a namespace.
    try {
      return fs.listStatus(metadataPath, TABLE_FILTER).length >= 1;
    } catch (FileNotFoundException e) {
      return false;
    } catch (IOException e) {
      if (shouldSuppressPermissionError(e)) {
        LOG.warn("Unable to list metadata directory {}: {}", metadataPath, e);
        return false;
      } else {
        throw new UncheckedIOException(e);
      }
    }
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 560
FRAGMENT LINE AVG SIZE: 32.94117647058823
DEPTHS:
1 2 2 2 3 3 3 3 3 4 4 4 4 3 2 1 1 
AREA: 45
AVG DEPTH: 2.6470588235294117
NUMBER OF LINES IN FRAGMENT: 17
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 78495a20bfc207e4f9c3f6cae2b9be14923f3562
URL: https://github.com/apache/incubator-iceberg/commit/78495a20bfc207e4f9c3f6cae2b9be14923f3562
DESCRIPTION: Extract Method	package write(file File, schema Schema, properties Map<String,String>, createWriterFunc Function<MessageType,ParquetValueWriter<?>>, records GenericData.Record...) : long extracted from package writeRecords(temp TemporaryFolder, schema Schema, properties Map<String,String>, createWriterFunc Function<MessageType,ParquetValueWriter<?>>, records GenericData.Record...) : File in class org.apache.iceberg.parquet.ParquetWritingTestUtils
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/78495a20bfc207e4f9c3f6cae2b9be14923f3562/parquet/src/test/java/org/apache/iceberg/parquet/ParquetWritingTestUtils.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/78495a20bfc207e4f9c3f6cae2b9be14923f3562/parquet/src/test/java/org/apache/iceberg/parquet/ParquetWritingTestUtils.java#L66
DIRECTLY EXTRACTED OPERATION:
                    Function<MessageType, ParquetValueWriter<?>> createWriterFunc,
                    GenericData.Record... records) throws IOException {

    long len = 0;

    FileAppender<GenericData.Record> writer = Parquet.write(localOutput(file))
            .schema(schema)
            .setAll(properties)
            .createWriterFunc(createWriterFunc)
            .build();

    try (Closeable toClose = writer) {
      writer.addAll(Lists.newArrayList(records));
      len = writer.length(); // in deprecated adapter we need to get the length first and then close the writer
    }

    if (writer instanceof ParquetWriter) {
      len = writer.length();
    }
    return len;
  }

PARAMS COUNT: 5
IS VOID METHOD: false
FRAGMENT LENGTH: 692
FRAGMENT LINE AVG SIZE: 31.454545454545453
DEPTHS:
0 1 2 2 2 2 2 2 2 2 2 2 3 3 2 2 2 3 2 2 1 1 
AREA: 42
AVG DEPTH: 1.9090909090909092
NUMBER OF LINES IN FRAGMENT: 22
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	package createTempFile(temp TemporaryFolder) : File extracted from package writeRecords(temp TemporaryFolder, schema Schema, properties Map<String,String>, createWriterFunc Function<MessageType,ParquetValueWriter<?>>, records GenericData.Record...) : File in class org.apache.iceberg.parquet.ParquetWritingTestUtils
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/78495a20bfc207e4f9c3f6cae2b9be14923f3562/parquet/src/test/java/org/apache/iceberg/parquet/ParquetWritingTestUtils.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/78495a20bfc207e4f9c3f6cae2b9be14923f3562/parquet/src/test/java/org/apache/iceberg/parquet/ParquetWritingTestUtils.java#L89
DIRECTLY EXTRACTED OPERATION:
    File tmpFolder = temp.newFolder("parquet");
    String filename = UUID.randomUUID().toString();
    return new File(tmpFolder, FileFormat.PARQUET.addExtension(filename));
  }
}
PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 181
FRAGMENT LINE AVG SIZE: 36.2
DEPTHS:
1 2 2 1 0 
AREA: 6
AVG DEPTH: 1.2
NUMBER OF LINES IN FRAGMENT: 5
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 4eb2010b3d51f63a6bfdafa9173f00ca354e66aa
URL: https://github.com/apache/incubator-iceberg/commit/4eb2010b3d51f63a6bfdafa9173f00ca354e66aa
DESCRIPTION: Extract Method	private cleanupMetadataAndUnlock(exceptionThrown boolean, metadataLocation String) : void extracted from protected doCommit(base TableMetadata, metadata TableMetadata) : void in class org.apache.iceberg.aws.glue.GlueTableOperations
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/4eb2010b3d51f63a6bfdafa9173f00ca354e66aa/aws/src/main/java/org/apache/iceberg/aws/glue/GlueTableOperations.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/4eb2010b3d51f63a6bfdafa9173f00ca354e66aa/aws/src/main/java/org/apache/iceberg/aws/glue/GlueTableOperations.java#L194
DIRECTLY EXTRACTED OPERATION:
    try {
      if (exceptionThrown) {
        // if anything went wrong, clean up the uncommitted metadata file
        io().deleteFile(metadataLocation);
      }
    } catch (RuntimeException e) {
      LOG.error("Fail to cleanup metadata file at {}", metadataLocation, e);
      throw e;
    } finally {
      lockManager.release(commitLockEntityId, metadataLocation);
    }
  }
}
PARAMS COUNT: 2
IS VOID METHOD: true
FRAGMENT LENGTH: 384
FRAGMENT LINE AVG SIZE: 29.53846153846154
DEPTHS:
2 3 4 4 3 3 3 3 3 3 2 1 0 
AREA: 34
AVG DEPTH: 2.6153846153846154
NUMBER OF LINES IN FRAGMENT: 13
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 6731211eaf746c6a4abfe71386ef53172a3fc137
URL: https://github.com/apache/incubator-iceberg/commit/6731211eaf746c6a4abfe71386ef53172a3fc137
DESCRIPTION: Extract Method	public initialize(name String, properties Map<String,String>) : void extracted from public HadoopCatalog(name String, conf Configuration, warehouseLocation String, properties Map<String,String>) in class org.apache.iceberg.hadoop.HadoopCatalog
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/6731211eaf746c6a4abfe71386ef53172a3fc137/core/src/main/java/org/apache/iceberg/hadoop/HadoopCatalog.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/6731211eaf746c6a4abfe71386ef53172a3fc137/core/src/main/java/org/apache/iceberg/hadoop/HadoopCatalog.java#L122
DIRECTLY EXTRACTED OPERATION:
  public void initialize(String name, Map<String, String> properties) {
    String inputWarehouseLocation = properties.get(CatalogProperties.WAREHOUSE_LOCATION);
    Preconditions.checkArgument(inputWarehouseLocation != null && !inputWarehouseLocation.equals(""),
        "Cannot instantiate hadoop catalog. No location provided for warehouse (Set warehouse config)");
    this.catalogName = name;
    this.warehouseLocation = inputWarehouseLocation.replaceAll("/*$", "");
    this.fs = Util.getFs(new Path(warehouseLocation), conf);

    String fileIOImpl = properties.get(CatalogProperties.FILE_IO_IMPL);
    this.fileIO = fileIOImpl == null ? new HadoopFileIO(conf) : CatalogUtil.loadFileIO(fileIOImpl, properties, conf);
  }

PARAMS COUNT: 2
IS VOID METHOD: true
FRAGMENT LENGTH: 730
FRAGMENT LINE AVG SIZE: 60.833333333333336
DEPTHS:
1 2 2 2 2 2 2 2 2 2 1 1 
AREA: 21
AVG DEPTH: 1.75
NUMBER OF LINES IN FRAGMENT: 12
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public initialize(inputName String, properties Map<String,String>) : void extracted from public HiveCatalog(name String, uri String, warehouse String, clientPoolSize int, conf Configuration, properties Map<String,String>) in class org.apache.iceberg.hive.HiveCatalog
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/6731211eaf746c6a4abfe71386ef53172a3fc137/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveCatalog.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/6731211eaf746c6a4abfe71386ef53172a3fc137/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveCatalog.java#L150
DIRECTLY EXTRACTED OPERATION:
  public void initialize(String inputName, Map<String, String> properties) {
    this.name = inputName;
    if (properties.containsKey(CatalogProperties.HIVE_URI)) {
      this.conf.set(HiveConf.ConfVars.METASTOREURIS.varname, properties.get(CatalogProperties.HIVE_URI));
    }

    if (properties.containsKey(CatalogProperties.WAREHOUSE_LOCATION)) {
      this.conf.set(HiveConf.ConfVars.METASTOREWAREHOUSE.varname, properties.get(CatalogProperties.WAREHOUSE_LOCATION));
    }

    int clientPoolSize = Integer.parseInt(
        properties.getOrDefault(CatalogProperties.HIVE_CLIENT_POOL_SIZE, "5"));
    this.clients = new HiveClientPool(clientPoolSize, this.conf);
    this.createStack = Thread.currentThread().getStackTrace();
    this.closed = false;

    String fileIOImpl = properties.get(CatalogProperties.FILE_IO_IMPL);
    this.fileIO = fileIOImpl == null ? new HadoopFileIO(conf) : CatalogUtil.loadFileIO(fileIOImpl, properties, conf);
  }

PARAMS COUNT: 2
IS VOID METHOD: true
FRAGMENT LENGTH: 952
FRAGMENT LINE AVG SIZE: 47.6
DEPTHS:
1 2 2 3 2 2 2 3 2 2 2 2 2 2 2 2 2 2 1 1 
AREA: 39
AVG DEPTH: 1.95
NUMBER OF LINES IN FRAGMENT: 20
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public setConf(conf Configuration) : void extracted from public HiveCatalog(name String, uri String, warehouse String, clientPoolSize int, conf Configuration, properties Map<String,String>) in class org.apache.iceberg.hive.HiveCatalog
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/6731211eaf746c6a4abfe71386ef53172a3fc137/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveCatalog.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/6731211eaf746c6a4abfe71386ef53172a3fc137/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveCatalog.java#L597
DIRECTLY EXTRACTED OPERATION:
  public void setConf(Configuration conf) {
    this.conf = new Configuration(conf);
  }

PARAMS COUNT: 1
IS VOID METHOD: true
FRAGMENT LENGTH: 90
FRAGMENT LINE AVG SIZE: 22.5
DEPTHS:
1 2 1 1 
AREA: 5
AVG DEPTH: 1.25
NUMBER OF LINES IN FRAGMENT: 4
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 6760b95e1997a3d43ce82f13e95944a7dda19d21
URL: https://github.com/apache/incubator-iceberg/commit/6760b95e1997a3d43ce82f13e95944a7dda19d21
DESCRIPTION: Extract Method	private cleanupMetadataAndUnlock(errorThrown boolean, metadataLocation String, lockId Optional<Long>) : void extracted from protected doCommit(base TableMetadata, metadata TableMetadata) : void in class org.apache.iceberg.hive.HiveTableOperations
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/6760b95e1997a3d43ce82f13e95944a7dda19d21/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/6760b95e1997a3d43ce82f13e95944a7dda19d21/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java#L366
DIRECTLY EXTRACTED OPERATION:
    try {
      if (errorThrown) {
        // if anything went wrong, clean up the uncommitted metadata file
        io().deleteFile(metadataLocation);
      }
    } catch (RuntimeException e) {
      LOG.error("Fail to cleanup metadata file at {}", metadataLocation, e);
      throw e;
    } finally {
      unlock(lockId);
    }
  }

PARAMS COUNT: 3
IS VOID METHOD: true
FRAGMENT LENGTH: 336
FRAGMENT LINE AVG SIZE: 25.846153846153847
DEPTHS:
2 3 4 4 3 3 3 3 3 3 2 1 1 
AREA: 35
AVG DEPTH: 2.6923076923076925
NUMBER OF LINES IN FRAGMENT: 13
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 545316ac274aae29903e924d2eb4c7faa3f9a177
URL: https://github.com/apache/incubator-iceberg/commit/545316ac274aae29903e924d2eb4c7faa3f9a177
DESCRIPTION: Extract Method	private initializeFileIO(properties Map<String,String>) : FileIO extracted from public initialize(name String, properties Map<String,String>) : void in class org.apache.iceberg.aws.glue.GlueCatalog
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/545316ac274aae29903e924d2eb4c7faa3f9a177/aws/src/main/java/org/apache/iceberg/aws/glue/GlueCatalog.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/545316ac274aae29903e924d2eb4c7faa3f9a177/aws/src/main/java/org/apache/iceberg/aws/glue/GlueCatalog.java#L100
DIRECTLY EXTRACTED OPERATION:
    String fileIOImpl = properties.get(CatalogProperties.FILE_IO_IMPL);
    if (fileIOImpl == null) {
      FileIO io = new S3FileIO();
      io.initialize(properties);
      return io;
    } else {
      return CatalogUtil.loadFileIO(fileIOImpl, properties, hadoopConf);
    }
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 283
FRAGMENT LINE AVG SIZE: 28.3
DEPTHS:
1 2 3 3 3 3 3 2 1 1 
AREA: 22
AVG DEPTH: 2.2
NUMBER OF LINES IN FRAGMENT: 10
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 05752965f5eb453d895db44fb2d072d270646644
URL: https://github.com/apache/incubator-iceberg/commit/05752965f5eb453d895db44fb2d072d270646644
DESCRIPTION: Extract Method	public validateData(expected List<Record>, actual List<Record>, sortBy int) : void extracted from public validateData(table Table, expected List<Record>, sortBy int) : void in class org.apache.iceberg.mr.hive.HiveIcebergTestUtils
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/05752965f5eb453d895db44fb2d072d270646644/mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergTestUtils.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/05752965f5eb453d895db44fb2d072d270646644/mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergTestUtils.java#L241
DIRECTLY EXTRACTED OPERATION:
   * Validates whether the 2 sets of records are the same. The results should be sorted by a unique key so we do
   * not end up with flaky tests.
   * @param expected The expected list of Records (The list will be sorted)
   * @param actual The actual list of Records (The list will be sorted)
   * @param sortBy The column position by which we will sort
   */
  public static void validateData(List<Record> expected, List<Record> actual, int sortBy) {
    // Sort based on the specified column
    expected.sort(Comparator.comparingLong(record -> (Long) record.get(sortBy)));
    actual.sort(Comparator.comparingLong(record -> (Long) record.get(sortBy)));

    Assert.assertEquals(expected.size(), actual.size());
    for (int i = 0; i < expected.size(); ++i) {
      assertEquals(expected.get(i), actual.get(i));
    }
  }

PARAMS COUNT: 3
IS VOID METHOD: true
FRAGMENT LENGTH: 827
FRAGMENT LINE AVG SIZE: 48.64705882352941
DEPTHS:
0 1 1 1 1 1 1 2 2 2 2 2 2 3 2 1 1 
AREA: 25
AVG DEPTH: 1.4705882352941178
NUMBER OF LINES IN FRAGMENT: 17
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private runCreateAndReadTest(identifier TableIdentifier, createSQL String, expectedSchema Schema, expectedSpec PartitionSpec, data Map<StructLike,List<Record>>) : void extracted from public testCreateTableWithColumnSpecification() : void in class org.apache.iceberg.mr.hive.TestHiveIcebergStorageHandler
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/05752965f5eb453d895db44fb2d072d270646644/mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandler.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/05752965f5eb453d895db44fb2d072d270646644/mr/src/test/java/org/apache/iceberg/mr/hive/TestHiveIcebergStorageHandler.java#L1042
DIRECTLY EXTRACTED OPERATION:
      PartitionSpec expectedSpec, Map<StructLike, List<Record>> data) throws IOException {
    shell.executeStatement(createSQL);

    org.apache.iceberg.Table icebergTable = testTables.loadTable(identifier);
    Assert.assertEquals(expectedSchema.asStruct(), icebergTable.schema().asStruct());
    Assert.assertEquals(expectedSpec, icebergTable.spec());

    List<Record> expected = Lists.newArrayList();
    for (StructLike partition : data.keySet()) {
      testTables.appendIcebergTable(shell.getHiveConf(), icebergTable, fileFormat, partition, data.get(partition));
      expected.addAll(data.get(partition));
    }

    List<Object[]> descRows = shell.executeStatement("SELECT * FROM " + identifier.toString() +
        " ORDER BY " + expectedSchema.columns().get(0).name() + " DESC");
    List<Record> records = HiveIcebergTestUtils.valueForRow(icebergTable.schema(), descRows);

    HiveIcebergTestUtils.validateData(expected, records, 0);
  }
}
PARAMS COUNT: 5
IS VOID METHOD: true
FRAGMENT LENGTH: 954
FRAGMENT LINE AVG SIZE: 47.7
DEPTHS:
1 2 2 2 2 2 2 2 2 3 3 2 2 2 2 2 2 2 1 0 
AREA: 38
AVG DEPTH: 1.9
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 77c5617c102c2ab27fbf35cac3fd75380a887d5d
URL: https://github.com/apache/incubator-iceberg/commit/77c5617c102c2ab27fbf35cac3fd75380a887d5d
DESCRIPTION: Extract Method	private convertToRecords(rows List<RowData>) : List<Record> extracted from public assertTableRows(tablePath String, expected List<RowData>) : void in class org.apache.iceberg.flink.SimpleDataUtil
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/77c5617c102c2ab27fbf35cac3fd75380a887d5d/flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/77c5617c102c2ab27fbf35cac3fd75380a887d5d/flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java#L170
DIRECTLY EXTRACTED OPERATION:
    List<Record> records = Lists.newArrayList();
    for (RowData row : rows) {
      Integer id = row.isNullAt(0) ? null : row.getInt(0);
      String data = row.isNullAt(1) ? null : row.getString(1).toString();
      records.add(createRecord(id, data));
    }
    return records;
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 287
FRAGMENT LINE AVG SIZE: 31.88888888888889
DEPTHS:
1 2 3 3 3 2 2 1 1 
AREA: 18
AVG DEPTH: 2.0
NUMBER OF LINES IN FRAGMENT: 9
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: ab6a5e9ebf2f580fb7be21fb00f1f543726f43eb
URL: https://github.com/apache/incubator-iceberg/commit/ab6a5e9ebf2f580fb7be21fb00f1f543726f43eb
DESCRIPTION: Extract Method	package add(sourceId int, fieldId int, name String, transform Transform<?,?>) : Builder extracted from package add(sourceId int, fieldId int, name String, transform String) : Builder in class org.apache.iceberg.PartitionSpec.Builder
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/ab6a5e9ebf2f580fb7be21fb00f1f543726f43eb/api/src/main/java/org/apache/iceberg/PartitionSpec.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/ab6a5e9ebf2f580fb7be21fb00f1f543726f43eb/api/src/main/java/org/apache/iceberg/PartitionSpec.java#L488
DIRECTLY EXTRACTED OPERATION:
      checkAndAddPartitionName(name, sourceId);
      fields.add(new PartitionField(sourceId, fieldId, name, transform));
      lastAssignedFieldId.getAndAccumulate(fieldId, Math::max);
      return this;
    }

PARAMS COUNT: 4
IS VOID METHOD: false
FRAGMENT LENGTH: 212
FRAGMENT LINE AVG SIZE: 35.333333333333336
DEPTHS:
2 3 3 3 2 2 
AREA: 15
AVG DEPTH: 2.5
NUMBER OF LINES IN FRAGMENT: 6
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public visit(schema Schema, field PartitionField, visitor PartitionSpecVisitor<R>) : R extracted from public visit(schema Schema, spec PartitionSpec, visitor PartitionSpecVisitor<R>) : List<R> in class org.apache.iceberg.transforms.PartitionSpecVisitor
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/ab6a5e9ebf2f580fb7be21fb00f1f543726f43eb/api/src/main/java/org/apache/iceberg/transforms/PartitionSpecVisitor.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/ab6a5e9ebf2f580fb7be21fb00f1f543726f43eb/api/src/main/java/org/apache/iceberg/transforms/PartitionSpecVisitor.java#L126
DIRECTLY EXTRACTED OPERATION:
  static <R> R visit(Schema schema, PartitionField field, PartitionSpecVisitor<R> visitor) {
    String sourceName = schema.findColumnName(field.sourceId());
    Transform<?, ?> transform = field.transform();

    if (transform instanceof Identity) {
      return visitor.identity(field.fieldId(), sourceName, field.sourceId());
    } else if (transform instanceof Bucket) {
      int numBuckets = ((Bucket<?>) transform).numBuckets();
      return visitor.bucket(field.fieldId(), sourceName, field.sourceId(), numBuckets);
    } else if (transform instanceof Truncate) {
      int width = ((Truncate<?>) transform).width();
      return visitor.truncate(field.fieldId(), sourceName, field.sourceId(), width);
    } else if (transform == Dates.YEAR || transform == Timestamps.YEAR) {
      return visitor.year(field.fieldId(), sourceName, field.sourceId());
    } else if (transform == Dates.MONTH || transform == Timestamps.MONTH) {
      return visitor.month(field.fieldId(), sourceName, field.sourceId());
    } else if (transform == Dates.DAY || transform == Timestamps.DAY) {
      return visitor.day(field.fieldId(), sourceName, field.sourceId());
    } else if (transform == Timestamps.HOUR) {
      return visitor.hour(field.fieldId(), sourceName, field.sourceId());
    } else if (transform instanceof VoidTransform) {
      return visitor.alwaysNull(field.fieldId(), sourceName, field.sourceId());
    } else if (transform instanceof UnknownTransform) {
      return visitor.unknown(field.fieldId(), sourceName, field.sourceId(), transform.toString());
    }

    throw new UnsupportedOperationException(
        String.format("Unknown transform class %s", field.transform().getClass().getName()));
  }
}
PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 1715
FRAGMENT LINE AVG SIZE: 57.166666666666664
DEPTHS:
1 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 1 0 
AREA: 75
AVG DEPTH: 2.5
NUMBER OF LINES IN FRAGMENT: 30
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 94bf40a50a2c796314993b028fabdafdb8b2a697
URL: https://github.com/apache/incubator-iceberg/commit/94bf40a50a2c796314993b028fabdafdb8b2a697
DESCRIPTION: Extract Method	protected toCatalogAndIdentifier(identifierAsString String, argName String, catalog CatalogPlugin) : CatalogAndIdentifier extracted from protected toIdentifier(identifierAsString String, argName String) : Identifier in class org.apache.iceberg.spark.procedures.BaseProcedure
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/94bf40a50a2c796314993b028fabdafdb8b2a697/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/94bf40a50a2c796314993b028fabdafdb8b2a697/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java#L95
DIRECTLY EXTRACTED OPERATION:
                                                        CatalogPlugin catalog) {
    Preconditions.checkArgument(identifierAsString != null && !identifierAsString.isEmpty(),
        "Cannot handle an empty identifier for argument %s", argName);

    return Spark3Util.catalogAndIdentifier("identifier for arg " + argName, spark, identifierAsString, catalog);
  }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 364
FRAGMENT LINE AVG SIZE: 52.0
DEPTHS:
1 2 2 2 2 1 1 
AREA: 11
AVG DEPTH: 1.5714285714285714
NUMBER OF LINES IN FRAGMENT: 7
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 04e73deb7d68e3c4011101384f725abb1aae6236
URL: https://github.com/apache/incubator-iceberg/commit/04e73deb7d68e3c4011101384f725abb1aae6236
DESCRIPTION: Extract Method	private handleEqual(attribute String, value Object) : Expression extracted from public convert(filter Filter) : Expression in class org.apache.iceberg.spark.SparkFilters
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/04e73deb7d68e3c4011101384f725abb1aae6236/spark2/src/main/java/org/apache/iceberg/spark/SparkFilters.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/04e73deb7d68e3c4011101384f725abb1aae6236/spark2/src/main/java/org/apache/iceberg/spark/SparkFilters.java#L183
DIRECTLY EXTRACTED OPERATION:
    if (NaNUtil.isNaN(value)) {
      return isNaN(attribute);
    } else {
      return equal(attribute, convertLiteral(value));
    }
  }
}
PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 142
FRAGMENT LINE AVG SIZE: 20.285714285714285
DEPTHS:
2 3 3 3 2 1 0 
AREA: 14
AVG DEPTH: 2.0
NUMBER OF LINES IN FRAGMENT: 7
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private handleEqual(attribute String, value Object) : Expression extracted from public convert(filter Filter) : Expression in class org.apache.iceberg.spark.SparkFilters
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/04e73deb7d68e3c4011101384f725abb1aae6236/spark2/src/main/java/org/apache/iceberg/spark/SparkFilters.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/04e73deb7d68e3c4011101384f725abb1aae6236/spark2/src/main/java/org/apache/iceberg/spark/SparkFilters.java#L183
DIRECTLY EXTRACTED OPERATION:
    if (NaNUtil.isNaN(value)) {
      return isNaN(attribute);
    } else {
      return equal(attribute, convertLiteral(value));
    }
  }
}
PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 142
FRAGMENT LINE AVG SIZE: 20.285714285714285
DEPTHS:
2 3 3 3 2 1 0 
AREA: 14
AVG DEPTH: 2.0
NUMBER OF LINES IN FRAGMENT: 7
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private handleEqual(attribute String, value Object) : Expression extracted from public convert(filter Filter) : Expression in class org.apache.iceberg.spark.SparkFilters
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/04e73deb7d68e3c4011101384f725abb1aae6236/spark3/src/main/java/org/apache/iceberg/spark/SparkFilters.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/04e73deb7d68e3c4011101384f725abb1aae6236/spark3/src/main/java/org/apache/iceberg/spark/SparkFilters.java#L205
DIRECTLY EXTRACTED OPERATION:
    if (NaNUtil.isNaN(value)) {
      return isNaN(attribute);
    } else {
      return equal(attribute, convertLiteral(value));
    }
  }
}
PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 142
FRAGMENT LINE AVG SIZE: 20.285714285714285
DEPTHS:
2 3 3 3 2 1 0 
AREA: 14
AVG DEPTH: 2.0
NUMBER OF LINES IN FRAGMENT: 7
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private handleEqual(attribute String, value Object) : Expression extracted from public convert(filter Filter) : Expression in class org.apache.iceberg.spark.SparkFilters
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/04e73deb7d68e3c4011101384f725abb1aae6236/spark3/src/main/java/org/apache/iceberg/spark/SparkFilters.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/04e73deb7d68e3c4011101384f725abb1aae6236/spark3/src/main/java/org/apache/iceberg/spark/SparkFilters.java#L205
DIRECTLY EXTRACTED OPERATION:
    if (NaNUtil.isNaN(value)) {
      return isNaN(attribute);
    } else {
      return equal(attribute, convertLiteral(value));
    }
  }
}
PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 142
FRAGMENT LINE AVG SIZE: 20.285714285714285
DEPTHS:
2 3 3 3 2 1 0 
AREA: 14
AVG DEPTH: 2.0
NUMBER OF LINES IN FRAGMENT: 7
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 2512e7f0a1e9d1e4bc34098634d3cca0fe8ebea9
URL: https://github.com/apache/incubator-iceberg/commit/2512e7f0a1e9d1e4bc34098634d3cca0fe8ebea9
DESCRIPTION: Extract Method	private implClass() : String extracted from private actionConstructor() : DynConstructors.Ctor<Actions> in class org.apache.iceberg.actions.Actions
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/2512e7f0a1e9d1e4bc34098634d3cca0fe8ebea9/spark/src/main/java/org/apache/iceberg/actions/Actions.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/2512e7f0a1e9d1e4bc34098634d3cca0fe8ebea9/spark/src/main/java/org/apache/iceberg/actions/Actions.java#L36
DIRECTLY EXTRACTED OPERATION:
    return Actions.class.getPackage().getName() + "." + IMPL_NAME;
  }

IS VOID METHOD: false
FRAGMENT LENGTH: 72
FRAGMENT LINE AVG SIZE: 24.0
DEPTHS:
1 1 1 
AREA: 3
AVG DEPTH: 1.0
NUMBER OF LINES IN FRAGMENT: 3
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: e39584ed89356cacf006610bffd475a3ad3a1ac4
URL: https://github.com/apache/incubator-iceberg/commit/e39584ed89356cacf006610bffd475a3ad3a1ac4
DESCRIPTION: Extract Method	public catalogAndIdentifier(spark SparkSession, name String, defaultCatalog CatalogPlugin) : CatalogAndIdentifier extracted from public catalogAndIdentifier(spark SparkSession, name String) : CatalogAndIdentifier in class org.apache.iceberg.spark.Spark3Util
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e39584ed89356cacf006610bffd475a3ad3a1ac4/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e39584ed89356cacf006610bffd475a3ad3a1ac4/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java#L611
DIRECTLY EXTRACTED OPERATION:
                                                          CatalogPlugin defaultCatalog) throws ParseException {
    ParserInterface parser = spark.sessionState().sqlParser();
    Seq<String> multiPartIdentifier = parser.parseMultipartIdentifier(name);
    List<String> javaMultiPartIdentifier = JavaConverters.seqAsJavaList(multiPartIdentifier);
    return catalogAndIdentifier(spark, javaMultiPartIdentifier, defaultCatalog);
  }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 432
FRAGMENT LINE AVG SIZE: 61.714285714285715
DEPTHS:
1 2 2 2 2 1 1 
AREA: 11
AVG DEPTH: 1.5714285714285714
NUMBER OF LINES IN FRAGMENT: 7
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public catalogAndIdentifier(spark SparkSession, nameParts List<String>, defaultCatalog CatalogPlugin) : CatalogAndIdentifier extracted from public catalogAndIdentifier(spark SparkSession, nameParts List<String>) : CatalogAndIdentifier in class org.apache.iceberg.spark.Spark3Util
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e39584ed89356cacf006610bffd475a3ad3a1ac4/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e39584ed89356cacf006610bffd475a3ad3a1ac4/spark3/src/main/java/org/apache/iceberg/spark/Spark3Util.java#L632
DIRECTLY EXTRACTED OPERATION:
   * A modified version of Spark's LookupCatalog.CatalogAndIdentifier.unapply
   * Attempts to find the catalog and identifier a multipart identifier represents
   * @param spark Spark session to use for resolution
   * @param nameParts Multipart identifier representing a table
   * @param defaultCatalog Catalog to use if none is specified
   * @return The CatalogPlugin and Identifier for the table
   */
  public static CatalogAndIdentifier catalogAndIdentifier(SparkSession spark, List<String> nameParts,
                                                          CatalogPlugin defaultCatalog) {
    Preconditions.checkArgument(!nameParts.isEmpty(),
        "Cannot determine catalog and Identifier from empty name parts");
    CatalogManager catalogManager = spark.sessionState().catalogManager();
    int lastElementIndex = nameParts.size() - 1;
    String name = nameParts.get(lastElementIndex);
    String[] currentNamespace;
    if (defaultCatalog.equals(catalogManager.currentCatalog())) {
      currentNamespace = catalogManager.currentNamespace();
    } else {
      currentNamespace = defaultCatalog.defaultNamespace();
    }

    if (nameParts.size() == 1) {
      // Only a single element, use current catalog and namespace
      return new CatalogAndIdentifier(defaultCatalog, Identifier.of(currentNamespace, name));
    } else {
      try {
        // Assume the first element is a valid catalog
        CatalogPlugin namedCatalog = catalogManager.catalog(nameParts.get(0));
        String[] namespace = nameParts.subList(1, lastElementIndex).toArray(new String[0]);
        return new CatalogAndIdentifier(namedCatalog, Identifier.of(namespace, name));
      } catch (Exception e) {
        // The first element was not a valid catalog, treat it like part of the namespace
        String[] namespace =  nameParts.subList(0, lastElementIndex).toArray(new String[0]);
        return new CatalogAndIdentifier(defaultCatalog, Identifier.of(namespace, name));
      }
    }
  }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 1992
FRAGMENT LINE AVG SIZE: 52.421052631578945
DEPTHS:
0 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 3 3 3 2 2 2 3 3 3 3 4 4 4 4 4 4 4 4 3 2 1 1 
AREA: 88
AVG DEPTH: 2.3157894736842106
NUMBER OF LINES IN FRAGMENT: 38
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: f3dc93faece50e174287fbf4d00ffd115a54e179
URL: https://github.com/apache/incubator-iceberg/commit/f3dc93faece50e174287fbf4d00ffd115a54e179
DESCRIPTION: Extract Method	private load(ident Identifier) : Table extracted from public loadTable(ident Identifier) : SparkTable in class org.apache.iceberg.spark.SparkCatalog
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/f3dc93faece50e174287fbf4d00ffd115a54e179/spark3/src/main/java/org/apache/iceberg/spark/SparkCatalog.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/f3dc93faece50e174287fbf4d00ffd115a54e179/spark3/src/main/java/org/apache/iceberg/spark/SparkCatalog.java#L473
DIRECTLY EXTRACTED OPERATION:
    return isPathIdentifier(ident) ?
        tables.load(((PathIdentifier) ident).location()) :
        icebergCatalog.loadTable(buildIdentifier(ident));
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 159
FRAGMENT LINE AVG SIZE: 31.8
DEPTHS:
1 2 2 1 1 
AREA: 7
AVG DEPTH: 1.4
NUMBER OF LINES IN FRAGMENT: 5
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private load(ident Identifier) : Table extracted from public alterTable(ident Identifier, changes TableChange...) : SparkTable in class org.apache.iceberg.spark.SparkCatalog
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/f3dc93faece50e174287fbf4d00ffd115a54e179/spark3/src/main/java/org/apache/iceberg/spark/SparkCatalog.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/f3dc93faece50e174287fbf4d00ffd115a54e179/spark3/src/main/java/org/apache/iceberg/spark/SparkCatalog.java#L473
DIRECTLY EXTRACTED OPERATION:
    return isPathIdentifier(ident) ?
        tables.load(((PathIdentifier) ident).location()) :
        icebergCatalog.loadTable(buildIdentifier(ident));
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 159
FRAGMENT LINE AVG SIZE: 31.8
DEPTHS:
1 2 2 1 1 
AREA: 7
AVG DEPTH: 1.4
NUMBER OF LINES IN FRAGMENT: 5
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private load(ident Identifier) : Table extracted from public invalidateTable(ident Identifier) : void in class org.apache.iceberg.spark.SparkCatalog
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/f3dc93faece50e174287fbf4d00ffd115a54e179/spark3/src/main/java/org/apache/iceberg/spark/SparkCatalog.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/f3dc93faece50e174287fbf4d00ffd115a54e179/spark3/src/main/java/org/apache/iceberg/spark/SparkCatalog.java#L473
DIRECTLY EXTRACTED OPERATION:
    return isPathIdentifier(ident) ?
        tables.load(((PathIdentifier) ident).location()) :
        icebergCatalog.loadTable(buildIdentifier(ident));
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 159
FRAGMENT LINE AVG SIZE: 31.8
DEPTHS:
1 2 2 1 1 
AREA: 7
AVG DEPTH: 1.4
NUMBER OF LINES IN FRAGMENT: 5
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 969a1ffc8d7392cc4c54eba156647c448757b45e
URL: https://github.com/apache/incubator-iceberg/commit/969a1ffc8d7392cc4c54eba156647c448757b45e
DESCRIPTION: Extract Method	package convertType(typeInfo TypeInfo) : Type extracted from package convert(typeInfo TypeInfo) : Type in class org.apache.iceberg.hive.HiveSchemaConverter
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/969a1ffc8d7392cc4c54eba156647c448757b45e/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveSchemaConverter.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/969a1ffc8d7392cc4c54eba156647c448757b45e/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveSchemaConverter.java#L64
DIRECTLY EXTRACTED OPERATION:
    switch (typeInfo.getCategory()) {
      case PRIMITIVE:
        switch (((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory()) {
          case FLOAT:
            return Types.FloatType.get();
          case DOUBLE:
            return Types.DoubleType.get();
          case BOOLEAN:
            return Types.BooleanType.get();
          case BYTE:
          case SHORT:
            throw new IllegalArgumentException("Unsupported Hive type (" +
                ((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory() +
                ") for Iceberg tables. Consider using INT/INTEGER type instead.");
          case INT:
            return Types.IntegerType.get();
          case LONG:
            return Types.LongType.get();
          case BINARY:
            return Types.BinaryType.get();
          case CHAR:
          case VARCHAR:
            throw new IllegalArgumentException("Unsupported Hive type (" +
                ((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory() +
                ") for Iceberg tables. Consider using STRING type instead.");
          case STRING:
            return Types.StringType.get();
          case TIMESTAMP:
            return Types.TimestampType.withoutZone();
          case DATE:
            return Types.DateType.get();
          case DECIMAL:
            DecimalTypeInfo decimalTypeInfo = (DecimalTypeInfo) typeInfo;
            return Types.DecimalType.of(decimalTypeInfo.precision(), decimalTypeInfo.scale());
          case INTERVAL_YEAR_MONTH:
          case INTERVAL_DAY_TIME:
          default:
            throw new IllegalArgumentException("Unsupported Hive type (" +
                ((PrimitiveTypeInfo) typeInfo).getPrimitiveCategory() +
                ") for Iceberg tables.");
        }
      case STRUCT:
        StructTypeInfo structTypeInfo = (StructTypeInfo) typeInfo;
        List<Types.NestedField> fields =
            convertInternal(structTypeInfo.getAllStructFieldNames(), structTypeInfo.getAllStructFieldTypeInfos());
        return Types.StructType.of(fields);
      case MAP:
        MapTypeInfo mapTypeInfo = (MapTypeInfo) typeInfo;
        Type keyType = convertType(mapTypeInfo.getMapKeyTypeInfo());
        Type valueType = convertType(mapTypeInfo.getMapValueTypeInfo());
        int keyId = id++;
        int valueId = id++;
        return Types.MapType.ofOptional(keyId, valueId, keyType, valueType);
      case LIST:
        ListTypeInfo listTypeInfo = (ListTypeInfo) typeInfo;
        Type listType = convertType(listTypeInfo.getListElementTypeInfo());
        return Types.ListType.ofOptional(id++, listType);
      case UNION:
      default:
        throw new IllegalArgumentException("Unknown type " + typeInfo.getCategory());
    }
  }
}
PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 2731
FRAGMENT LINE AVG SIZE: 43.34920634920635
DEPTHS:
2 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 1 0 
AREA: 219
AVG DEPTH: 3.4761904761904763
NUMBER OF LINES IN FRAGMENT: 63
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 7383b9d70a8edd8b86e6066d6d60b6073989783f
URL: https://github.com/apache/incubator-iceberg/commit/7383b9d70a8edd8b86e6066d6d60b6073989783f
DESCRIPTION: Extract Method	public partitionedCreateWithTargetFileSizeViaOption(option IcebergOptionsType) : void extracted from public testPartitionedCreateWithTargetFileSizeViaOption() : void in class org.apache.iceberg.spark.source.TestSparkDataWrite
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/7383b9d70a8edd8b86e6066d6d60b6073989783f/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/7383b9d70a8edd8b86e6066d6d60b6073989783f/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java#L476
DIRECTLY EXTRACTED OPERATION:
      throws IOException {
    File parent = temp.newFolder(format.toString());
    File location = new File(parent, "test");

    HadoopTables tables = new HadoopTables(CONF);
    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity("data").build();
    Table table = tables.create(SCHEMA, spec, location.toString());

    List<SimpleRecord> expected = Lists.newArrayListWithCapacity(8000);
    for (int i = 0; i < 2000; i++) {
      expected.add(new SimpleRecord(i, "a"));
      expected.add(new SimpleRecord(i, "b"));
      expected.add(new SimpleRecord(i, "c"));
      expected.add(new SimpleRecord(i, "d"));
    }

    Dataset<Row> df = spark.createDataFrame(expected, SimpleRecord.class);

    switch (option) {
      case NONE:
        df.select("id", "data").sort("data").write()
            .format("iceberg")
            .option("write-format", format.toString())
            .mode("append")
            .option("target-file-size-bytes", 4) // ~4 bytes; low enough to trigger
            .save(location.toString());
        break;
      case TABLE:
        table.updateProperties().set(WRITE_PARTITIONED_FANOUT_ENABLED, "true").commit();
        df.select("id", "data").write()
            .format("iceberg")
            .option("write-format", format.toString())
            .mode("append")
            .option("target-file-size-bytes", 4) // ~4 bytes; low enough to trigger
            .save(location.toString());
        break;
      case JOB:
        df.select("id", "data").write()
            .format("iceberg")
            .option("write-format", format.toString())
            .mode("append")
            .option("target-file-size-bytes", 4) // ~4 bytes; low enough to trigger
            .option("partitioned.fanout.enabled", true)
            .save(location.toString());
        break;
      default:
        break;
    }

    table.refresh();

    Dataset<Row> result = spark.read()
        .format("iceberg")
        .load(location.toString());

    List<SimpleRecord> actual = result.orderBy("id").as(Encoders.bean(SimpleRecord.class)).collectAsList();
    Assert.assertEquals("Number of rows should match", expected.size(), actual.size());
    Assert.assertEquals("Result rows should match", expected, actual);

    List<DataFile> files = Lists.newArrayList();
    for (ManifestFile manifest : table.currentSnapshot().allManifests()) {
      for (DataFile file : ManifestFiles.read(manifest, table.io())) {
        files.add(file);
      }
    }
    // TODO: ORC file now not support target file size
    if (!format.equals(FileFormat.ORC)) {
      Assert.assertEquals("Should have 8 DataFiles", 8, files.size());
      Assert.assertTrue("All DataFiles contain 1000 rows", files.stream().allMatch(d -> d.recordCount() == 1000));
    }
  }

PARAMS COUNT: 1
IS VOID METHOD: true
FRAGMENT LENGTH: 2774
FRAGMENT LINE AVG SIZE: 38.52777777777778
DEPTHS:
1 2 2 2 2 2 2 2 2 2 3 3 3 3 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 4 3 2 2 2 3 3 2 1 1 
AREA: 179
AVG DEPTH: 2.486111111111111
NUMBER OF LINES IN FRAGMENT: 72
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: e69e52146d27956221ea4df4ad0baf2af7c827cd
URL: https://github.com/apache/incubator-iceberg/commit/e69e52146d27956221ea4df4ad0baf2af7c827cd
DESCRIPTION: Extract Method	private spec(schema Schema, properties Properties, hmsTable Table) : PartitionSpec extracted from public preCreateTable(hmsTable Table) : void in class org.apache.iceberg.mr.hive.HiveIcebergMetaHook
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e69e52146d27956221ea4df4ad0baf2af7c827cd/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e69e52146d27956221ea4df4ad0baf2af7c827cd/mr/src/main/java/org/apache/iceberg/mr/hive/HiveIcebergMetaHook.java#L203
DIRECTLY EXTRACTED OPERATION:
      org.apache.hadoop.hive.metastore.api.Table hmsTable) {

    Preconditions.checkArgument(hmsTable.getPartitionKeys() == null || hmsTable.getPartitionKeys().isEmpty(),
        "Partitioned Hive tables are currently not supported");

    if (properties.getProperty(InputFormatConfig.PARTITION_SPEC) != null) {
      return PartitionSpecParser.fromJson(schema, properties.getProperty(InputFormatConfig.PARTITION_SPEC));
    } else {
      return PartitionSpec.unpartitioned();
    }
  }
}
PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 491
FRAGMENT LINE AVG SIZE: 40.916666666666664
DEPTHS:
1 2 2 2 2 2 3 3 3 2 1 0 
AREA: 23
AVG DEPTH: 1.9166666666666667
NUMBER OF LINES IN FRAGMENT: 12
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private loadTable(identifier TableIdentifier) : Table extracted from public testCreateDropTable() : void in class org.apache.iceberg.mr.hive.HiveIcebergStorageHandlerBaseTest
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e69e52146d27956221ea4df4ad0baf2af7c827cd/mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e69e52146d27956221ea4df4ad0baf2af7c827cd/mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java#L1010
DIRECTLY EXTRACTED OPERATION:
    Properties properties = new Properties();
    properties.put(Catalogs.NAME, identifier.toString());
    String expectedLocation = testTables.loadLocation(identifier);
    if (expectedLocation != null) {
      properties.put(Catalogs.LOCATION, expectedLocation);
    }

    // Check the Iceberg table data
    return Catalogs.loadTable(shell.getHiveConf(), properties);
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 378
FRAGMENT LINE AVG SIZE: 34.36363636363637
DEPTHS:
1 2 2 2 3 2 2 2 2 1 1 
AREA: 20
AVG DEPTH: 1.8181818181818181
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private loadTable(identifier TableIdentifier) : Table extracted from public testCreateDropTable() : void in class org.apache.iceberg.mr.hive.HiveIcebergStorageHandlerBaseTest
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e69e52146d27956221ea4df4ad0baf2af7c827cd/mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e69e52146d27956221ea4df4ad0baf2af7c827cd/mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java#L1010
DIRECTLY EXTRACTED OPERATION:
    Properties properties = new Properties();
    properties.put(Catalogs.NAME, identifier.toString());
    String expectedLocation = testTables.loadLocation(identifier);
    if (expectedLocation != null) {
      properties.put(Catalogs.LOCATION, expectedLocation);
    }

    // Check the Iceberg table data
    return Catalogs.loadTable(shell.getHiveConf(), properties);
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 378
FRAGMENT LINE AVG SIZE: 34.36363636363637
DEPTHS:
1 2 2 2 3 2 2 2 2 1 1 
AREA: 20
AVG DEPTH: 1.8181818181818181
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private loadTable(identifier TableIdentifier) : Table extracted from public testCreateDropTable() : void in class org.apache.iceberg.mr.hive.HiveIcebergStorageHandlerBaseTest
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e69e52146d27956221ea4df4ad0baf2af7c827cd/mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e69e52146d27956221ea4df4ad0baf2af7c827cd/mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java#L1010
DIRECTLY EXTRACTED OPERATION:
    Properties properties = new Properties();
    properties.put(Catalogs.NAME, identifier.toString());
    String expectedLocation = testTables.loadLocation(identifier);
    if (expectedLocation != null) {
      properties.put(Catalogs.LOCATION, expectedLocation);
    }

    // Check the Iceberg table data
    return Catalogs.loadTable(shell.getHiveConf(), properties);
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 378
FRAGMENT LINE AVG SIZE: 34.36363636363637
DEPTHS:
1 2 2 2 3 2 2 2 2 1 1 
AREA: 20
AVG DEPTH: 1.8181818181818181
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private loadTable(identifier TableIdentifier) : Table extracted from public testCreateTableWithoutSpec() : void in class org.apache.iceberg.mr.hive.HiveIcebergStorageHandlerBaseTest
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e69e52146d27956221ea4df4ad0baf2af7c827cd/mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e69e52146d27956221ea4df4ad0baf2af7c827cd/mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java#L1010
DIRECTLY EXTRACTED OPERATION:
    Properties properties = new Properties();
    properties.put(Catalogs.NAME, identifier.toString());
    String expectedLocation = testTables.loadLocation(identifier);
    if (expectedLocation != null) {
      properties.put(Catalogs.LOCATION, expectedLocation);
    }

    // Check the Iceberg table data
    return Catalogs.loadTable(shell.getHiveConf(), properties);
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 378
FRAGMENT LINE AVG SIZE: 34.36363636363637
DEPTHS:
1 2 2 2 3 2 2 2 2 1 1 
AREA: 20
AVG DEPTH: 1.8181818181818181
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private loadTable(identifier TableIdentifier) : Table extracted from public testCreateTableWithUnpartitionedSpec() : void in class org.apache.iceberg.mr.hive.HiveIcebergStorageHandlerBaseTest
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e69e52146d27956221ea4df4ad0baf2af7c827cd/mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e69e52146d27956221ea4df4ad0baf2af7c827cd/mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java#L1010
DIRECTLY EXTRACTED OPERATION:
    Properties properties = new Properties();
    properties.put(Catalogs.NAME, identifier.toString());
    String expectedLocation = testTables.loadLocation(identifier);
    if (expectedLocation != null) {
      properties.put(Catalogs.LOCATION, expectedLocation);
    }

    // Check the Iceberg table data
    return Catalogs.loadTable(shell.getHiveConf(), properties);
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 378
FRAGMENT LINE AVG SIZE: 34.36363636363637
DEPTHS:
1 2 2 2 3 2 2 2 2 1 1 
AREA: 20
AVG DEPTH: 1.8181818181818181
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private loadTable(identifier TableIdentifier) : Table extracted from public testDeleteBackingTable() : void in class org.apache.iceberg.mr.hive.HiveIcebergStorageHandlerBaseTest
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e69e52146d27956221ea4df4ad0baf2af7c827cd/mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e69e52146d27956221ea4df4ad0baf2af7c827cd/mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java#L1010
DIRECTLY EXTRACTED OPERATION:
    Properties properties = new Properties();
    properties.put(Catalogs.NAME, identifier.toString());
    String expectedLocation = testTables.loadLocation(identifier);
    if (expectedLocation != null) {
      properties.put(Catalogs.LOCATION, expectedLocation);
    }

    // Check the Iceberg table data
    return Catalogs.loadTable(shell.getHiveConf(), properties);
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 378
FRAGMENT LINE AVG SIZE: 34.36363636363637
DEPTHS:
1 2 2 2 3 2 2 2 2 1 1 
AREA: 20
AVG DEPTH: 1.8181818181818181
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private loadTable(identifier TableIdentifier) : Table extracted from public testDeleteBackingTable() : void in class org.apache.iceberg.mr.hive.HiveIcebergStorageHandlerBaseTest
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e69e52146d27956221ea4df4ad0baf2af7c827cd/mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e69e52146d27956221ea4df4ad0baf2af7c827cd/mr/src/test/java/org/apache/iceberg/mr/hive/HiveIcebergStorageHandlerBaseTest.java#L1010
DIRECTLY EXTRACTED OPERATION:
    Properties properties = new Properties();
    properties.put(Catalogs.NAME, identifier.toString());
    String expectedLocation = testTables.loadLocation(identifier);
    if (expectedLocation != null) {
      properties.put(Catalogs.LOCATION, expectedLocation);
    }

    // Check the Iceberg table data
    return Catalogs.loadTable(shell.getHiveConf(), properties);
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 378
FRAGMENT LINE AVG SIZE: 34.36363636363637
DEPTHS:
1 2 2 2 3 2 2 2 2 1 1 
AREA: 20
AVG DEPTH: 1.8181818181818181
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: b1eff51620a462993450a5125b19e1f465c04c65
URL: https://github.com/apache/incubator-iceberg/commit/b1eff51620a462993450a5125b19e1f465c04c65
DESCRIPTION: Extract Method	private execute(namespace String, tableName String, refreshSparkCache boolean, func Function<org.apache.iceberg.Table,T>) : T extracted from protected modifyIcebergTable(namespace String, tableName String, func Function<org.apache.iceberg.Table,T>) : T in class org.apache.iceberg.spark.procedures.BaseProcedure
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/b1eff51620a462993450a5125b19e1f465c04c65/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/b1eff51620a462993450a5125b19e1f465c04c65/spark3/src/main/java/org/apache/iceberg/spark/procedures/BaseProcedure.java#L59
DIRECTLY EXTRACTED OPERATION:
                        Function<org.apache.iceberg.Table, T> func) {
    Preconditions.checkArgument(namespace != null && !namespace.isEmpty(), "Namespace cannot be empty");
    Preconditions.checkArgument(tableName != null && !tableName.isEmpty(), "Table name cannot be empty");

    Identifier ident = toIdentifier(namespace, tableName);
    SparkTable sparkTable = loadSparkTable(ident);
    org.apache.iceberg.Table icebergTable = sparkTable.table();

    T result = func.apply(icebergTable);

    if (refreshSparkCache) {
      refreshSparkCache(ident, sparkTable);
    }

    return result;
  }

PARAMS COUNT: 4
IS VOID METHOD: false
FRAGMENT LENGTH: 603
FRAGMENT LINE AVG SIZE: 35.470588235294116
DEPTHS:
1 2 2 2 2 2 2 2 2 2 2 3 2 2 2 1 1 
AREA: 32
AVG DEPTH: 1.8823529411764706
NUMBER OF LINES IN FRAGMENT: 17
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: b64bac571f1d63b53c732a5786a1bd988009086d
URL: https://github.com/apache/incubator-iceberg/commit/b64bac571f1d63b53c732a5786a1bd988009086d
DESCRIPTION: Extract Method	package configureEncryption(awsProperties AwsProperties, encryptionSetter Function<ServerSideEncryption,S3Request.Builder>, kmsKeySetter Function<String,S3Request.Builder>, customAlgorithmSetter Function<String,S3Request.Builder>, customKeySetter Function<String,S3Request.Builder>, customMd5Setter Function<String,S3Request.Builder>) : void extracted from package configureEncryption(awsProperties AwsProperties, requestBuilder PutObjectRequest.Builder) : void in class org.apache.iceberg.aws.s3.S3RequestUtil
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/b64bac571f1d63b53c732a5786a1bd988009086d/aws/src/main/java/org/apache/iceberg/aws/s3/S3RequestUtil.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/b64bac571f1d63b53c732a5786a1bd988009086d/aws/src/main/java/org/apache/iceberg/aws/s3/S3RequestUtil.java#L66
DIRECTLY EXTRACTED OPERATION:
  static void configureEncryption(
      AwsProperties awsProperties,
      Function<ServerSideEncryption, S3Request.Builder> encryptionSetter,
      Function<String, S3Request.Builder> kmsKeySetter,
      Function<String, S3Request.Builder> customAlgorithmSetter,
      Function<String, S3Request.Builder> customKeySetter,
      Function<String, S3Request.Builder> customMd5Setter) {

    switch (awsProperties.s3FileIoSseType().toLowerCase(Locale.ENGLISH)) {
      case AwsProperties.S3FILEIO_SSE_TYPE_NONE:
        break;

      case AwsProperties.S3FILEIO_SSE_TYPE_KMS:
        encryptionSetter.apply(ServerSideEncryption.AWS_KMS);
        kmsKeySetter.apply(awsProperties.s3FileIoSseKey());
        break;

      case AwsProperties.S3FILEIO_SSE_TYPE_S3:
        encryptionSetter.apply(ServerSideEncryption.AES256);
        break;

      case AwsProperties.S3FILEIO_SSE_TYPE_CUSTOM:
        // setters for SSE-C exist for all request builders, no need to check null
        customAlgorithmSetter.apply(ServerSideEncryption.AES256.name());
        customKeySetter.apply(awsProperties.s3FileIoSseKey());
        customMd5Setter.apply(awsProperties.s3FileIoSseMd5());
        break;

      default:
        throw new IllegalArgumentException(
            "Cannot support given S3 encryption type: " + awsProperties.s3FileIoSseType());
    }
  }
}
PARAMS COUNT: 6
IS VOID METHOD: true
FRAGMENT LENGTH: 1348
FRAGMENT LINE AVG SIZE: 39.64705882352941
DEPTHS:
0 1 1 1 1 1 1 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 1 0 
AREA: 79
AVG DEPTH: 2.323529411764706
NUMBER OF LINES IN FRAGMENT: 34
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	package configureEncryption(awsProperties AwsProperties, encryptionSetter Function<ServerSideEncryption,S3Request.Builder>, kmsKeySetter Function<String,S3Request.Builder>, customAlgorithmSetter Function<String,S3Request.Builder>, customKeySetter Function<String,S3Request.Builder>, customMd5Setter Function<String,S3Request.Builder>) : void extracted from package configureEncryption(awsProperties AwsProperties, requestBuilder CreateMultipartUploadRequest.Builder) : void in class org.apache.iceberg.aws.s3.S3RequestUtil
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/b64bac571f1d63b53c732a5786a1bd988009086d/aws/src/main/java/org/apache/iceberg/aws/s3/S3RequestUtil.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/b64bac571f1d63b53c732a5786a1bd988009086d/aws/src/main/java/org/apache/iceberg/aws/s3/S3RequestUtil.java#L66
DIRECTLY EXTRACTED OPERATION:
  static void configureEncryption(
      AwsProperties awsProperties,
      Function<ServerSideEncryption, S3Request.Builder> encryptionSetter,
      Function<String, S3Request.Builder> kmsKeySetter,
      Function<String, S3Request.Builder> customAlgorithmSetter,
      Function<String, S3Request.Builder> customKeySetter,
      Function<String, S3Request.Builder> customMd5Setter) {

    switch (awsProperties.s3FileIoSseType().toLowerCase(Locale.ENGLISH)) {
      case AwsProperties.S3FILEIO_SSE_TYPE_NONE:
        break;

      case AwsProperties.S3FILEIO_SSE_TYPE_KMS:
        encryptionSetter.apply(ServerSideEncryption.AWS_KMS);
        kmsKeySetter.apply(awsProperties.s3FileIoSseKey());
        break;

      case AwsProperties.S3FILEIO_SSE_TYPE_S3:
        encryptionSetter.apply(ServerSideEncryption.AES256);
        break;

      case AwsProperties.S3FILEIO_SSE_TYPE_CUSTOM:
        // setters for SSE-C exist for all request builders, no need to check null
        customAlgorithmSetter.apply(ServerSideEncryption.AES256.name());
        customKeySetter.apply(awsProperties.s3FileIoSseKey());
        customMd5Setter.apply(awsProperties.s3FileIoSseMd5());
        break;

      default:
        throw new IllegalArgumentException(
            "Cannot support given S3 encryption type: " + awsProperties.s3FileIoSseType());
    }
  }
}
PARAMS COUNT: 6
IS VOID METHOD: true
FRAGMENT LENGTH: 1348
FRAGMENT LINE AVG SIZE: 39.64705882352941
DEPTHS:
0 1 1 1 1 1 1 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 1 0 
AREA: 79
AVG DEPTH: 2.323529411764706
NUMBER OF LINES IN FRAGMENT: 34
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	package configureEncryption(awsProperties AwsProperties, encryptionSetter Function<ServerSideEncryption,S3Request.Builder>, kmsKeySetter Function<String,S3Request.Builder>, customAlgorithmSetter Function<String,S3Request.Builder>, customKeySetter Function<String,S3Request.Builder>, customMd5Setter Function<String,S3Request.Builder>) : void extracted from package configureEncryption(awsProperties AwsProperties, requestBuilder UploadPartRequest.Builder) : void in class org.apache.iceberg.aws.s3.S3RequestUtil
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/b64bac571f1d63b53c732a5786a1bd988009086d/aws/src/main/java/org/apache/iceberg/aws/s3/S3RequestUtil.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/b64bac571f1d63b53c732a5786a1bd988009086d/aws/src/main/java/org/apache/iceberg/aws/s3/S3RequestUtil.java#L66
DIRECTLY EXTRACTED OPERATION:
  static void configureEncryption(
      AwsProperties awsProperties,
      Function<ServerSideEncryption, S3Request.Builder> encryptionSetter,
      Function<String, S3Request.Builder> kmsKeySetter,
      Function<String, S3Request.Builder> customAlgorithmSetter,
      Function<String, S3Request.Builder> customKeySetter,
      Function<String, S3Request.Builder> customMd5Setter) {

    switch (awsProperties.s3FileIoSseType().toLowerCase(Locale.ENGLISH)) {
      case AwsProperties.S3FILEIO_SSE_TYPE_NONE:
        break;

      case AwsProperties.S3FILEIO_SSE_TYPE_KMS:
        encryptionSetter.apply(ServerSideEncryption.AWS_KMS);
        kmsKeySetter.apply(awsProperties.s3FileIoSseKey());
        break;

      case AwsProperties.S3FILEIO_SSE_TYPE_S3:
        encryptionSetter.apply(ServerSideEncryption.AES256);
        break;

      case AwsProperties.S3FILEIO_SSE_TYPE_CUSTOM:
        // setters for SSE-C exist for all request builders, no need to check null
        customAlgorithmSetter.apply(ServerSideEncryption.AES256.name());
        customKeySetter.apply(awsProperties.s3FileIoSseKey());
        customMd5Setter.apply(awsProperties.s3FileIoSseMd5());
        break;

      default:
        throw new IllegalArgumentException(
            "Cannot support given S3 encryption type: " + awsProperties.s3FileIoSseType());
    }
  }
}
PARAMS COUNT: 6
IS VOID METHOD: true
FRAGMENT LENGTH: 1348
FRAGMENT LINE AVG SIZE: 39.64705882352941
DEPTHS:
0 1 1 1 1 1 1 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 1 0 
AREA: 79
AVG DEPTH: 2.323529411764706
NUMBER OF LINES IN FRAGMENT: 34
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	package configureEncryption(awsProperties AwsProperties, encryptionSetter Function<ServerSideEncryption,S3Request.Builder>, kmsKeySetter Function<String,S3Request.Builder>, customAlgorithmSetter Function<String,S3Request.Builder>, customKeySetter Function<String,S3Request.Builder>, customMd5Setter Function<String,S3Request.Builder>) : void extracted from package configureEncryption(awsProperties AwsProperties, requestBuilder GetObjectRequest.Builder) : void in class org.apache.iceberg.aws.s3.S3RequestUtil
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/b64bac571f1d63b53c732a5786a1bd988009086d/aws/src/main/java/org/apache/iceberg/aws/s3/S3RequestUtil.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/b64bac571f1d63b53c732a5786a1bd988009086d/aws/src/main/java/org/apache/iceberg/aws/s3/S3RequestUtil.java#L66
DIRECTLY EXTRACTED OPERATION:
  static void configureEncryption(
      AwsProperties awsProperties,
      Function<ServerSideEncryption, S3Request.Builder> encryptionSetter,
      Function<String, S3Request.Builder> kmsKeySetter,
      Function<String, S3Request.Builder> customAlgorithmSetter,
      Function<String, S3Request.Builder> customKeySetter,
      Function<String, S3Request.Builder> customMd5Setter) {

    switch (awsProperties.s3FileIoSseType().toLowerCase(Locale.ENGLISH)) {
      case AwsProperties.S3FILEIO_SSE_TYPE_NONE:
        break;

      case AwsProperties.S3FILEIO_SSE_TYPE_KMS:
        encryptionSetter.apply(ServerSideEncryption.AWS_KMS);
        kmsKeySetter.apply(awsProperties.s3FileIoSseKey());
        break;

      case AwsProperties.S3FILEIO_SSE_TYPE_S3:
        encryptionSetter.apply(ServerSideEncryption.AES256);
        break;

      case AwsProperties.S3FILEIO_SSE_TYPE_CUSTOM:
        // setters for SSE-C exist for all request builders, no need to check null
        customAlgorithmSetter.apply(ServerSideEncryption.AES256.name());
        customKeySetter.apply(awsProperties.s3FileIoSseKey());
        customMd5Setter.apply(awsProperties.s3FileIoSseMd5());
        break;

      default:
        throw new IllegalArgumentException(
            "Cannot support given S3 encryption type: " + awsProperties.s3FileIoSseType());
    }
  }
}
PARAMS COUNT: 6
IS VOID METHOD: true
FRAGMENT LENGTH: 1348
FRAGMENT LINE AVG SIZE: 39.64705882352941
DEPTHS:
0 1 1 1 1 1 1 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 1 0 
AREA: 79
AVG DEPTH: 2.323529411764706
NUMBER OF LINES IN FRAGMENT: 34
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 5e3f9198e5675a852df4f0e1c28b4e3cf6630f86
URL: https://github.com/apache/incubator-iceberg/commit/5e3f9198e5675a852df4f0e1c28b4e3cf6630f86
DESCRIPTION: Extract Method	private completeUploads() : void extracted from public close() : void in class org.apache.iceberg.aws.s3.S3OutputStream
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/5e3f9198e5675a852df4f0e1c28b4e3cf6630f86/aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/5e3f9198e5675a852df4f0e1c28b4e3cf6630f86/aws/src/main/java/org/apache/iceberg/aws/s3/S3OutputStream.java#L299
DIRECTLY EXTRACTED OPERATION:
    if (multipartUploadId == null) {
      long contentLength = stagingFiles.stream().mapToLong(File::length).sum();
      InputStream contentStream = new BufferedInputStream(stagingFiles.stream()
          .map(S3OutputStream::uncheckedInputStream)
          .reduce(SequenceInputStream::new)
          .orElseGet(() -> new ByteArrayInputStream(new byte[0])));

      PutObjectRequest.Builder requestBuilder = PutObjectRequest.builder()
          .bucket(location.bucket())
          .key(location.key());

      S3RequestUtil.configureEncryption(awsProperties, requestBuilder);

      s3.putObject(requestBuilder.build(), RequestBody.fromInputStream(contentStream, contentLength));
    } else {
      uploadParts();
      completeMultiPartUpload();
    }
  }

IS VOID METHOD: true
FRAGMENT LENGTH: 762
FRAGMENT LINE AVG SIZE: 38.1
DEPTHS:
2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 1 1 
AREA: 54
AVG DEPTH: 2.7
NUMBER OF LINES IN FRAGMENT: 20
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private writeAndVerify(client S3Client, uri S3URI, data byte[], arrayWrite boolean) : void extracted from public testWrite() : void in class org.apache.iceberg.aws.s3.S3OutputStreamTest
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/5e3f9198e5675a852df4f0e1c28b4e3cf6630f86/aws/src/test/java/org/apache/iceberg/aws/s3/S3OutputStreamTest.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/5e3f9198e5675a852df4f0e1c28b4e3cf6630f86/aws/src/test/java/org/apache/iceberg/aws/s3/S3OutputStreamTest.java#L143
DIRECTLY EXTRACTED OPERATION:
    try (S3OutputStream stream = new S3OutputStream(client, uri, properties)) {
      if (arrayWrite) {
        stream.write(data);
        assertEquals(data.length, stream.getPos());
      } else {
        for (int i = 0; i < data.length; i++) {
          stream.write(data[i]);
          assertEquals(i + 1, stream.getPos());
        }
      }
    } catch (IOException e) {
      throw new UncheckedIOException(e);
    }

    byte[] actual = readS3Data(uri);
    assertArrayEquals(data, actual);

    // Verify all staging files are cleaned up
    try {
      assertEquals(0, Files.list(tmpDir).count());
    } catch (IOException e) {
      throw new UncheckedIOException(e);
    }
  }

PARAMS COUNT: 4
IS VOID METHOD: true
FRAGMENT LENGTH: 689
FRAGMENT LINE AVG SIZE: 27.56
DEPTHS:
2 3 4 4 4 4 5 5 4 3 3 3 2 2 2 2 2 2 2 3 3 3 2 1 1 
AREA: 71
AVG DEPTH: 2.84
NUMBER OF LINES IN FRAGMENT: 25
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private randomData(size int) : byte[] extracted from public testWrite() : void in class org.apache.iceberg.aws.s3.S3OutputStreamTest
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/5e3f9198e5675a852df4f0e1c28b4e3cf6630f86/aws/src/test/java/org/apache/iceberg/aws/s3/S3OutputStreamTest.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/5e3f9198e5675a852df4f0e1c28b4e3cf6630f86/aws/src/test/java/org/apache/iceberg/aws/s3/S3OutputStreamTest.java#L177
DIRECTLY EXTRACTED OPERATION:
    byte [] result = new byte[size];
    random.nextBytes(result);
    return result;
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 91
FRAGMENT LINE AVG SIZE: 18.2
DEPTHS:
1 2 2 1 1 
AREA: 7
AVG DEPTH: 1.4
NUMBER OF LINES IN FRAGMENT: 5
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 53cac10b25dbef9e25343de36856822466aad493
URL: https://github.com/apache/incubator-iceberg/commit/53cac10b25dbef9e25343de36856822466aad493
DESCRIPTION: Extract Method	private invalidate(ident TableIdentifier) : void extracted from public dropTable(ident TableIdentifier, purge boolean) : boolean in class org.apache.iceberg.CachingCatalog
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/53cac10b25dbef9e25343de36856822466aad493/core/src/main/java/org/apache/iceberg/CachingCatalog.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/53cac10b25dbef9e25343de36856822466aad493/core/src/main/java/org/apache/iceberg/CachingCatalog.java#L147
DIRECTLY EXTRACTED OPERATION:
    tableCache.invalidate(ident);
    tableCache.invalidateAll(metadataTableIdentifiers(ident));
  }

PARAMS COUNT: 1
IS VOID METHOD: true
FRAGMENT LENGTH: 102
FRAGMENT LINE AVG SIZE: 25.5
DEPTHS:
1 2 1 1 
AREA: 5
AVG DEPTH: 1.25
NUMBER OF LINES IN FRAGMENT: 4
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private invalidate(ident TableIdentifier) : void extracted from public renameTable(from TableIdentifier, to TableIdentifier) : void in class org.apache.iceberg.CachingCatalog
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/53cac10b25dbef9e25343de36856822466aad493/core/src/main/java/org/apache/iceberg/CachingCatalog.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/53cac10b25dbef9e25343de36856822466aad493/core/src/main/java/org/apache/iceberg/CachingCatalog.java#L147
DIRECTLY EXTRACTED OPERATION:
    tableCache.invalidate(ident);
    tableCache.invalidateAll(metadataTableIdentifiers(ident));
  }

PARAMS COUNT: 1
IS VOID METHOD: true
FRAGMENT LENGTH: 102
FRAGMENT LINE AVG SIZE: 25.5
DEPTHS:
1 2 1 1 
AREA: 5
AVG DEPTH: 1.25
NUMBER OF LINES IN FRAGMENT: 4
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 944a437f1057b8be60292426aebdb0b8059d90e0
URL: https://github.com/apache/incubator-iceberg/commit/944a437f1057b8be60292426aebdb0b8059d90e0
DESCRIPTION: Extract Method	protected assertCounts(fieldId int, valueCount Long, nullValueCount Long, nanValueCount Long, metrics Metrics) : void extracted from protected assertCounts(fieldId int, valueCount Long, nullValueCount Long, metrics Metrics) : void in class org.apache.iceberg.TestMetrics
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/944a437f1057b8be60292426aebdb0b8059d90e0/core/src/test/java/org/apache/iceberg/TestMetrics.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/944a437f1057b8be60292426aebdb0b8059d90e0/core/src/test/java/org/apache/iceberg/TestMetrics.java#L612
DIRECTLY EXTRACTED OPERATION:
    Map<Integer, Long> valueCounts = metrics.valueCounts();
    Map<Integer, Long> nullValueCounts = metrics.nullValueCounts();
    Map<Integer, Long> nanValueCounts = metrics.nanValueCounts();
    Assert.assertEquals(valueCount, valueCounts.get(fieldId));
    Assert.assertEquals(nullValueCount, nullValueCounts.get(fieldId));
    if (fileFormat() != FileFormat.ORC) {
      Assert.assertEquals(nanValueCount, nanValueCounts.get(fieldId));
    }
  }

PARAMS COUNT: 5
IS VOID METHOD: true
FRAGMENT LENGTH: 452
FRAGMENT LINE AVG SIZE: 45.2
DEPTHS:
1 2 2 2 2 2 3 2 1 1 
AREA: 18
AVG DEPTH: 1.8
NUMBER OF LINES IN FRAGMENT: 10
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private getDecimalInternal(ordinal int, precision int, scale int) : Decimal extracted from public getDecimal(ordinal int, precision int, scale int) : Decimal in class org.apache.iceberg.spark.source.StructInternalRow
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/944a437f1057b8be60292426aebdb0b8059d90e0/spark/src/main/java/org/apache/iceberg/spark/source/StructInternalRow.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/944a437f1057b8be60292426aebdb0b8059d90e0/spark/src/main/java/org/apache/iceberg/spark/source/StructInternalRow.java#L141
DIRECTLY EXTRACTED OPERATION:
    return Decimal.apply(struct.get(ordinal, BigDecimal.class));
  }

FRAGMENT LENGTH: 70
FRAGMENT LINE AVG SIZE: 23.333333333333332
DEPTHS:
1 1 1 
AREA: 3
AVG DEPTH: 1.0
NUMBER OF LINES IN FRAGMENT: 3
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private getUTF8StringInternal(ordinal int) : UTF8String extracted from public getUTF8String(ordinal int) : UTF8String in class org.apache.iceberg.spark.source.StructInternalRow
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/944a437f1057b8be60292426aebdb0b8059d90e0/spark/src/main/java/org/apache/iceberg/spark/source/StructInternalRow.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/944a437f1057b8be60292426aebdb0b8059d90e0/spark/src/main/java/org/apache/iceberg/spark/source/StructInternalRow.java#L150
DIRECTLY EXTRACTED OPERATION:
    CharSequence seq = struct.get(ordinal, CharSequence.class);
    return UTF8String.fromString(seq.toString());
  }

FRAGMENT LENGTH: 119
FRAGMENT LINE AVG SIZE: 29.75
DEPTHS:
1 2 1 1 
AREA: 5
AVG DEPTH: 1.25
NUMBER OF LINES IN FRAGMENT: 4
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private getStructInternal(ordinal int, numFields int) : InternalRow extracted from public getStruct(ordinal int, numFields int) : InternalRow in class org.apache.iceberg.spark.source.StructInternalRow
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/944a437f1057b8be60292426aebdb0b8059d90e0/spark/src/main/java/org/apache/iceberg/spark/source/StructInternalRow.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/944a437f1057b8be60292426aebdb0b8059d90e0/spark/src/main/java/org/apache/iceberg/spark/source/StructInternalRow.java#L183
DIRECTLY EXTRACTED OPERATION:
    return new StructInternalRow(
        type.fields().get(ordinal).type().asStructType(),
        struct.get(ordinal, StructLike.class));
  }

FRAGMENT LENGTH: 145
FRAGMENT LINE AVG SIZE: 29.0
DEPTHS:
1 2 2 1 1 
AREA: 7
AVG DEPTH: 1.4
NUMBER OF LINES IN FRAGMENT: 5
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private getArrayInternal(ordinal int) : ArrayData extracted from public getArray(ordinal int) : ArrayData in class org.apache.iceberg.spark.source.StructInternalRow
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/944a437f1057b8be60292426aebdb0b8059d90e0/spark/src/main/java/org/apache/iceberg/spark/source/StructInternalRow.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/944a437f1057b8be60292426aebdb0b8059d90e0/spark/src/main/java/org/apache/iceberg/spark/source/StructInternalRow.java#L194
DIRECTLY EXTRACTED OPERATION:
    return collectionToArrayData(
        type.fields().get(ordinal).type().asListType().elementType(),
        struct.get(ordinal, Collection.class));
  }

FRAGMENT LENGTH: 157
FRAGMENT LINE AVG SIZE: 31.4
DEPTHS:
1 2 2 1 1 
AREA: 7
AVG DEPTH: 1.4
NUMBER OF LINES IN FRAGMENT: 5
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private getMapInternal(ordinal int) : MapData extracted from public getMap(ordinal int) : MapData in class org.apache.iceberg.spark.source.StructInternalRow
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/944a437f1057b8be60292426aebdb0b8059d90e0/spark/src/main/java/org/apache/iceberg/spark/source/StructInternalRow.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/944a437f1057b8be60292426aebdb0b8059d90e0/spark/src/main/java/org/apache/iceberg/spark/source/StructInternalRow.java#L205
DIRECTLY EXTRACTED OPERATION:
    return mapToMapData(type.fields().get(ordinal).type().asMapType(), struct.get(ordinal, Map.class));
  }

FRAGMENT LENGTH: 109
FRAGMENT LINE AVG SIZE: 36.333333333333336
DEPTHS:
1 1 1 
AREA: 3
AVG DEPTH: 1.0
NUMBER OF LINES IN FRAGMENT: 3
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 2a371cf0be5dfeb66320163e02b606a129b3203a
URL: https://github.com/apache/incubator-iceberg/commit/2a371cf0be5dfeb66320163e02b606a129b3203a
DESCRIPTION: Extract Method	private writeVersionToPath(fs FileSystem, path Path, versionToWrite int) : void extracted from private writeVersionHint(versionToWrite int) : void in class org.apache.iceberg.hadoop.HadoopTableOperations
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/2a371cf0be5dfeb66320163e02b606a129b3203a/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/2a371cf0be5dfeb66320163e02b606a129b3203a/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java#L307
DIRECTLY EXTRACTED OPERATION:
    try (FSDataOutputStream out = fs.create(path, false /* overwrite */)) {
      out.write(String.valueOf(versionToWrite).getBytes(StandardCharsets.UTF_8));
    }
  }

PARAMS COUNT: 3
IS VOID METHOD: true
FRAGMENT LENGTH: 169
FRAGMENT LINE AVG SIZE: 33.8
DEPTHS:
2 3 2 1 1 
AREA: 9
AVG DEPTH: 1.8
NUMBER OF LINES IN FRAGMENT: 5
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 3d308d3c292ba06ebce554efe05561b8f2e79b51
URL: https://github.com/apache/incubator-iceberg/commit/3d308d3c292ba06ebce554efe05561b8f2e79b51
DESCRIPTION: Extract Method	private writeAndValidate(schema Schema, numRecords int, seed long, nullPercentage float, setAndCheckArrowValidityVector boolean, reuseContainers boolean, batchSize int, transform Function<GenericData.Record,GenericData.Record>) : void extracted from private writeAndValidate(schema Schema, numRecords int, seed long, nullPercentage float, setAndCheckArrowValidityVector boolean, reuseContainers boolean) : void in class org.apache.iceberg.spark.data.parquet.vectorized.TestParquetVectorizedReads
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/3d308d3c292ba06ebce554efe05561b8f2e79b51/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/3d308d3c292ba06ebce554efe05561b8f2e79b51/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java#L72
DIRECTLY EXTRACTED OPERATION:
          Schema schema, int numRecords, long seed, float nullPercentage,
          boolean setAndCheckArrowValidityVector, boolean reuseContainers, int batchSize,
          Function<GenericData.Record, GenericData.Record> transform)
      throws IOException {
    // Write test data
    Assume.assumeTrue("Parquet Avro cannot write non-string map keys", null == TypeUtil.find(
        schema,
        type -> type.isMapType() && type.asMapType().keyType() != Types.StringType.get()));

    Iterable<GenericData.Record> expected = generateData(schema, numRecords, seed, nullPercentage, transform);

    // write a test parquet file using iceberg writer
    File testFile = temp.newFile();
    Assert.assertTrue("Delete should succeed", testFile.delete());

    try (FileAppender<GenericData.Record> writer = getParquetWriter(schema, testFile)) {
      writer.addAll(expected);
    }
    assertRecordsMatch(schema, numRecords, expected, testFile, setAndCheckArrowValidityVector,
            reuseContainers, batchSize);
  }

PARAMS COUNT: 8
IS VOID METHOD: true
FRAGMENT LENGTH: 1024
FRAGMENT LINE AVG SIZE: 46.54545454545455
DEPTHS:
0 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 1 1 
AREA: 38
AVG DEPTH: 1.7272727272727273
NUMBER OF LINES IN FRAGMENT: 22
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: d132474e7c8706f504a47f75b58e5a58d0bef9ff
URL: https://github.com/apache/incubator-iceberg/commit/d132474e7c8706f504a47f75b58e5a58d0bef9ff
DESCRIPTION: Extract Method	private getWarehouseLocation() : String extracted from protected defaultWarehouseLocation(tableIdentifier TableIdentifier) : String in class org.apache.iceberg.hive.HiveCatalog
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/d132474e7c8706f504a47f75b58e5a58d0bef9ff/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveCatalog.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/d132474e7c8706f504a47f75b58e5a58d0bef9ff/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveCatalog.java#L441
DIRECTLY EXTRACTED OPERATION:
    String warehouseLocation = conf.get(HiveConf.ConfVars.METASTOREWAREHOUSE.varname);
    Preconditions.checkNotNull(warehouseLocation, "Warehouse location is not set: hive.metastore.warehouse.dir=null");
    return warehouseLocation;
  }

IS VOID METHOD: false
FRAGMENT LENGTH: 241
FRAGMENT LINE AVG SIZE: 48.2
DEPTHS:
1 2 2 1 1 
AREA: 7
AVG DEPTH: 1.4
NUMBER OF LINES IN FRAGMENT: 5
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 50d10f39fb9836e8bb38535637860dbad320ecc1
URL: https://github.com/apache/incubator-iceberg/commit/50d10f39fb9836e8bb38535637860dbad320ecc1
DESCRIPTION: Extract Method	public start(poolSize int) : void extracted from public start() : void in class org.apache.iceberg.hive.TestHiveMetastore
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/50d10f39fb9836e8bb38535637860dbad320ecc1/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/50d10f39fb9836e8bb38535637860dbad320ecc1/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java#L85
DIRECTLY EXTRACTED OPERATION:
   * Starts a TestHiveMetastore with a provided connection pool size.
   * @param poolSize The number of threads in the executor pool
   */
  public void start(int poolSize) {
    try {
      this.hiveLocalDir = createTempDirectory("hive", asFileAttribute(fromString("rwxrwxrwx"))).toFile();
      File derbyLogFile = new File(hiveLocalDir, "derby.log");
      System.setProperty("derby.stream.error.file", derbyLogFile.getAbsolutePath());
      setupMetastoreDB("jdbc:derby:" + getDerbyPath() + ";create=true");

      TServerSocket socket = new TServerSocket(0);
      int port = socket.getServerSocket().getLocalPort();
      this.hiveConf = newHiveConf(port);
      this.server = newThriftServer(socket, poolSize, hiveConf);
      this.executorService = Executors.newSingleThreadExecutor();
      this.executorService.submit(() -> server.serve());

      // in Hive3, setting this as a system prop ensures that it will be picked up whenever a new HiveConf is created
      System.setProperty(HiveConf.ConfVars.METASTOREURIS.varname, hiveConf.getVar(HiveConf.ConfVars.METASTOREURIS));

      this.clientPool = new HiveClientPool(1, hiveConf);
    } catch (Exception e) {
      throw new RuntimeException("Cannot start TestHiveMetastore", e);
    }
  }

PARAMS COUNT: 1
IS VOID METHOD: true
FRAGMENT LENGTH: 1256
FRAGMENT LINE AVG SIZE: 48.30769230769231
DEPTHS:
0 1 1 1 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 1 1 
AREA: 63
AVG DEPTH: 2.423076923076923
NUMBER OF LINES IN FRAGMENT: 26
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 80ea771f761e3f5398f40d7bef1e6b8513170564
URL: https://github.com/apache/incubator-iceberg/commit/80ea771f761e3f5398f40d7bef1e6b8513170564
DESCRIPTION: Extract Method	private persistTable(hmsTable Table, updateHiveTable boolean) : void extracted from protected doCommit(base TableMetadata, metadata TableMetadata) : void in class org.apache.iceberg.hive.HiveTableOperations
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/80ea771f761e3f5398f40d7bef1e6b8513170564/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/80ea771f761e3f5398f40d7bef1e6b8513170564/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java#L209
DIRECTLY EXTRACTED OPERATION:
    if (updateHiveTable) {
      metaClients.run(client -> {
        EnvironmentContext envContext = new EnvironmentContext(
            ImmutableMap.of(StatsSetupConst.DO_NOT_UPDATE_STATS, StatsSetupConst.TRUE)
        );
        ALTER_TABLE.invoke(client, database, tableName, hmsTable, envContext);
        return null;
      });
    } else {
      metaClients.run(client -> {
        client.createTable(hmsTable);
        return null;
      });
    }
  }

PARAMS COUNT: 2
IS VOID METHOD: true
FRAGMENT LENGTH: 460
FRAGMENT LINE AVG SIZE: 28.75
DEPTHS:
2 3 4 4 4 4 4 3 3 3 4 4 3 2 1 1 
AREA: 49
AVG DEPTH: 3.0625
NUMBER OF LINES IN FRAGMENT: 16
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private newHmsTable() : Table extracted from protected doCommit(base TableMetadata, metadata TableMetadata) : void in class org.apache.iceberg.hive.HiveTableOperations
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/80ea771f761e3f5398f40d7bef1e6b8513170564/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/80ea771f761e3f5398f40d7bef1e6b8513170564/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java#L235
DIRECTLY EXTRACTED OPERATION:
    final long currentTimeMillis = System.currentTimeMillis();

    Table newTable = new Table(tableName,
        database,
        System.getProperty("user.name"),
        (int) currentTimeMillis / 1000,
        (int) currentTimeMillis / 1000,
        Integer.MAX_VALUE,
        null,
        Collections.emptyList(),
        new HashMap<>(),
        null,
        null,
        TableType.EXTERNAL_TABLE.toString());

    newTable.getParameters().put("EXTERNAL", "TRUE"); // using the external table type also requires this
    return newTable;
  }

IS VOID METHOD: false
FRAGMENT LENGTH: 551
FRAGMENT LINE AVG SIZE: 29.0
DEPTHS:
1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 
AREA: 35
AVG DEPTH: 1.8421052631578947
NUMBER OF LINES IN FRAGMENT: 19
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: b4ba650441674276a27f432c857fa65a6215779b
URL: https://github.com/apache/incubator-iceberg/commit/b4ba650441674276a27f432c857fa65a6215779b
DESCRIPTION: Extract Method	public fromHadoopTable(location String, hadoopConf Configuration) : TableLoader extracted from public fromHadoopTable(location String) : TableLoader in class org.apache.iceberg.flink.TableLoader
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/b4ba650441674276a27f432c857fa65a6215779b/flink/src/main/java/org/apache/iceberg/flink/TableLoader.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/b4ba650441674276a27f432c857fa65a6215779b/flink/src/main/java/org/apache/iceberg/flink/TableLoader.java#L52
DIRECTLY EXTRACTED OPERATION:
    return new HadoopTableLoader(location, hadoopConf);
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 61
FRAGMENT LINE AVG SIZE: 20.333333333333332
DEPTHS:
1 1 1 
AREA: 3
AVG DEPTH: 1.0
NUMBER OF LINES IN FRAGMENT: 3
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: d4dcadb3a74d4552f85fa6c99637f86a0b763426
URL: https://github.com/apache/incubator-iceberg/commit/d4dcadb3a74d4552f85fa6c99637f86a0b763426
DESCRIPTION: Extract Method	public validateNoConflictingAppends(newConflictDetectionFilter Expression) : OverwriteFiles extracted from public validateNoConflictingAppends(newReadSnapshotId Long, newConflictDetectionFilter Expression) : OverwriteFiles in class org.apache.iceberg.BaseOverwriteFiles
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/d4dcadb3a74d4552f85fa6c99637f86a0b763426/core/src/main/java/org/apache/iceberg/BaseOverwriteFiles.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/d4dcadb3a74d4552f85fa6c99637f86a0b763426/core/src/main/java/org/apache/iceberg/BaseOverwriteFiles.java#L95
DIRECTLY EXTRACTED OPERATION:
    Preconditions.checkArgument(newConflictDetectionFilter != null, "Conflict detection filter cannot be null");
    this.conflictDetectionFilter = newConflictDetectionFilter;
    failMissingDeletePaths();
    return this;
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 228
FRAGMENT LINE AVG SIZE: 38.0
DEPTHS:
1 2 2 2 1 1 
AREA: 9
AVG DEPTH: 1.5
NUMBER OF LINES IN FRAGMENT: 6
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 118cc80c16950d6c6b87eaadd843b2b156c9435a
URL: https://github.com/apache/incubator-iceberg/commit/118cc80c16950d6c6b87eaadd843b2b156c9435a
DESCRIPTION: Extract Method	private openTask(currentTask FileScanTask, readSchema Schema) : CloseableIterable<T> extracted from private open(currentTask FileScanTask, readSchema Schema) : CloseableIterable<T> in class org.apache.iceberg.mr.mapreduce.IcebergInputFormat.IcebergRecordReader
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/118cc80c16950d6c6b87eaadd843b2b156c9435a/mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/118cc80c16950d6c6b87eaadd843b2b156c9435a/mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java#L236
DIRECTLY EXTRACTED OPERATION:
      DataFile file = currentTask.file();
      InputFile inputFile = encryptionManager.decrypt(EncryptedFiles.encryptedInput(
          io.newInputFile(file.path().toString()),
          file.keyMetadata()));

      CloseableIterable<T> iterable;
      switch (file.format()) {
        case AVRO:
          iterable = newAvroIterable(inputFile, currentTask, readSchema);
          break;
        case ORC:
          iterable = newOrcIterable(inputFile, currentTask, readSchema);
          break;
        case PARQUET:
          iterable = newParquetIterable(inputFile, currentTask, readSchema);
          break;
        default:
          throw new UnsupportedOperationException(
              String.format("Cannot read %s file: %s", file.format().name(), file.path()));
      }

      return iterable;
    }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 812
FRAGMENT LINE AVG SIZE: 33.833333333333336
DEPTHS:
2 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 3 3 3 2 2 
AREA: 81
AVG DEPTH: 3.375
NUMBER OF LINES IN FRAGMENT: 24
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: c07b23b313e9992d16b8ea8a4eb89ed5a6b12985
URL: https://github.com/apache/incubator-iceberg/commit/c07b23b313e9992d16b8ea8a4eb89ed5a6b12985
DESCRIPTION: Extract Method	public fromInputFile(file InputFile, metricsConfig MetricsConfig, mapping NameMapping) : Metrics extracted from public fromInputFile(file InputFile, metricsConfig MetricsConfig) : Metrics in class org.apache.iceberg.orc.OrcMetrics
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/c07b23b313e9992d16b8ea8a4eb89ed5a6b12985/orc/src/main/java/org/apache/iceberg/orc/OrcMetrics.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/c07b23b313e9992d16b8ea8a4eb89ed5a6b12985/orc/src/main/java/org/apache/iceberg/orc/OrcMetrics.java#L80
DIRECTLY EXTRACTED OPERATION:
    final Configuration config = (file instanceof HadoopInputFile) ?
        ((HadoopInputFile) file).getConf() : new Configuration();
    return fromInputFile(file, config, metricsConfig, mapping);
  }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 204
FRAGMENT LINE AVG SIZE: 40.8
DEPTHS:
1 2 2 1 1 
AREA: 7
AVG DEPTH: 1.4
NUMBER OF LINES IN FRAGMENT: 5
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 2f2f461d5f280dff4de486fc54d0123591f0fe0a
URL: https://github.com/apache/incubator-iceberg/commit/2f2f461d5f280dff4de486fc54d0123591f0fe0a
DESCRIPTION: Extract Method	private idFor(fullName String) : int extracted from public list(list Types.ListType, future Supplier<Type>) : Type in class org.apache.iceberg.types.AssignFreshIds
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/2f2f461d5f280dff4de486fc54d0123591f0fe0a/api/src/main/java/org/apache/iceberg/types/AssignFreshIds.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/2f2f461d5f280dff4de486fc54d0123591f0fe0a/api/src/main/java/org/apache/iceberg/types/AssignFreshIds.java#L52
DIRECTLY EXTRACTED OPERATION:
    if (baseSchema != null && fullName != null) {
      Types.NestedField field = baseSchema.findField(fullName);
      if (field != null) {
        return field.fieldId();
      }
    }

    return nextId.get();
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 218
FRAGMENT LINE AVG SIZE: 21.8
DEPTHS:
2 3 3 4 3 2 2 2 1 1 
AREA: 23
AVG DEPTH: 2.3
NUMBER OF LINES IN FRAGMENT: 10
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private idFor(fullName String) : int extracted from public map(map Types.MapType, keyFuture Supplier<Type>, valuefuture Supplier<Type>) : Type in class org.apache.iceberg.types.AssignFreshIds
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/2f2f461d5f280dff4de486fc54d0123591f0fe0a/api/src/main/java/org/apache/iceberg/types/AssignFreshIds.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/2f2f461d5f280dff4de486fc54d0123591f0fe0a/api/src/main/java/org/apache/iceberg/types/AssignFreshIds.java#L52
DIRECTLY EXTRACTED OPERATION:
    if (baseSchema != null && fullName != null) {
      Types.NestedField field = baseSchema.findField(fullName);
      if (field != null) {
        return field.fieldId();
      }
    }

    return nextId.get();
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 218
FRAGMENT LINE AVG SIZE: 21.8
DEPTHS:
2 3 3 4 3 2 2 2 1 1 
AREA: 23
AVG DEPTH: 2.3
NUMBER OF LINES IN FRAGMENT: 10
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 8634dab82d605dfb866bf271a25e63da32f22726
URL: https://github.com/apache/incubator-iceberg/commit/8634dab82d605dfb866bf271a25e63da32f22726
DESCRIPTION: Extract Method	private readTable(location String) : List<Row> extracted from private writeAndValidateWithLocations(table Table, location File, expectedDataDir File) : void in class org.apache.iceberg.spark.source.TestDataFrameWrites
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/8634dab82d605dfb866bf271a25e63da32f22726/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWrites.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/8634dab82d605dfb866bf271a25e63da32f22726/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWrites.java#L195
DIRECTLY EXTRACTED OPERATION:
    Dataset<Row> result = spark.read()
        .format("iceberg")
        .load(location);

    return result.collectAsList();
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 132
FRAGMENT LINE AVG SIZE: 18.857142857142858
DEPTHS:
1 2 2 2 2 1 1 
AREA: 11
AVG DEPTH: 1.5714285714285714
NUMBER OF LINES IN FRAGMENT: 7
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 606b003272520034da9557b871aed379bf6aee9f
URL: https://github.com/apache/incubator-iceberg/commit/606b003272520034da9557b871aed379bf6aee9f
DESCRIPTION: Extract Method	private addVersionsToTable(table Table) : void extracted from public testVersionHintFile() : void in class org.apache.iceberg.hadoop.TestHadoopCatalog
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/606b003272520034da9557b871aed379bf6aee9f/core/src/test/java/org/apache/iceberg/hadoop/TestHadoopCatalog.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/606b003272520034da9557b871aed379bf6aee9f/core/src/test/java/org/apache/iceberg/hadoop/TestHadoopCatalog.java#L528
DIRECTLY EXTRACTED OPERATION:
    DataFile dataFile1 = DataFiles.builder(SPEC)
        .withPath("/a.parquet")
        .withFileSizeInBytes(10)
        .withRecordCount(1)
        .build();

    DataFile dataFile2 = DataFiles.builder(SPEC)
        .withPath("/b.parquet")
        .withFileSizeInBytes(10)
        .withRecordCount(1)
        .build();

    table.newAppend().appendFile(dataFile1).commit();
    table.newAppend().appendFile(dataFile2).commit();
  }
}
PARAMS COUNT: 1
IS VOID METHOD: true
FRAGMENT LENGTH: 436
FRAGMENT LINE AVG SIZE: 27.25
DEPTHS:
1 2 2 2 2 2 2 2 2 2 2 2 2 2 1 0 
AREA: 28
AVG DEPTH: 1.75
NUMBER OF LINES IN FRAGMENT: 16
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: d8f5e0b6298dede4ad0f9aeccd05b30633b2b2f7
URL: https://github.com/apache/incubator-iceberg/commit/d8f5e0b6298dede4ad0f9aeccd05b30633b2b2f7
DESCRIPTION: Extract Method	private createDatabase(databaseName String, metadata Map<String,String>, ignoreIfExists boolean) : void extracted from public createDatabase(name String, database CatalogDatabase, ignoreIfExists boolean) : void in class org.apache.iceberg.flink.FlinkCatalog
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/d8f5e0b6298dede4ad0f9aeccd05b30633b2b2f7/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/d8f5e0b6298dede4ad0f9aeccd05b30633b2b2f7/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java#L191
DIRECTLY EXTRACTED OPERATION:
      throws DatabaseAlreadyExistException, CatalogException {
    if (asNamespaceCatalog != null) {
      try {
        asNamespaceCatalog.createNamespace(toNamespace(databaseName), metadata);
      } catch (AlreadyExistsException e) {
        if (!ignoreIfExists) {
          throw new DatabaseAlreadyExistException(getName(), databaseName, e);
        }
      }
    } else {
      throw new UnsupportedOperationException("Namespaces are not supported by catalog: " + getName());
    }
  }

PARAMS COUNT: 3
IS VOID METHOD: true
FRAGMENT LENGTH: 493
FRAGMENT LINE AVG SIZE: 35.214285714285715
DEPTHS:
1 2 3 4 4 4 5 4 3 3 3 2 1 1 
AREA: 40
AVG DEPTH: 2.857142857142857
NUMBER OF LINES IN FRAGMENT: 14
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 454101c3573acb9cd94d6d9a306ed99a5a324ed9
URL: https://github.com/apache/incubator-iceberg/commit/454101c3573acb9cd94d6d9a306ed99a5a324ed9
DESCRIPTION: Extract Method	public create(temp File, name String, schema Schema, spec PartitionSpec, sortOrder SortOrder, formatVersion int) : TestTable extracted from public create(temp File, name String, schema Schema, spec PartitionSpec, formatVersion int) : TestTable in class org.apache.iceberg.TestTables
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/454101c3573acb9cd94d6d9a306ed99a5a324ed9/core/src/test/java/org/apache/iceberg/TestTables.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/454101c3573acb9cd94d6d9a306ed99a5a324ed9/core/src/test/java/org/apache/iceberg/TestTables.java#L54
DIRECTLY EXTRACTED OPERATION:
                                 SortOrder sortOrder, int formatVersion) {
    TestTableOperations ops = new TestTableOperations(name, temp);
    if (ops.current() != null) {
      throw new AlreadyExistsException("Table %s already exists at location: %s", name, temp);
    }

    ops.commit(null, newTableMetadata(schema, spec, sortOrder, temp.toString(), ImmutableMap.of(), formatVersion));

    return new TestTable(ops, name);
  }

PARAMS COUNT: 6
IS VOID METHOD: false
FRAGMENT LENGTH: 436
FRAGMENT LINE AVG SIZE: 39.63636363636363
DEPTHS:
1 2 2 3 2 2 2 2 2 1 1 
AREA: 20
AVG DEPTH: 1.8181818181818181
NUMBER OF LINES IN FRAGMENT: 11
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public beginCreate(temp File, name String, schema Schema, spec PartitionSpec, sortOrder SortOrder) : Transaction extracted from public beginCreate(temp File, name String, schema Schema, spec PartitionSpec) : Transaction in class org.apache.iceberg.TestTables
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/454101c3573acb9cd94d6d9a306ed99a5a324ed9/core/src/test/java/org/apache/iceberg/TestTables.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/454101c3573acb9cd94d6d9a306ed99a5a324ed9/core/src/test/java/org/apache/iceberg/TestTables.java#L70
DIRECTLY EXTRACTED OPERATION:
                                        PartitionSpec spec, SortOrder sortOrder) {
    TableOperations ops = new TestTableOperations(name, temp);
    if (ops.current() != null) {
      throw new AlreadyExistsException("Table %s already exists at location: %s", name, temp);
    }

    TableMetadata metadata = newTableMetadata(schema, spec, sortOrder, temp.toString(), ImmutableMap.of(), 1);

    return Transactions.createTableTransaction(name, ops, metadata);
  }

PARAMS COUNT: 5
IS VOID METHOD: false
FRAGMENT LENGTH: 467
FRAGMENT LINE AVG SIZE: 42.45454545454545
DEPTHS:
1 2 2 3 2 2 2 2 2 1 1 
AREA: 20
AVG DEPTH: 1.8181818181818181
NUMBER OF LINES IN FRAGMENT: 11
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: d4dee8e667368945d00ad6f0051e2f492c9d59a5
URL: https://github.com/apache/incubator-iceberg/commit/d4dee8e667368945d00ad6f0051e2f492c9d59a5
DESCRIPTION: Extract Method	private loadIcebergTable(tablePath ObjectPath) : Table extracted from public getTable(tablePath ObjectPath) : CatalogBaseTable in class org.apache.iceberg.flink.FlinkCatalog
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/d4dee8e667368945d00ad6f0051e2f492c9d59a5/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/d4dee8e667368945d00ad6f0051e2f492c9d59a5/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java#L295
DIRECTLY EXTRACTED OPERATION:
    try {
      return icebergCatalog.loadTable(toIdentifier(tablePath));
    } catch (org.apache.iceberg.exceptions.NoSuchTableException e) {
      throw new TableNotExistException(getName(), tablePath, e);
    }
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 219
FRAGMENT LINE AVG SIZE: 31.285714285714285
DEPTHS:
2 3 3 3 2 1 1 
AREA: 15
AVG DEPTH: 2.142857142857143
NUMBER OF LINES IN FRAGMENT: 7
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	package toCatalogTable(table Table) : CatalogTable extracted from public getTable(tablePath ObjectPath) : CatalogBaseTable in class org.apache.iceberg.flink.FlinkCatalog
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/d4dee8e667368945d00ad6f0051e2f492c9d59a5/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/d4dee8e667368945d00ad6f0051e2f492c9d59a5/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java#L497
DIRECTLY EXTRACTED OPERATION:
    TableSchema schema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(table.schema()));
    List<String> partitionKeys = toPartitionKeys(table.spec(), table.schema());

    // NOTE: We can not create a IcebergCatalogTable extends CatalogTable, because Flink optimizer may use
    // CatalogTableImpl to copy a new catalog table.
    // Let's re-loading table from Iceberg catalog when creating source/sink operators.
    // Iceberg does not have Table comment, so pass a null (Default comment value in Flink).
    return new CatalogTableImpl(schema, partitionKeys, table.properties(), null);
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 601
FRAGMENT LINE AVG SIZE: 60.1
DEPTHS:
1 2 2 2 2 2 2 2 1 1 
AREA: 17
AVG DEPTH: 1.7
NUMBER OF LINES IN FRAGMENT: 10
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 3172585e42f50f312b3a0a4abbd12fb33a0ddf29
URL: https://github.com/apache/incubator-iceberg/commit/3172585e42f50f312b3a0a4abbd12fb33a0ddf29
DESCRIPTION: Extract Method	private rightAfterSnapshot(snapshotId long) : Long extracted from private rightAfterSnapshot() : Long in class org.apache.iceberg.actions.TestExpireSnapshotsAction
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/3172585e42f50f312b3a0a4abbd12fb33a0ddf29/spark/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/3172585e42f50f312b3a0a4abbd12fb33a0ddf29/spark/src/test/java/org/apache/iceberg/actions/TestExpireSnapshotsAction.java#L116
DIRECTLY EXTRACTED OPERATION:
    Long end = System.currentTimeMillis();
    while (end <= table.snapshot(snapshotId).timestampMillis()) {
      end = System.currentTimeMillis();
    }
    return end;
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 176
FRAGMENT LINE AVG SIZE: 25.142857142857142
DEPTHS:
1 2 3 2 2 1 1 
AREA: 12
AVG DEPTH: 1.7142857142857142
NUMBER OF LINES IN FRAGMENT: 7
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 1bcee68fafba40135d5e231d23317c86cda90ff4
URL: https://github.com/apache/incubator-iceberg/commit/1bcee68fafba40135d5e231d23317c86cda90ff4
DESCRIPTION: Extract Method	public expire() : Dataset<Row> extracted from public execute() : ExpireSnapshotsActionResult in class org.apache.iceberg.actions.ExpireSnapshotsAction
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/1bcee68fafba40135d5e231d23317c86cda90ff4/spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/1bcee68fafba40135d5e231d23317c86cda90ff4/spark/src/main/java/org/apache/iceberg/actions/ExpireSnapshotsAction.java#L150
DIRECTLY EXTRACTED OPERATION:
   * Expires snapshots and commits the changes to the table, returning a Dataset of files to delete.
   * <p>
   * This does not delete data files. To delete data files, run {@link #execute()}.
   * <p>
   * This may be called before or after {@link #execute()} is called to return the expired file list.
   *
   * @return a Dataset of files that are no longer referenced by the table
   */
  public Dataset<Row> expire() {
    if (expiredFiles == null) {
      // Metadata before Expiration
      Dataset<Row> originalFiles = buildValidFileDF(ops.current());

      // Perform Expiration
      ExpireSnapshots expireSnaps = table.expireSnapshots().cleanExpiredFiles(false);
      for (final Long id : expireSnapshotIdValues) {
        expireSnaps = expireSnaps.expireSnapshotId(id);
      }

      if (expireOlderThanValue != null) {
        expireSnaps = expireSnaps.expireOlderThan(expireOlderThanValue);
      }

      if (retainLastValue != null) {
        expireSnaps = expireSnaps.retainLast(retainLastValue);
      }

      expireSnaps.commit();

      // Metadata after Expiration
      Dataset<Row> validFiles = buildValidFileDF(ops.refresh());

      this.expiredFiles = originalFiles.except(validFiles);
    }

    return expiredFiles;
  }

IS VOID METHOD: false
FRAGMENT LENGTH: 1253
FRAGMENT LINE AVG SIZE: 32.973684210526315
DEPTHS:
0 1 1 1 1 1 1 1 1 2 3 3 3 3 3 3 4 3 3 3 4 3 3 3 4 3 3 3 3 3 3 3 3 2 2 2 1 1 
AREA: 90
AVG DEPTH: 2.3684210526315788
NUMBER OF LINES IN FRAGMENT: 38
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 6e2692c02bc79e1cfc40d03a9703776d9e92d2a0
URL: https://github.com/apache/incubator-iceberg/commit/6e2692c02bc79e1cfc40d03a9703776d9e92d2a0
DESCRIPTION: Extract Method	private buildNestedTestRecord() : Record extracted from public testMetricsForNestedStructFields() : void in class org.apache.iceberg.TestMetrics
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/6e2692c02bc79e1cfc40d03a9703776d9e92d2a0/core/src/test/java/org/apache/iceberg/TestMetrics.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/6e2692c02bc79e1cfc40d03a9703776d9e92d2a0/core/src/test/java/org/apache/iceberg/TestMetrics.java#L289
DIRECTLY EXTRACTED OPERATION:
    Record leafStruct = GenericRecord.create(LEAF_STRUCT_TYPE);
    leafStruct.setField("leafLongCol", 20L);
    leafStruct.setField("leafBinaryCol", ByteBuffer.wrap("A".getBytes()));
    Record nestedStruct = GenericRecord.create(NESTED_STRUCT_TYPE);
    nestedStruct.setField("longCol", 100L);
    nestedStruct.setField("leafStructCol", leafStruct);
    Record record = GenericRecord.create(NESTED_SCHEMA);
    record.setField("intCol", Integer.MAX_VALUE);
    record.setField("nestedStructCol", nestedStruct);

    return record;
  }

IS VOID METHOD: false
FRAGMENT LENGTH: 538
FRAGMENT LINE AVG SIZE: 41.38461538461539
DEPTHS:
1 2 2 2 2 2 2 2 2 2 2 1 1 
AREA: 23
AVG DEPTH: 1.7692307692307692
NUMBER OF LINES IN FRAGMENT: 13
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public fromInputFile(file InputFile, metricsConfig MetricsConfig) : Metrics extracted from public fromInputFile(file InputFile) : Metrics in class org.apache.iceberg.orc.OrcMetrics
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/6e2692c02bc79e1cfc40d03a9703776d9e92d2a0/orc/src/main/java/org/apache/iceberg/orc/OrcMetrics.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/6e2692c02bc79e1cfc40d03a9703776d9e92d2a0/orc/src/main/java/org/apache/iceberg/orc/OrcMetrics.java#L75
DIRECTLY EXTRACTED OPERATION:
    final Configuration config = (file instanceof HadoopInputFile) ?
        ((HadoopInputFile) file).getConf() : new Configuration();
    return fromInputFile(file, config, metricsConfig);
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 195
FRAGMENT LINE AVG SIZE: 39.0
DEPTHS:
1 2 2 1 1 
AREA: 7
AVG DEPTH: 1.4
NUMBER OF LINES IN FRAGMENT: 5
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 0b9d9940dc49602da585fd6bdc937f3cb1df70c0
URL: https://github.com/apache/incubator-iceberg/commit/0b9d9940dc49602da585fd6bdc937f3cb1df70c0
DESCRIPTION: Extract Method	private openFile(task FileScanTask, fileProjection Schema) : CloseableIterable<Record> extracted from private open(task FileScanTask) : CloseableIterable<Record> in class org.apache.iceberg.data.GenericReader
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/0b9d9940dc49602da585fd6bdc937f3cb1df70c0/data/src/main/java/org/apache/iceberg/data/GenericReader.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/0b9d9940dc49602da585fd6bdc937f3cb1df70c0/data/src/main/java/org/apache/iceberg/data/GenericReader.java#L257
DIRECTLY EXTRACTED OPERATION:
    InputFile input = io.newInputFile(task.file().path().toString());
    Map<Integer, ?> partition = PartitionUtil.constantsMap(task, IdentityPartitionConverters::convertConstant);

    switch (task.file().format()) {
      case AVRO:
        Avro.ReadBuilder avro = Avro.read(input)
            .project(fileProjection)
            .createReaderFunc(
                avroSchema -> DataReader.create(fileProjection, avroSchema, partition))
            .split(task.start(), task.length());

        if (reuseContainers) {
          avro.reuseContainers();
        }

        return avro.build();

      case PARQUET:
        Parquet.ReadBuilder parquet = Parquet.read(input)
            .project(fileProjection)
            .createReaderFunc(fileSchema -> GenericParquetReaders.buildReader(fileProjection, fileSchema, partition))
            .split(task.start(), task.length())
            .filter(task.residual());

        if (reuseContainers) {
          parquet.reuseContainers();
        }

        return parquet.build();

      case ORC:
        Schema projectionWithoutConstantAndMetadataFields = TypeUtil.selectNot(fileProjection,
            Sets.union(partition.keySet(), MetadataColumns.metadataFieldIds()));
        ORC.ReadBuilder orc = ORC.read(input)
            .project(projectionWithoutConstantAndMetadataFields)
            .createReaderFunc(fileSchema -> GenericOrcReader.buildReader(fileProjection, fileSchema, partition))
            .split(task.start(), task.length())
            .filter(task.residual());

        return orc.build();

      default:
        throw new UnsupportedOperationException(String.format("Cannot read %s file: %s",
            task.file().format().name(), task.file().path()));
    }
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 1739
FRAGMENT LINE AVG SIZE: 37.0
DEPTHS:
1 2 2 2 3 3 3 3 3 3 3 3 4 3 3 3 3 3 3 3 3 3 3 3 3 4 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 1 1 
AREA: 133
AVG DEPTH: 2.8297872340425534
NUMBER OF LINES IN FRAGMENT: 47
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: e2d0d73715ff97b4a401874b5889f844d78f1508
URL: https://github.com/apache/incubator-iceberg/commit/e2d0d73715ff97b4a401874b5889f844d78f1508
DESCRIPTION: Extract Method	protected createCatalog(name String, properties Map<String,String>, hadoopConf Configuration) : Catalog extracted from public createCatalog(name String, properties Map<String,String>) : Catalog in class org.apache.iceberg.flink.FlinkCatalogFactory
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e2d0d73715ff97b4a401874b5889f844d78f1508/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalogFactory.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e2d0d73715ff97b4a401874b5889f844d78f1508/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalogFactory.java#L111
DIRECTLY EXTRACTED OPERATION:
    CatalogLoader catalogLoader = createCatalogLoader(name, properties);
    String defaultDatabase = properties.getOrDefault(DEFAULT_DATABASE, "default");
    String[] baseNamespace = properties.containsKey(BASE_NAMESPACE) ?
        Splitter.on('.').splitToList(properties.get(BASE_NAMESPACE)).toArray(new String[0]) :
        new String[0];
    boolean cacheEnabled = Boolean.parseBoolean(properties.getOrDefault("cache-enabled", "true"));
    return new FlinkCatalog(name, defaultDatabase, baseNamespace, catalogLoader, hadoopConf, cacheEnabled);
  }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 555
FRAGMENT LINE AVG SIZE: 61.666666666666664
DEPTHS:
1 2 2 2 2 2 2 1 1 
AREA: 15
AVG DEPTH: 1.6666666666666667
NUMBER OF LINES IN FRAGMENT: 9
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: d46c7d653c7e6a0e3ba3b6237a051c2d04647963
URL: https://github.com/apache/incubator-iceberg/commit/d46c7d653c7e6a0e3ba3b6237a051c2d04647963
DESCRIPTION: Extract Method	protected metadataTableName(tableName String, type MetadataTableType) : String extracted from protected metadataTableName(type MetadataTableType) : String in class org.apache.iceberg.actions.BaseAction
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/d46c7d653c7e6a0e3ba3b6237a051c2d04647963/spark/src/main/java/org/apache/iceberg/actions/BaseAction.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/d46c7d653c7e6a0e3ba3b6237a051c2d04647963/spark/src/main/java/org/apache/iceberg/actions/BaseAction.java#L44
DIRECTLY EXTRACTED OPERATION:
    if (tableName.contains("/")) {
      return tableName + "#" + type;
    } else if (tableName.startsWith("hadoop.")) {
      // for HadoopCatalog tables, use the table location to load the metadata table
      // because IcebergCatalog uses HiveCatalog when the table is identified by name
      return table().location() + "#" + type;
    } else if (tableName.startsWith("hive.")) {
      // HiveCatalog prepend a logical name which we need to drop for Spark 2.4
      return tableName.replaceFirst("hive\\.", "") + "." + type;
    } else {
      return tableName + "." + type;
    }
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 593
FRAGMENT LINE AVG SIZE: 42.357142857142854
DEPTHS:
2 3 3 3 3 3 3 3 3 3 3 2 1 1 
AREA: 36
AVG DEPTH: 2.5714285714285716
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	protected buildValidDataFileDF(spark SparkSession, tableName String) : Dataset<Row> extracted from protected buildValidDataFileDF(spark SparkSession) : Dataset<Row> in class org.apache.iceberg.actions.BaseAction
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/d46c7d653c7e6a0e3ba3b6237a051c2d04647963/spark/src/main/java/org/apache/iceberg/actions/BaseAction.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/d46c7d653c7e6a0e3ba3b6237a051c2d04647963/spark/src/main/java/org/apache/iceberg/actions/BaseAction.java#L97
DIRECTLY EXTRACTED OPERATION:
    String allDataFilesMetadataTable = metadataTableName(tableName, MetadataTableType.ALL_DATA_FILES);
    return spark.read().format("iceberg").load(allDataFilesMetadataTable).select("file_path");
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 203
FRAGMENT LINE AVG SIZE: 50.75
DEPTHS:
1 2 1 1 
AREA: 5
AVG DEPTH: 1.25
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: d026e0b7b74719c77d5a29fdd1c96064a4b1c31c
URL: https://github.com/apache/incubator-iceberg/commit/d026e0b7b74719c77d5a29fdd1c96064a4b1c31c
DESCRIPTION: Extract Method	private parseMetadataType(location String) : Pair<String,MetadataTableType> extracted from public load(location String) : Table in class org.apache.iceberg.hadoop.HadoopTables
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/d026e0b7b74719c77d5a29fdd1c96064a4b1c31c/core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/d026e0b7b74719c77d5a29fdd1c96064a4b1c31c/core/src/main/java/org/apache/iceberg/hadoop/HadoopTables.java#L90
DIRECTLY EXTRACTED OPERATION:
   * Try to resolve a metadata table, which we encode as URI fragments
   * e.g. hdfs:///warehouse/my_table#snapshots
   * @param location Path to parse
   * @return A base table name and MetadataTableType if a type is found, null if not
   */
  private Pair<String, MetadataTableType> parseMetadataType(String location) {
    int hashIndex = location.lastIndexOf('#');
    if (hashIndex != -1 & !location.endsWith("#")) {
      String baseTable = location.substring(0, hashIndex);
      String metaTable = location.substring(hashIndex + 1);
      MetadataTableType type = MetadataTableType.from(metaTable);
      return (type == null) ? null : Pair.of(baseTable, type);
    } else {
      return null;
    }
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 714
FRAGMENT LINE AVG SIZE: 42.0
DEPTHS:
0 1 1 1 1 1 2 2 3 3 3 3 3 3 2 1 1 
AREA: 31
AVG DEPTH: 1.8235294117647058
NUMBER OF LINES IN FRAGMENT: 17
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 131c9c0eebf55d02ab88dc62677e46f2d9501048
URL: https://github.com/apache/incubator-iceberg/commit/131c9c0eebf55d02ab88dc62677e46f2d9501048
DESCRIPTION: Extract Method	private assertNameMappingProjection(dataFile DataFile, tableName String) : void extracted from public testAvroReaderWithNameMapping() : void in class org.apache.iceberg.spark.source.TestNameMappingProjection
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/131c9c0eebf55d02ab88dc62677e46f2d9501048/spark2/src/test/java/org/apache/iceberg/spark/source/TestNameMappingProjection.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/131c9c0eebf55d02ab88dc62677e46f2d9501048/spark2/src/test/java/org/apache/iceberg/spark/source/TestNameMappingProjection.java#L172
DIRECTLY EXTRACTED OPERATION:
    Schema filteredSchema = new Schema(
        required(1, "name", Types.StringType.get())
    );
    NameMapping nameMapping = MappingUtil.create(filteredSchema);

    Schema tableSchema = new Schema(
        required(1, "name", Types.StringType.get()),
        optional(2, "id", Types.IntegerType.get())
    );

    Table table = catalog.createTable(
        org.apache.iceberg.catalog.TableIdentifier.of(DB_NAME, tableName),
        tableSchema,
        PartitionSpec.unpartitioned());

    table.updateProperties()
        .set(DEFAULT_NAME_MAPPING, NameMappingParser.toJson(nameMapping))
        .commit();

    table.newFastAppend().appendFile(dataFile).commit();

    List<Row> actual = spark.read().format("iceberg")
        .load(String.format("%s.%s", DB_NAME, tableName))
        .filter("name='Alice'")
        .collectAsList();

    Assert.assertEquals("Should project 1 record", 1, actual.size());
    Assert.assertEquals("Should equal to 'Alice'", "Alice", actual.get(0).getString(0));
    Assert.assertNull("should be null", actual.get(0).get(1));
  }
}
PARAMS COUNT: 2
IS VOID METHOD: true
FRAGMENT LENGTH: 1071
FRAGMENT LINE AVG SIZE: 34.54838709677419
DEPTHS:
1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 0 
AREA: 58
AVG DEPTH: 1.8709677419354838
NUMBER OF LINES IN FRAGMENT: 31
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 7a686d2c9047590a189228b84a5195db254dc101
URL: https://github.com/apache/incubator-iceberg/commit/7a686d2c9047590a189228b84a5195db254dc101
DESCRIPTION: Extract Method	public properties() : Map<String,String> extracted from public createTable(theSchema Schema, theSpec PartitionSpec) : Table in class org.apache.iceberg.mr.TestHelper
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/7a686d2c9047590a189228b84a5195db254dc101/mr/src/test/java/org/apache/iceberg/mr/TestHelper.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/7a686d2c9047590a189228b84a5195db254dc101/mr/src/test/java/org/apache/iceberg/mr/TestHelper.java#L88
DIRECTLY EXTRACTED OPERATION:
    return ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, fileFormat.name());
  }

IS VOID METHOD: false
FRAGMENT LENGTH: 89
FRAGMENT LINE AVG SIZE: 29.666666666666668
DEPTHS:
1 1 1 
AREA: 3
AVG DEPTH: 1.0
NUMBER OF LINES IN FRAGMENT: 3
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 3990daae506822dc6b85c7b6d8461be28ca5362e
URL: https://github.com/apache/incubator-iceberg/commit/3990daae506822dc6b85c7b6d8461be28ca5362e
DESCRIPTION: Extract Method	private lazyNameToId() : Map<String,Integer> extracted from private MappedFields(fields List<MappedField>) in class org.apache.iceberg.mapping.MappedFields
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/3990daae506822dc6b85c7b6d8461be28ca5362e/core/src/main/java/org/apache/iceberg/mapping/MappedFields.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/3990daae506822dc6b85c7b6d8461be28ca5362e/core/src/main/java/org/apache/iceberg/mapping/MappedFields.java#L89
DIRECTLY EXTRACTED OPERATION:
    if (nameToId == null) {
      this.nameToId = indexIds(fields);
    }
    return nameToId;
  }

IS VOID METHOD: false
FRAGMENT LENGTH: 100
FRAGMENT LINE AVG SIZE: 16.666666666666668
DEPTHS:
2 3 2 2 1 1 
AREA: 11
AVG DEPTH: 1.8333333333333333
NUMBER OF LINES IN FRAGMENT: 6
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private lazyIdToField() : Map<Integer,MappedField> extracted from private MappedFields(fields List<MappedField>) in class org.apache.iceberg.mapping.MappedFields
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/3990daae506822dc6b85c7b6d8461be28ca5362e/core/src/main/java/org/apache/iceberg/mapping/MappedFields.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/3990daae506822dc6b85c7b6d8461be28ca5362e/core/src/main/java/org/apache/iceberg/mapping/MappedFields.java#L96
DIRECTLY EXTRACTED OPERATION:
    if (idToField == null) {
      this.idToField = indexFields(fields);
    }
    return idToField;
  }

IS VOID METHOD: false
FRAGMENT LENGTH: 106
FRAGMENT LINE AVG SIZE: 17.666666666666668
DEPTHS:
2 3 2 2 1 1 
AREA: 11
AVG DEPTH: 1.8333333333333333
NUMBER OF LINES IN FRAGMENT: 6
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private lazyFieldsById() : Map<Integer,MappedField> extracted from package NameMapping(mapping MappedFields) in class org.apache.iceberg.mapping.NameMapping
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/3990daae506822dc6b85c7b6d8461be28ca5362e/core/src/main/java/org/apache/iceberg/mapping/NameMapping.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/3990daae506822dc6b85c7b6d8461be28ca5362e/core/src/main/java/org/apache/iceberg/mapping/NameMapping.java#L72
DIRECTLY EXTRACTED OPERATION:
    if (fieldsById == null) {
      this.fieldsById = MappingUtil.indexById(mapping);
    }
    return fieldsById;
  }

IS VOID METHOD: false
FRAGMENT LENGTH: 120
FRAGMENT LINE AVG SIZE: 20.0
DEPTHS:
2 3 2 2 1 1 
AREA: 11
AVG DEPTH: 1.8333333333333333
NUMBER OF LINES IN FRAGMENT: 6
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private lazyFieldsByName() : Map<String,MappedField> extracted from package NameMapping(mapping MappedFields) in class org.apache.iceberg.mapping.NameMapping
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/3990daae506822dc6b85c7b6d8461be28ca5362e/core/src/main/java/org/apache/iceberg/mapping/NameMapping.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/3990daae506822dc6b85c7b6d8461be28ca5362e/core/src/main/java/org/apache/iceberg/mapping/NameMapping.java#L79
DIRECTLY EXTRACTED OPERATION:
    if (fieldsByName == null) {
      this.fieldsByName = MappingUtil.indexByName(mapping);
    }
    return fieldsByName;
  }

IS VOID METHOD: false
FRAGMENT LENGTH: 128
FRAGMENT LINE AVG SIZE: 21.333333333333332
DEPTHS:
2 3 2 2 1 1 
AREA: 11
AVG DEPTH: 1.8333333333333333
NUMBER OF LINES IN FRAGMENT: 6
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private convertInternal(parquetSchema MessageType, nameToIdFunc Function<String[],Integer>) : Schema extracted from public convert(parquetSchema MessageType) : Schema in class org.apache.iceberg.parquet.ParquetSchemaUtil
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/3990daae506822dc6b85c7b6d8461be28ca5362e/parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/3990daae506822dc6b85c7b6d8461be28ca5362e/parquet/src/main/java/org/apache/iceberg/parquet/ParquetSchemaUtil.java#L70
DIRECTLY EXTRACTED OPERATION:
    MessageTypeToType converter = new MessageTypeToType(nameToIdFunc);
    return new Schema(
        ParquetTypeVisitor.visit(parquetSchema, converter).asNestedType().fields(),
        converter.getAliases());
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 216
FRAGMENT LINE AVG SIZE: 36.0
DEPTHS:
1 2 2 2 1 1 
AREA: 9
AVG DEPTH: 1.5
NUMBER OF LINES IN FRAGMENT: 6
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public fileMetrics(file InputFile, metricsConfig MetricsConfig, nameMapping NameMapping) : Metrics extracted from public fileMetrics(file InputFile, metricsConfig MetricsConfig) : Metrics in class org.apache.iceberg.parquet.ParquetUtil
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/3990daae506822dc6b85c7b6d8461be28ca5362e/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/3990daae506822dc6b85c7b6d8461be28ca5362e/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java#L77
DIRECTLY EXTRACTED OPERATION:
    try (ParquetFileReader reader = ParquetFileReader.open(ParquetIO.file(file))) {
      return footerMetrics(reader.getFooter(), metricsConfig, nameMapping);
    } catch (IOException e) {
      throw new RuntimeIOException(e, "Failed to read footer of file: %s", file);
    }
  }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 283
FRAGMENT LINE AVG SIZE: 40.42857142857143
DEPTHS:
2 3 3 3 2 1 1 
AREA: 15
AVG DEPTH: 2.142857142857143
NUMBER OF LINES IN FRAGMENT: 7
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private getParquetTypeWithIds(metadata ParquetMetadata, nameMapping NameMapping) : MessageType extracted from public footerMetrics(metadata ParquetMetadata, metricsConfig MetricsConfig) : Metrics in class org.apache.iceberg.parquet.ParquetUtil
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/3990daae506822dc6b85c7b6d8461be28ca5362e/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/3990daae506822dc6b85c7b6d8461be28ca5362e/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java#L156
DIRECTLY EXTRACTED OPERATION:
    MessageType type = metadata.getFileMetaData().getSchema();

    if (ParquetSchemaUtil.hasIds(type)) {
      return type;
    }

    if (nameMapping != null) {
      return ParquetSchemaUtil.applyNameMapping(type, nameMapping);
    }

    return ParquetSchemaUtil.addFallbackIds(type);
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 294
FRAGMENT LINE AVG SIZE: 22.615384615384617
DEPTHS:
1 2 2 3 2 2 2 3 2 2 2 1 1 
AREA: 25
AVG DEPTH: 1.9230769230769231
NUMBER OF LINES IN FRAGMENT: 13
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public footerMetrics(metadata ParquetMetadata, metricsConfig MetricsConfig, nameMapping NameMapping) : Metrics extracted from public footerMetrics(metadata ParquetMetadata, metricsConfig MetricsConfig) : Metrics in class org.apache.iceberg.parquet.ParquetUtil
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/3990daae506822dc6b85c7b6d8461be28ca5362e/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/3990daae506822dc6b85c7b6d8461be28ca5362e/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java#L89
DIRECTLY EXTRACTED OPERATION:
    long rowCount = 0;
    Map<Integer, Long> columnSizes = Maps.newHashMap();
    Map<Integer, Long> valueCounts = Maps.newHashMap();
    Map<Integer, Long> nullValueCounts = Maps.newHashMap();
    Map<Integer, Literal<?>> lowerBounds = Maps.newHashMap();
    Map<Integer, Literal<?>> upperBounds = Maps.newHashMap();
    Set<Integer> missingStats = Sets.newHashSet();

    // ignore metrics for fields we failed to determine reliable IDs
    MessageType parquetTypeWithIds = getParquetTypeWithIds(metadata, nameMapping);
    Schema fileSchema = ParquetSchemaUtil.convertAndPrune(parquetTypeWithIds);

    List<BlockMetaData> blocks = metadata.getBlocks();
    for (BlockMetaData block : blocks) {
      rowCount += block.getRowCount();
      for (ColumnChunkMetaData column : block.getColumns()) {
        ColumnPath path = column.getPath();

        Integer fieldId = fileSchema.aliasToId(path.toDotString());
        if (fieldId == null) {
          // fileSchema may contain a subset of columns present in the file
          // as we prune columns we could not assign ids
          continue;
        }

        increment(columnSizes, fieldId, column.getTotalSize());

        String columnName = fileSchema.findColumnName(fieldId);
        MetricsMode metricsMode = metricsConfig.columnMode(columnName);
        if (metricsMode == MetricsModes.None.get()) {
          continue;
        }
        increment(valueCounts, fieldId, column.getValueCount());

        Statistics stats = column.getStatistics();
        if (stats == null) {
          missingStats.add(fieldId);
        } else if (!stats.isEmpty()) {
          increment(nullValueCounts, fieldId, stats.getNumNulls());

          if (metricsMode != MetricsModes.Counts.get()) {
            Types.NestedField field = fileSchema.findField(fieldId);
            if (field != null && stats.hasNonNullValue() && shouldStoreBounds(path, fileSchema)) {
              Literal<?> min = ParquetConversions.fromParquetPrimitive(
                  field.type(), column.getPrimitiveType(), stats.genericGetMin());
              updateMin(lowerBounds, fieldId, field.type(), min, metricsMode);
              Literal<?> max = ParquetConversions.fromParquetPrimitive(
                  field.type(), column.getPrimitiveType(), stats.genericGetMax());
              updateMax(upperBounds, fieldId, field.type(), max, metricsMode);
            }
          }
        }
      }
    }

    // discard accumulated values if any stats were missing
    for (Integer fieldId : missingStats) {
      nullValueCounts.remove(fieldId);
      lowerBounds.remove(fieldId);
      upperBounds.remove(fieldId);
    }

    return new Metrics(rowCount, columnSizes, valueCounts, nullValueCounts,
        toBufferMap(fileSchema, lowerBounds), toBufferMap(fileSchema, upperBounds));
  }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 2814
FRAGMENT LINE AVG SIZE: 42.63636363636363
DEPTHS:
1 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 4 4 4 4 5 5 5 4 4 4 4 4 4 4 5 4 4 4 4 4 5 5 5 5 5 6 6 7 7 7 7 7 7 6 5 4 3 2 2 2 2 3 3 3 2 2 2 2 1 1 
AREA: 241
AVG DEPTH: 3.6515151515151514
NUMBER OF LINES IN FRAGMENT: 66
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public listPartition(partition SparkPartition, spec PartitionSpec, conf SerializableConfiguration, metricsConfig MetricsConfig, mapping NameMapping) : List<DataFile> extracted from public listPartition(partition SparkPartition, spec PartitionSpec, conf SerializableConfiguration, metricsConfig MetricsConfig) : List<DataFile> in class org.apache.iceberg.spark.SparkTableUtil
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/3990daae506822dc6b85c7b6d8461be28ca5362e/spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/3990daae506822dc6b85c7b6d8461be28ca5362e/spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java#L265
DIRECTLY EXTRACTED OPERATION:
   * Returns the data files in a partition by listing the partition location.
   *
   * For Parquet and ORC partitions, this will read metrics from the file footer. For Avro partitions,
   * metrics are set to null.
   *
   * @param partition a partition
   * @param conf a serializable Hadoop conf
   * @param metricsConfig a metrics conf
   * @param mapping a name mapping
   * @return a List of DataFile
   */
  public static List<DataFile> listPartition(SparkPartition partition, PartitionSpec spec,
                                             SerializableConfiguration conf, MetricsConfig metricsConfig,
                                             NameMapping mapping) {
    return listPartition(partition.values, partition.uri, partition.format, spec, conf.get(), metricsConfig, mapping);
  }

FRAGMENT LENGTH: 802
FRAGMENT LINE AVG SIZE: 47.1764705882353
DEPTHS:
0 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 
AREA: 17
AVG DEPTH: 1.0
NUMBER OF LINES IN FRAGMENT: 17
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public listPartition(partition Map<String,String>, uri String, format String, spec PartitionSpec, conf Configuration, metricsConfig MetricsConfig, mapping NameMapping) : List<DataFile> extracted from public listPartition(partition Map<String,String>, uri String, format String, spec PartitionSpec, conf Configuration, metricsConfig MetricsConfig) : List<DataFile> in class org.apache.iceberg.spark.SparkTableUtil
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/3990daae506822dc6b85c7b6d8461be28ca5362e/spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/3990daae506822dc6b85c7b6d8461be28ca5362e/spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java#L302
DIRECTLY EXTRACTED OPERATION:
   * Returns the data files in a partition by listing the partition location.
   *
   * For Parquet and ORC partitions, this will read metrics from the file footer. For Avro partitions,
   * metrics are set to null.
   *
   * @param partition partition key, e.g., "a=1/b=2"
   * @param uri partition location URI
   * @param format partition format, avro or parquet
   * @param spec a partition spec
   * @param conf a Hadoop conf
   * @param metricsConfig a metrics conf
   * @param mapping a name mapping
   * @return a List of DataFile
   */
  public static List<DataFile> listPartition(Map<String, String> partition, String uri, String format,
                                             PartitionSpec spec, Configuration conf, MetricsConfig metricsConfig,
                                             NameMapping mapping) {
    if (format.contains("avro")) {
      return listAvroPartition(partition, uri, spec, conf);
    } else if (format.contains("parquet")) {
      return listParquetPartition(partition, uri, spec, conf, metricsConfig, mapping);
    } else if (format.contains("orc")) {
      // TODO: use MetricsConfig in listOrcPartition
      // TODO: use NameMapping in listOrcPartition
      return listOrcPartition(partition, uri, spec, conf);
    } else {
      throw new UnsupportedOperationException("Unknown partition format: " + format);
    }
  }

FRAGMENT LENGTH: 1371
FRAGMENT LINE AVG SIZE: 45.7
DEPTHS:
0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 3 3 3 3 3 3 3 3 3 2 1 1 
AREA: 49
AVG DEPTH: 1.6333333333333333
NUMBER OF LINES IN FRAGMENT: 30
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: a6a511a98a6c679a7ac27c33832960ef8a4e88f1
URL: https://github.com/apache/incubator-iceberg/commit/a6a511a98a6c679a7ac27c33832960ef8a4e88f1
DESCRIPTION: Extract Method	private createIcebergStreamWriter(icebergTable Table, flinkSchema TableSchema) : OneInputStreamOperatorTestHarness<RowData,DataFile> extracted from private createIcebergStreamWriter() : OneInputStreamOperatorTestHarness<Row,DataFile> in class org.apache.iceberg.flink.TestIcebergStreamWriter
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/a6a511a98a6c679a7ac27c33832960ef8a4e88f1/flink/src/test/java/org/apache/iceberg/flink/TestIcebergStreamWriter.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/a6a511a98a6c679a7ac27c33832960ef8a4e88f1/flink/src/test/java/org/apache/iceberg/flink/TestIcebergStreamWriter.java#L310
DIRECTLY EXTRACTED OPERATION:
      Table icebergTable, TableSchema flinkSchema) throws Exception {
    IcebergStreamWriter<RowData> streamWriter = IcebergSinkUtil.createStreamWriter(icebergTable, flinkSchema);
    OneInputStreamOperatorTestHarness<RowData, DataFile> harness = new OneInputStreamOperatorTestHarness<>(
        streamWriter, 1, 1, 0);

    harness.setup();
    harness.open();

    return harness;
  }
}
PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 390
FRAGMENT LINE AVG SIZE: 35.45454545454545
DEPTHS:
1 2 2 2 2 2 2 2 2 1 0 
AREA: 18
AVG DEPTH: 1.6363636363636365
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 01986ea77b761fe2190699bba6f3dbb25fa93042
URL: https://github.com/apache/incubator-iceberg/commit/01986ea77b761fe2190699bba6f3dbb25fa93042
DESCRIPTION: Extract Method	private summaries(manifest ManifestFile, specLookup Function<Integer,PartitionSpec>) : List<FieldSummary<?>> extracted from public canContainAny(manifest ManifestFile, partitions Iterable<StructLike>, specLookup Function<Integer,PartitionSpec>) : boolean in class org.apache.iceberg.util.ManifestFileUtil
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/01986ea77b761fe2190699bba6f3dbb25fa93042/core/src/main/java/org/apache/iceberg/util/ManifestFileUtil.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/01986ea77b761fe2190699bba6f3dbb25fa93042/core/src/main/java/org/apache/iceberg/util/ManifestFileUtil.java#L136
DIRECTLY EXTRACTED OPERATION:
    Types.StructType partitionType = specLookup.apply(manifest.partitionSpecId()).partitionType();
    List<ManifestFile.PartitionFieldSummary> fieldSummaries = manifest.partitions();
    List<Types.NestedField> fields = partitionType.fields();

    List<FieldSummary<?>> summaries = Lists.newArrayListWithExpectedSize(fieldSummaries.size());
    for (int pos = 0; pos < fieldSummaries.size(); pos += 1) {
      Type.PrimitiveType primitive = fields.get(pos).type().asPrimitiveType();
      summaries.add(new FieldSummary<>(primitive, fieldSummaries.get(pos)));
    }

    return summaries;
  }
}
PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 597
FRAGMENT LINE AVG SIZE: 45.92307692307692
DEPTHS:
1 2 2 2 2 2 3 3 2 2 2 1 0 
AREA: 24
AVG DEPTH: 1.8461538461538463
NUMBER OF LINES IN FRAGMENT: 13
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 71de51805be7954dd423ec29fd4c7d6b303adc1f
URL: https://github.com/apache/incubator-iceberg/commit/71de51805be7954dd423ec29fd4c7d6b303adc1f
DESCRIPTION: Extract Method	public buildReader(expectedSchema Schema, fileSchema MessageType, setArrowValidityVector boolean, idToConstant Map<Integer,?>) : ColumnarBatchReader extracted from public buildReader(expectedSchema Schema, fileSchema MessageType, setArrowValidityVector boolean) : ColumnarBatchReader in class org.apache.iceberg.spark.data.vectorized.VectorizedSparkParquetReaders
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/71de51805be7954dd423ec29fd4c7d6b303adc1f/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/71de51805be7954dd423ec29fd4c7d6b303adc1f/spark/src/main/java/org/apache/iceberg/spark/data/vectorized/VectorizedSparkParquetReaders.java#L54
DIRECTLY EXTRACTED OPERATION:
      Schema expectedSchema,
      MessageType fileSchema,
      boolean setArrowValidityVector,
      Map<Integer, ?> idToConstant) {
    return (ColumnarBatchReader)
        TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,
            new VectorizedReaderBuilder(expectedSchema, fileSchema, setArrowValidityVector, idToConstant));
  }

PARAMS COUNT: 4
IS VOID METHOD: false
FRAGMENT LENGTH: 356
FRAGMENT LINE AVG SIZE: 39.55555555555556
DEPTHS:
0 1 1 1 2 2 2 1 1 
AREA: 11
AVG DEPTH: 1.2222222222222223
NUMBER OF LINES IN FRAGMENT: 9
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 339d83b0c492c66151b4b9324eeb488ed6a0ea02
URL: https://github.com/apache/incubator-iceberg/commit/339d83b0c492c66151b4b9324eeb488ed6a0ea02
DESCRIPTION: Extract Method	public select(struct Types.StructType, fieldIds Set<Integer>) : Types.StructType extracted from public select(schema Schema, fieldIds Set<Integer>) : Schema in class org.apache.iceberg.types.TypeUtil
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/339d83b0c492c66151b4b9324eeb488ed6a0ea02/api/src/main/java/org/apache/iceberg/types/TypeUtil.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/339d83b0c492c66151b4b9324eeb488ed6a0ea02/api/src/main/java/org/apache/iceberg/types/TypeUtil.java#L59
DIRECTLY EXTRACTED OPERATION:
    Preconditions.checkNotNull(struct, "Struct cannot be null");
    Preconditions.checkNotNull(fieldIds, "Field ids cannot be null");

    Type result = visit(struct, new PruneColumns(fieldIds));
    if (struct == result) {
      return struct;
    } else if (result != null) {
      return result.asStructType();
    }

    return Types.StructType.of();
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 361
FRAGMENT LINE AVG SIZE: 27.76923076923077
DEPTHS:
1 2 2 2 2 3 3 3 2 2 2 1 1 
AREA: 26
AVG DEPTH: 2.0
NUMBER OF LINES IN FRAGMENT: 13
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private getIdsInternal(type Type) : Set<Integer> extracted from public getProjectedIds(schema Schema) : Set<Integer> in class org.apache.iceberg.types.TypeUtil
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/339d83b0c492c66151b4b9324eeb488ed6a0ea02/api/src/main/java/org/apache/iceberg/types/TypeUtil.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/339d83b0c492c66151b4b9324eeb488ed6a0ea02/api/src/main/java/org/apache/iceberg/types/TypeUtil.java#L84
DIRECTLY EXTRACTED OPERATION:
    return visit(type, new GetProjectedIds());
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 52
FRAGMENT LINE AVG SIZE: 17.333333333333332
DEPTHS:
1 1 1 
AREA: 3
AVG DEPTH: 1.0
NUMBER OF LINES IN FRAGMENT: 3
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: a07c2b98892634e009470b4d01fd6775063a7dda
URL: https://github.com/apache/incubator-iceberg/commit/a07c2b98892634e009470b4d01fd6775063a7dda
DESCRIPTION: Extract Method	package validateTableIsIceberg(table Table, fullName String) : void extracted from protected doRefresh() : void in class org.apache.iceberg.hive.HiveTableOperations
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/a07c2b98892634e009470b4d01fd6775063a7dda/hive/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/a07c2b98892634e009470b4d01fd6775063a7dda/hive/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java#L308
DIRECTLY EXTRACTED OPERATION:
    String tableType = table.getParameters().get(TABLE_TYPE_PROP);
    NoSuchIcebergTableException.check(tableType != null && tableType.equalsIgnoreCase(ICEBERG_TABLE_TYPE_VALUE),
        "Not an iceberg table: %s (type=%s)", fullName, tableType);
    NoSuchIcebergTableException.check(table.getParameters().get(METADATA_LOCATION_PROP) != null,
        "Not an iceberg table: %s missing %s", fullName, METADATA_LOCATION_PROP);
  }
}
PARAMS COUNT: 2
IS VOID METHOD: true
FRAGMENT LENGTH: 433
FRAGMENT LINE AVG SIZE: 61.857142857142854
DEPTHS:
1 2 2 2 2 1 0 
AREA: 10
AVG DEPTH: 1.4285714285714286
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: edb8d7759d74418daa677a8c39420f29e28edb66
URL: https://github.com/apache/incubator-iceberg/commit/edb8d7759d74418daa677a8c39420f29e28edb66
DESCRIPTION: Extract Method	protected createStructReader(types List<Type>, readers List<ParquetValueReader<?>>, struct StructType) : StructReader<?,?> extracted from public struct(expected StructType, struct GroupType, fieldReaders List<ParquetValueReader<?>>) : ParquetValueReader<?> in class org.apache.iceberg.data.parquet.GenericParquetReaders.ReadBuilder
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/edb8d7759d74418daa677a8c39420f29e28edb66/data/src/main/java/org/apache/iceberg/data/parquet/GenericParquetReaders.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/edb8d7759d74418daa677a8c39420f29e28edb66/data/src/main/java/org/apache/iceberg/data/parquet/GenericParquetReaders.java#L134
DIRECTLY EXTRACTED OPERATION:
                                                    List<ParquetValueReader<?>> readers,
                                                    StructType struct) {
      return new RecordReader(types, readers, struct);
    }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 224
FRAGMENT LINE AVG SIZE: 44.8
DEPTHS:
1 2 3 2 2 
AREA: 10
AVG DEPTH: 2.0
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	protected createStructWriter(writers List<ParquetValueWriter<?>>) : StructWriter<?> extracted from public struct(struct GroupType, fieldWriters List<ParquetValueWriter<?>>) : ParquetValueWriter<?> in class org.apache.iceberg.data.parquet.GenericParquetWriter.WriteBuilder
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/edb8d7759d74418daa677a8c39420f29e28edb66/data/src/main/java/org/apache/iceberg/data/parquet/GenericParquetWriter.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/edb8d7759d74418daa677a8c39420f29e28edb66/data/src/main/java/org/apache/iceberg/data/parquet/GenericParquetWriter.java#L70
DIRECTLY EXTRACTED OPERATION:
      return new RecordWriter(writers);
    }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 47
FRAGMENT LINE AVG SIZE: 15.666666666666666
DEPTHS:
2 2 2 
AREA: 6
AVG DEPTH: 2.0
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	protected randomValue(primitive Type.PrimitiveType, rand Random) : Object extracted from public primitive(primitive Type.PrimitiveType) : Object in class org.apache.iceberg.data.RandomGenericData.RandomDataGenerator
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/edb8d7759d74418daa677a8c39420f29e28edb66/data/src/test/java/org/apache/iceberg/data/RandomGenericData.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/edb8d7759d74418daa677a8c39420f29e28edb66/data/src/test/java/org/apache/iceberg/data/RandomGenericData.java#L182
DIRECTLY EXTRACTED OPERATION:
      return RandomUtil.generatePrimitive(primitive, rand);
    }
  }
PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 70
FRAGMENT LINE AVG SIZE: 23.333333333333332
DEPTHS:
2 2 1 
AREA: 5
AVG DEPTH: 1.6666666666666667
NUMBER OF LINES IN FRAGMENT: 3
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: ffdcf09027e09460b7d7505e65aea119107934a3
URL: https://github.com/apache/incubator-iceberg/commit/ffdcf09027e09460b7d7505e65aea119107934a3
DESCRIPTION: Extract Method	private setNotNull(vector FieldVector, nullabilityHolder NullabilityHolder, idx int) : void extracted from package readBatchOfDictionaryEncodedFixedWidthBinary(vector FieldVector, typeWidth int, startOffset int, numValuesToRead int, dict Dictionary, nullabilityHolder NullabilityHolder) : void in class org.apache.iceberg.arrow.vectorized.parquet.VectorizedDictionaryEncodedParquetValuesReader
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java#L268
DIRECTLY EXTRACTED OPERATION:
    nullabilityHolder.setNotNull(idx);
    if (setArrowValidityVector) {
      BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);
    }
  }

PARAMS COUNT: 3
IS VOID METHOD: true
FRAGMENT LENGTH: 160
FRAGMENT LINE AVG SIZE: 26.666666666666668
DEPTHS:
1 2 3 2 1 1 
AREA: 10
AVG DEPTH: 1.6666666666666667
NUMBER OF LINES IN FRAGMENT: 6
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private setNotNull(vector FieldVector, nullabilityHolder NullabilityHolder, idx int) : void extracted from package readBatchOfDictionaryEncodedFixedWidthBinary(vector FieldVector, typeWidth int, startOffset int, numValuesToRead int, dict Dictionary, nullabilityHolder NullabilityHolder) : void in class org.apache.iceberg.arrow.vectorized.parquet.VectorizedDictionaryEncodedParquetValuesReader
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java#L268
DIRECTLY EXTRACTED OPERATION:
    nullabilityHolder.setNotNull(idx);
    if (setArrowValidityVector) {
      BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);
    }
  }

PARAMS COUNT: 3
IS VOID METHOD: true
FRAGMENT LENGTH: 160
FRAGMENT LINE AVG SIZE: 26.666666666666668
DEPTHS:
1 2 3 2 1 1 
AREA: 10
AVG DEPTH: 1.6666666666666667
NUMBER OF LINES IN FRAGMENT: 6
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private setFixedWidthBinary(vector FieldVector, typeWidth int, nullabilityHolder NullabilityHolder, idx int, buffer ByteBuffer) : void extracted from package readBatchOfDictionaryEncodedFixedWidthBinary(vector FieldVector, typeWidth int, startOffset int, numValuesToRead int, dict Dictionary, nullabilityHolder NullabilityHolder) : void in class org.apache.iceberg.arrow.vectorized.parquet.VectorizedDictionaryEncodedParquetValuesReader
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java#L259
DIRECTLY EXTRACTED OPERATION:
      FieldVector vector, int typeWidth, NullabilityHolder nullabilityHolder,
      int idx, ByteBuffer buffer) {
    vector.getDataBuffer()
        .setBytes(idx * typeWidth, buffer.array(),
            buffer.position() + buffer.arrayOffset(), buffer.limit() - buffer.position());
    setNotNull(vector, nullabilityHolder, idx);
  }

PARAMS COUNT: 5
IS VOID METHOD: true
FRAGMENT LENGTH: 336
FRAGMENT LINE AVG SIZE: 42.0
DEPTHS:
0 1 2 2 2 2 1 1 
AREA: 11
AVG DEPTH: 1.375
NUMBER OF LINES IN FRAGMENT: 8
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private setFixedWidthBinary(vector FieldVector, typeWidth int, nullabilityHolder NullabilityHolder, idx int, buffer ByteBuffer) : void extracted from package readBatchOfDictionaryEncodedFixedWidthBinary(vector FieldVector, typeWidth int, startOffset int, numValuesToRead int, dict Dictionary, nullabilityHolder NullabilityHolder) : void in class org.apache.iceberg.arrow.vectorized.parquet.VectorizedDictionaryEncodedParquetValuesReader
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java#L259
DIRECTLY EXTRACTED OPERATION:
      FieldVector vector, int typeWidth, NullabilityHolder nullabilityHolder,
      int idx, ByteBuffer buffer) {
    vector.getDataBuffer()
        .setBytes(idx * typeWidth, buffer.array(),
            buffer.position() + buffer.arrayOffset(), buffer.limit() - buffer.position());
    setNotNull(vector, nullabilityHolder, idx);
  }

PARAMS COUNT: 5
IS VOID METHOD: true
FRAGMENT LENGTH: 336
FRAGMENT LINE AVG SIZE: 42.0
DEPTHS:
0 1 2 2 2 2 1 1 
AREA: 11
AVG DEPTH: 1.375
NUMBER OF LINES IN FRAGMENT: 8
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private setNotNull(vector FieldVector, nullabilityHolder NullabilityHolder, idx int) : void extracted from package readBatchOfDictionaryIds(intVector IntVector, startOffset int, numValuesToRead int, nullabilityHolder NullabilityHolder) : void in class org.apache.iceberg.arrow.vectorized.parquet.VectorizedDictionaryEncodedParquetValuesReader
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java#L268
DIRECTLY EXTRACTED OPERATION:
    nullabilityHolder.setNotNull(idx);
    if (setArrowValidityVector) {
      BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);
    }
  }

PARAMS COUNT: 3
IS VOID METHOD: true
FRAGMENT LENGTH: 160
FRAGMENT LINE AVG SIZE: 26.666666666666668
DEPTHS:
1 2 3 2 1 1 
AREA: 10
AVG DEPTH: 1.6666666666666667
NUMBER OF LINES IN FRAGMENT: 6
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private setNotNull(vector FieldVector, nullabilityHolder NullabilityHolder, idx int) : void extracted from package readBatchOfDictionaryIds(intVector IntVector, startOffset int, numValuesToRead int, nullabilityHolder NullabilityHolder) : void in class org.apache.iceberg.arrow.vectorized.parquet.VectorizedDictionaryEncodedParquetValuesReader
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java#L268
DIRECTLY EXTRACTED OPERATION:
    nullabilityHolder.setNotNull(idx);
    if (setArrowValidityVector) {
      BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);
    }
  }

PARAMS COUNT: 3
IS VOID METHOD: true
FRAGMENT LENGTH: 160
FRAGMENT LINE AVG SIZE: 26.666666666666668
DEPTHS:
1 2 3 2 1 1 
AREA: 10
AVG DEPTH: 1.6666666666666667
NUMBER OF LINES IN FRAGMENT: 6
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private setNotNull(vector FieldVector, nullabilityHolder NullabilityHolder, idx int) : void extracted from package readBatchOfDictionaryEncodedLongs(vector FieldVector, startOffset int, numValuesToRead int, dict Dictionary, nullabilityHolder NullabilityHolder) : void in class org.apache.iceberg.arrow.vectorized.parquet.VectorizedDictionaryEncodedParquetValuesReader
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java#L268
DIRECTLY EXTRACTED OPERATION:
    nullabilityHolder.setNotNull(idx);
    if (setArrowValidityVector) {
      BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);
    }
  }

PARAMS COUNT: 3
IS VOID METHOD: true
FRAGMENT LENGTH: 160
FRAGMENT LINE AVG SIZE: 26.666666666666668
DEPTHS:
1 2 3 2 1 1 
AREA: 10
AVG DEPTH: 1.6666666666666667
NUMBER OF LINES IN FRAGMENT: 6
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private setNotNull(vector FieldVector, nullabilityHolder NullabilityHolder, idx int) : void extracted from package readBatchOfDictionaryEncodedLongs(vector FieldVector, startOffset int, numValuesToRead int, dict Dictionary, nullabilityHolder NullabilityHolder) : void in class org.apache.iceberg.arrow.vectorized.parquet.VectorizedDictionaryEncodedParquetValuesReader
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java#L268
DIRECTLY EXTRACTED OPERATION:
    nullabilityHolder.setNotNull(idx);
    if (setArrowValidityVector) {
      BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);
    }
  }

PARAMS COUNT: 3
IS VOID METHOD: true
FRAGMENT LENGTH: 160
FRAGMENT LINE AVG SIZE: 26.666666666666668
DEPTHS:
1 2 3 2 1 1 
AREA: 10
AVG DEPTH: 1.6666666666666667
NUMBER OF LINES IN FRAGMENT: 6
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private setNotNull(vector FieldVector, nullabilityHolder NullabilityHolder, idx int) : void extracted from package readBatchOfDictionaryEncodedTimestampMillis(vector FieldVector, startOffset int, numValuesToRead int, dict Dictionary, nullabilityHolder NullabilityHolder) : void in class org.apache.iceberg.arrow.vectorized.parquet.VectorizedDictionaryEncodedParquetValuesReader
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java#L268
DIRECTLY EXTRACTED OPERATION:
    nullabilityHolder.setNotNull(idx);
    if (setArrowValidityVector) {
      BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);
    }
  }

PARAMS COUNT: 3
IS VOID METHOD: true
FRAGMENT LENGTH: 160
FRAGMENT LINE AVG SIZE: 26.666666666666668
DEPTHS:
1 2 3 2 1 1 
AREA: 10
AVG DEPTH: 1.6666666666666667
NUMBER OF LINES IN FRAGMENT: 6
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private setNotNull(vector FieldVector, nullabilityHolder NullabilityHolder, idx int) : void extracted from package readBatchOfDictionaryEncodedTimestampMillis(vector FieldVector, startOffset int, numValuesToRead int, dict Dictionary, nullabilityHolder NullabilityHolder) : void in class org.apache.iceberg.arrow.vectorized.parquet.VectorizedDictionaryEncodedParquetValuesReader
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java#L268
DIRECTLY EXTRACTED OPERATION:
    nullabilityHolder.setNotNull(idx);
    if (setArrowValidityVector) {
      BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);
    }
  }

PARAMS COUNT: 3
IS VOID METHOD: true
FRAGMENT LENGTH: 160
FRAGMENT LINE AVG SIZE: 26.666666666666668
DEPTHS:
1 2 3 2 1 1 
AREA: 10
AVG DEPTH: 1.6666666666666667
NUMBER OF LINES IN FRAGMENT: 6
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private setNotNull(vector FieldVector, nullabilityHolder NullabilityHolder, idx int) : void extracted from package readBatchOfDictionaryEncodedIntegers(vector FieldVector, startOffset int, numValuesToRead int, dict Dictionary, nullabilityHolder NullabilityHolder) : void in class org.apache.iceberg.arrow.vectorized.parquet.VectorizedDictionaryEncodedParquetValuesReader
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java#L268
DIRECTLY EXTRACTED OPERATION:
    nullabilityHolder.setNotNull(idx);
    if (setArrowValidityVector) {
      BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);
    }
  }

PARAMS COUNT: 3
IS VOID METHOD: true
FRAGMENT LENGTH: 160
FRAGMENT LINE AVG SIZE: 26.666666666666668
DEPTHS:
1 2 3 2 1 1 
AREA: 10
AVG DEPTH: 1.6666666666666667
NUMBER OF LINES IN FRAGMENT: 6
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private setNotNull(vector FieldVector, nullabilityHolder NullabilityHolder, idx int) : void extracted from package readBatchOfDictionaryEncodedIntegers(vector FieldVector, startOffset int, numValuesToRead int, dict Dictionary, nullabilityHolder NullabilityHolder) : void in class org.apache.iceberg.arrow.vectorized.parquet.VectorizedDictionaryEncodedParquetValuesReader
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java#L268
DIRECTLY EXTRACTED OPERATION:
    nullabilityHolder.setNotNull(idx);
    if (setArrowValidityVector) {
      BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);
    }
  }

PARAMS COUNT: 3
IS VOID METHOD: true
FRAGMENT LENGTH: 160
FRAGMENT LINE AVG SIZE: 26.666666666666668
DEPTHS:
1 2 3 2 1 1 
AREA: 10
AVG DEPTH: 1.6666666666666667
NUMBER OF LINES IN FRAGMENT: 6
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private setNotNull(vector FieldVector, nullabilityHolder NullabilityHolder, idx int) : void extracted from package readBatchOfDictionaryEncodedFloats(vector FieldVector, startOffset int, numValuesToRead int, dict Dictionary, nullabilityHolder NullabilityHolder) : void in class org.apache.iceberg.arrow.vectorized.parquet.VectorizedDictionaryEncodedParquetValuesReader
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java#L268
DIRECTLY EXTRACTED OPERATION:
    nullabilityHolder.setNotNull(idx);
    if (setArrowValidityVector) {
      BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);
    }
  }

PARAMS COUNT: 3
IS VOID METHOD: true
FRAGMENT LENGTH: 160
FRAGMENT LINE AVG SIZE: 26.666666666666668
DEPTHS:
1 2 3 2 1 1 
AREA: 10
AVG DEPTH: 1.6666666666666667
NUMBER OF LINES IN FRAGMENT: 6
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private setNotNull(vector FieldVector, nullabilityHolder NullabilityHolder, idx int) : void extracted from package readBatchOfDictionaryEncodedFloats(vector FieldVector, startOffset int, numValuesToRead int, dict Dictionary, nullabilityHolder NullabilityHolder) : void in class org.apache.iceberg.arrow.vectorized.parquet.VectorizedDictionaryEncodedParquetValuesReader
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java#L268
DIRECTLY EXTRACTED OPERATION:
    nullabilityHolder.setNotNull(idx);
    if (setArrowValidityVector) {
      BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);
    }
  }

PARAMS COUNT: 3
IS VOID METHOD: true
FRAGMENT LENGTH: 160
FRAGMENT LINE AVG SIZE: 26.666666666666668
DEPTHS:
1 2 3 2 1 1 
AREA: 10
AVG DEPTH: 1.6666666666666667
NUMBER OF LINES IN FRAGMENT: 6
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private setNotNull(vector FieldVector, nullabilityHolder NullabilityHolder, idx int) : void extracted from package readBatchOfDictionaryEncodedDoubles(vector FieldVector, startOffset int, numValuesToRead int, dict Dictionary, nullabilityHolder NullabilityHolder) : void in class org.apache.iceberg.arrow.vectorized.parquet.VectorizedDictionaryEncodedParquetValuesReader
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java#L268
DIRECTLY EXTRACTED OPERATION:
    nullabilityHolder.setNotNull(idx);
    if (setArrowValidityVector) {
      BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);
    }
  }

PARAMS COUNT: 3
IS VOID METHOD: true
FRAGMENT LENGTH: 160
FRAGMENT LINE AVG SIZE: 26.666666666666668
DEPTHS:
1 2 3 2 1 1 
AREA: 10
AVG DEPTH: 1.6666666666666667
NUMBER OF LINES IN FRAGMENT: 6
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private setNotNull(vector FieldVector, nullabilityHolder NullabilityHolder, idx int) : void extracted from package readBatchOfDictionaryEncodedDoubles(vector FieldVector, startOffset int, numValuesToRead int, dict Dictionary, nullabilityHolder NullabilityHolder) : void in class org.apache.iceberg.arrow.vectorized.parquet.VectorizedDictionaryEncodedParquetValuesReader
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/ffdcf09027e09460b7d7505e65aea119107934a3/arrow/src/main/java/org/apache/iceberg/arrow/vectorized/parquet/VectorizedDictionaryEncodedParquetValuesReader.java#L268
DIRECTLY EXTRACTED OPERATION:
    nullabilityHolder.setNotNull(idx);
    if (setArrowValidityVector) {
      BitVectorHelper.setValidityBitToOne(vector.getValidityBuffer(), idx);
    }
  }

PARAMS COUNT: 3
IS VOID METHOD: true
FRAGMENT LENGTH: 160
FRAGMENT LINE AVG SIZE: 26.666666666666668
DEPTHS:
1 2 3 2 1 1 
AREA: 10
AVG DEPTH: 1.6666666666666667
NUMBER OF LINES IN FRAGMENT: 6
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	protected setupSpark(enableDictionaryEncoding boolean) : void extracted from protected setupSpark() : void in class org.apache.iceberg.spark.source.IcebergSourceBenchmark
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/ffdcf09027e09460b7d7505e65aea119107934a3/spark/src/jmh/java/org/apache/iceberg/spark/source/IcebergSourceBenchmark.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/ffdcf09027e09460b7d7505e65aea119107934a3/spark/src/jmh/java/org/apache/iceberg/spark/source/IcebergSourceBenchmark.java#L96
DIRECTLY EXTRACTED OPERATION:
    SparkSession.Builder builder = SparkSession.builder()
            .config("spark.ui.enabled", false);
    if (!enableDictionaryEncoding) {
      builder.config("parquet.dictionary.page.size", "1")
              .config("parquet.enable.dictionary", false)
              .config(TableProperties.PARQUET_DICT_SIZE_BYTES, "1");
    }
    builder.master("local");
    spark = builder.getOrCreate();
    Configuration sparkHadoopConf = spark.sessionState().newHadoopConf();
    hadoopConf.forEach(entry -> sparkHadoopConf.set(entry.getKey(), entry.getValue()));
  }

PARAMS COUNT: 1
IS VOID METHOD: true
FRAGMENT LENGTH: 565
FRAGMENT LINE AVG SIZE: 43.46153846153846
DEPTHS:
1 2 2 3 3 3 2 2 2 2 2 1 1 
AREA: 26
AVG DEPTH: 2.0
NUMBER OF LINES IN FRAGMENT: 13
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private newIterable(newGenerator Supplier<RandomDataGenerator>, schema Schema, numRecords int) : Iterable<Record> extracted from public generate(schema Schema, numRecords int, seed long) : Iterable<Record> in class org.apache.iceberg.spark.data.RandomData
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/ffdcf09027e09460b7d7505e65aea119107934a3/spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/ffdcf09027e09460b7d7505e65aea119107934a3/spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java#L108
DIRECTLY EXTRACTED OPERATION:
                                              Schema schema, int numRecords) {
    return () -> new Iterator<Record>() {
      private int count = 0;
      private RandomDataGenerator generator = newGenerator.get();

      @Override
      public boolean hasNext() {
        return count < numRecords;
      }

      @Override
      public Record next() {
        if (count >= numRecords) {
          throw new NoSuchElementException();
        }
        count += 1;
        return (Record) TypeUtil.visit(schema, generator);
      }
    };
  }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 545
FRAGMENT LINE AVG SIZE: 25.952380952380953
DEPTHS:
1 2 3 3 3 3 3 4 3 3 3 3 4 5 4 4 4 3 2 1 1 
AREA: 62
AVG DEPTH: 2.9523809523809526
NUMBER OF LINES IN FRAGMENT: 21
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	protected randomValue(primitive Type.PrimitiveType, rand Random) : Object extracted from public primitive(primitive Type.PrimitiveType) : Object in class org.apache.iceberg.spark.data.RandomData.RandomDataGenerator
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/ffdcf09027e09460b7d7505e65aea119107934a3/spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/ffdcf09027e09460b7d7505e65aea119107934a3/spark/src/test/java/org/apache/iceberg/spark/data/RandomData.java#L233
DIRECTLY EXTRACTED OPERATION:
      return RandomUtil.generatePrimitive(primitive, random);
    }
  }
PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 72
FRAGMENT LINE AVG SIZE: 24.0
DEPTHS:
2 2 1 
AREA: 5
AVG DEPTH: 1.6666666666666667
NUMBER OF LINES IN FRAGMENT: 3
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: bd606f56b14c49f39aa745534718d0cb6a50a8c9
URL: https://github.com/apache/incubator-iceberg/commit/bd606f56b14c49f39aa745534718d0cb6a50a8c9
DESCRIPTION: Extract Method	private prepareNewManifests() : Iterable<ManifestFile> extracted from public apply(base TableMetadata) : List<ManifestFile> in class org.apache.iceberg.MergingSnapshotProducer
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/bd606f56b14c49f39aa745534718d0cb6a50a8c9/core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/bd606f56b14c49f39aa745534718d0cb6a50a8c9/core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java#L300
DIRECTLY EXTRACTED OPERATION:
    Iterable<ManifestFile> newManifests;
    if (newFiles.size() > 0) {
      // add all of the new files to the summary builder
      for (DataFile file : newFiles) {
        summaryBuilder.addedFile(spec, file);
      }

      ManifestFile newManifest = newFilesAsManifest();
      newManifests = Iterables.concat(ImmutableList.of(newManifest), appendManifests, rewrittenAppendManifests);
    } else {
      newManifests = Iterables.concat(appendManifests, rewrittenAppendManifests);
    }

    return Iterables.transform(
        newManifests,
        manifest -> GenericManifestFile.copyOf(manifest).withSnapshotId(snapshotId()).build());
  }

IS VOID METHOD: false
FRAGMENT LENGTH: 648
FRAGMENT LINE AVG SIZE: 36.0
DEPTHS:
1 2 3 3 4 3 3 3 3 3 3 2 2 2 2 2 1 1 
AREA: 43
AVG DEPTH: 2.388888888888889
NUMBER OF LINES IN FRAGMENT: 18
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	package validateManifest(manifest ManifestFile, seqs Iterator<Long>, ids Iterator<Long>, expectedFiles Iterator<DataFile>, statuses Iterator<ManifestEntry.Status>) : void extracted from package validateManifest(manifest ManifestFile, seqs Iterator<Long>, ids Iterator<Long>, expectedFiles Iterator<DataFile>) : void in class org.apache.iceberg.TableTestBase
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/bd606f56b14c49f39aa745534718d0cb6a50a8c9/core/src/test/java/org/apache/iceberg/TableTestBase.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/bd606f56b14c49f39aa745534718d0cb6a50a8c9/core/src/test/java/org/apache/iceberg/TableTestBase.java#L302
DIRECTLY EXTRACTED OPERATION:
                        Iterator<Long> seqs,
                        Iterator<Long> ids,
                        Iterator<DataFile> expectedFiles,
                        Iterator<ManifestEntry.Status> statuses) {
    for (ManifestEntry<DataFile> entry : ManifestFiles.read(manifest, FILE_IO).entries()) {
      DataFile file = entry.file();
      DataFile expected = expectedFiles.next();
      if (seqs != null) {
        V1Assert.assertEquals("Sequence number should default to 0", 0, entry.sequenceNumber().longValue());
        V2Assert.assertEquals("Sequence number should match expected", seqs.next(), entry.sequenceNumber());
      }
      Assert.assertEquals("Path should match expected",
          expected.path().toString(), file.path().toString());
      Assert.assertEquals("Snapshot ID should match expected ID",
          ids.next(), entry.snapshotId());
      if (statuses != null) {
        Assert.assertEquals("Status should match expected",
            statuses.next(), entry.status());
      }
    }

    Assert.assertFalse("Should find all files in the manifest", expectedFiles.hasNext());
  }

PARAMS COUNT: 5
IS VOID METHOD: true
FRAGMENT LENGTH: 1116
FRAGMENT LINE AVG SIZE: 46.5
DEPTHS:
0 1 1 1 2 3 3 3 4 4 3 3 3 3 3 3 4 4 3 2 2 2 1 1 
AREA: 59
AVG DEPTH: 2.4583333333333335
NUMBER OF LINES IN FRAGMENT: 24
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 527240b445b23cef1a655eccbb3b2c0eb7d178c1
URL: https://github.com/apache/incubator-iceberg/commit/527240b445b23cef1a655eccbb3b2c0eb7d178c1
DESCRIPTION: Extract Method	private writeManifest(file DataFile, formatVersion int) : ManifestFile extracted from private writeManifest(formatVersion int) : ManifestFile in class org.apache.iceberg.TestManifestWriterVersions
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/527240b445b23cef1a655eccbb3b2c0eb7d178c1/core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/527240b445b23cef1a655eccbb3b2c0eb7d178c1/core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java#L254
DIRECTLY EXTRACTED OPERATION:
    OutputFile manifestFile = Files.localOutput(FileFormat.AVRO.addExtension(temp.newFile().toString()));
    ManifestWriter<DataFile> writer = ManifestFiles.write(formatVersion, SPEC, manifestFile, SNAPSHOT_ID);
    try {
      writer.add(file);
    } finally {
      writer.close();
    }
    return writer.toManifestFile();
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 332
FRAGMENT LINE AVG SIZE: 33.2
DEPTHS:
1 2 2 3 3 3 2 2 1 1 
AREA: 20
AVG DEPTH: 2.0
NUMBER OF LINES IN FRAGMENT: 10
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 0ee6f120a475ee3a7026e689886195d4a9acbf51
URL: https://github.com/apache/incubator-iceberg/commit/0ee6f120a475ee3a7026e689886195d4a9acbf51
DESCRIPTION: Extract Method	private writeFile(location String, filename String, schema Schema, records List<Record>) : DataFile extracted from private writeFile(location String, filename String, records List<Record>) : DataFile in class org.apache.iceberg.data.TestLocalScan
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/0ee6f120a475ee3a7026e689886195d4a9acbf51/data/src/test/java/org/apache/iceberg/data/TestLocalScan.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/0ee6f120a475ee3a7026e689886195d4a9acbf51/data/src/test/java/org/apache/iceberg/data/TestLocalScan.java#L399
DIRECTLY EXTRACTED OPERATION:
    Path path = new Path(location, filename);
    FileFormat fileFormat = FileFormat.fromFileName(filename);
    Preconditions.checkNotNull(fileFormat, "Cannot determine format for file: %s", filename);
    switch (fileFormat) {
      case AVRO:
        FileAppender<Record> avroAppender = Avro.write(fromPath(path, CONF))
            .schema(schema)
            .createWriterFunc(DataWriter::create)
            .named(fileFormat.name())
            .build();
        try {
          avroAppender.addAll(records);
        } finally {
          avroAppender.close();
        }

        return DataFiles.builder(PartitionSpec.unpartitioned())
            .withInputFile(HadoopInputFile.fromPath(path, CONF))
            .withMetrics(avroAppender.metrics())
            .build();

      case PARQUET:
        FileAppender<Record> parquetAppender = Parquet.write(fromPath(path, CONF))
            .schema(schema)
            .createWriterFunc(GenericParquetWriter::buildWriter)
            .build();
        try {
          parquetAppender.addAll(records);
        } finally {
          parquetAppender.close();
        }

        return DataFiles.builder(PartitionSpec.unpartitioned())
            .withInputFile(HadoopInputFile.fromPath(path, CONF))
            .withMetrics(parquetAppender.metrics())
            .build();

      case ORC:
        FileAppender<Record> orcAppender = ORC.write(fromPath(path, CONF))
            .schema(schema)
            .createWriterFunc(GenericOrcWriter::buildWriter)
            .build();
        try {
          orcAppender.addAll(records);
        } finally {
          orcAppender.close();
        }

        return DataFiles.builder(PartitionSpec.unpartitioned())
                .withInputFile(HadoopInputFile.fromPath(path, CONF))
                .withMetrics(orcAppender.metrics())
                .build();

      default:
        throw new UnsupportedOperationException("Cannot write format: " + fileFormat);
    }
  }

PARAMS COUNT: 4
IS VOID METHOD: false
FRAGMENT LENGTH: 1966
FRAGMENT LINE AVG SIZE: 33.89655172413793
DEPTHS:
1 2 2 2 3 3 3 3 3 3 3 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 3 3 3 3 3 3 3 3 3 2 1 1 
AREA: 173
AVG DEPTH: 2.9827586206896552
NUMBER OF LINES IN FRAGMENT: 58
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: e8f7379ffe253623e9dd27a1ada7d4421af8b937
URL: https://github.com/apache/incubator-iceberg/commit/e8f7379ffe253623e9dd27a1ada7d4421af8b937
DESCRIPTION: Extract Method	public createOrcInputFile() : void extracted from public createInputFile() : void in class org.apache.iceberg.data.TestMetricsRowGroupFilter
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e8f7379ffe253623e9dd27a1ada7d4421af8b937/data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilter.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e8f7379ffe253623e9dd27a1ada7d4421af8b937/data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilter.java#L170
DIRECTLY EXTRACTED OPERATION:
    if (orcFile.exists()) {
      Assert.assertTrue(orcFile.delete());
    }

    OutputFile outFile = Files.localOutput(orcFile);
    try (FileAppender<GenericRecord> appender = ORC.write(outFile)
        .schema(FILE_SCHEMA)
        .createWriterFunc(GenericOrcWriter::buildWriter)
        .build()) {
      GenericRecord record = GenericRecord.create(FILE_SCHEMA);
      // create 50 records
      for (int i = 0; i < INT_MAX_VALUE - INT_MIN_VALUE + 1; i += 1) {
        record.setField("_id", INT_MIN_VALUE + i); // min=30, max=79, num-nulls=0
        record.setField("_no_stats_parquet", TOO_LONG_FOR_STATS_PARQUET); // value longer than 4k will produce no stats
                                                                          // in Parquet, but will produce stats for ORC
        record.setField("_required", "req"); // required, always non-null
        record.setField("_all_nulls", null); // never non-null
        record.setField("_some_nulls", (i % 10 == 0) ? null : "some"); // includes some null values
        record.setField("_no_nulls", ""); // optional, but always non-null
        record.setField("_str", i + "str" + i);

        GenericRecord structNotNull = GenericRecord.create(_structFieldType);
        structNotNull.setField("_int_field", INT_MIN_VALUE + i);
        record.setField("_struct_not_null", structNotNull); // struct with int

        appender.add(record);
      }
    }

    InputFile inFile = Files.localInput(orcFile);
    try (Reader reader = OrcFile.createReader(new Path(inFile.location()),
        OrcFile.readerOptions(new Configuration()))) {
      Assert.assertEquals("Should create only one stripe", 1, reader.getStripes().size());
    }

    orcFile.deleteOnExit();
  }

IS VOID METHOD: true
FRAGMENT LENGTH: 1728
FRAGMENT LINE AVG SIZE: 45.473684210526315
DEPTHS:
2 3 2 2 2 2 2 2 2 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 2 2 2 2 2 3 2 2 2 1 1 
AREA: 108
AVG DEPTH: 2.8421052631578947
NUMBER OF LINES IN FRAGMENT: 38
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private createParquetInputFile() : void extracted from public createInputFile() : void in class org.apache.iceberg.data.TestMetricsRowGroupFilter
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e8f7379ffe253623e9dd27a1ada7d4421af8b937/data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilter.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e8f7379ffe253623e9dd27a1ada7d4421af8b937/data/src/test/java/org/apache/iceberg/data/TestMetricsRowGroupFilter.java#L209
DIRECTLY EXTRACTED OPERATION:
    if (parquetFile.exists()) {
      Assert.assertTrue(parquetFile.delete());
    }

    // build struct field schema
    org.apache.avro.Schema structSchema = AvroSchemaUtil.convert(_structFieldType);

    OutputFile outFile = Files.localOutput(parquetFile);
    try (FileAppender<Record> appender = Parquet.write(outFile)
        .schema(FILE_SCHEMA)
        .build()) {
      GenericRecordBuilder builder = new GenericRecordBuilder(convert(FILE_SCHEMA, "table"));
      // create 50 records
      for (int i = 0; i < INT_MAX_VALUE - INT_MIN_VALUE + 1; i += 1) {
        builder.set("_id", INT_MIN_VALUE + i); // min=30, max=79, num-nulls=0
        builder.set("_no_stats_parquet", TOO_LONG_FOR_STATS_PARQUET); // value longer than 4k will produce no stats
                                                                      // in Parquet
        builder.set("_required", "req"); // required, always non-null
        builder.set("_all_nulls", null); // never non-null
        builder.set("_some_nulls", (i % 10 == 0) ? null : "some"); // includes some null values
        builder.set("_no_nulls", ""); // optional, but always non-null
        builder.set("_str", i + "str" + i);

        Record structNotNull = new Record(structSchema);
        structNotNull.put("_int_field", INT_MIN_VALUE + i);
        builder.set("_struct_not_null", structNotNull); // struct with int

        appender.add(builder.build());
      }
    }

    InputFile inFile = Files.localInput(parquetFile);
    try (ParquetFileReader reader = ParquetFileReader.open(parquetInputFile(inFile))) {
      Assert.assertEquals("Should create only one row group", 1, reader.getRowGroups().size());
      rowGroupMetadata = reader.getRowGroups().get(0);
      parquetSchema = reader.getFileMetaData().getSchema();
    }

    parquetFile.deleteOnExit();
  }

IS VOID METHOD: true
FRAGMENT LENGTH: 1829
FRAGMENT LINE AVG SIZE: 44.609756097560975
DEPTHS:
2 3 2 2 2 2 2 2 2 2 2 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 2 2 2 2 3 3 3 2 2 2 1 1 
AREA: 116
AVG DEPTH: 2.8292682926829267
NUMBER OF LINES IN FRAGMENT: 41
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 8db4a61a091b9e713153ba69ce1a85015b21538f
URL: https://github.com/apache/incubator-iceberg/commit/8db4a61a091b9e713153ba69ce1a85015b21538f
DESCRIPTION: Extract Method	package validateSnapshot(old Snapshot, snap Snapshot, sequenceNumber Long, newFiles DataFile...) : void extracted from package validateSnapshot(old Snapshot, snap Snapshot, newFiles DataFile...) : void in class org.apache.iceberg.TableTestBase
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/8db4a61a091b9e713153ba69ce1a85015b21538f/core/src/test/java/org/apache/iceberg/TableTestBase.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/8db4a61a091b9e713153ba69ce1a85015b21538f/core/src/test/java/org/apache/iceberg/TableTestBase.java#L226
DIRECTLY EXTRACTED OPERATION:
    List<ManifestFile> oldManifests = old != null ? old.manifests() : ImmutableList.of();

    // copy the manifests to a modifiable list and remove the existing manifests
    List<ManifestFile> newManifests = Lists.newArrayList(snap.manifests());
    for (ManifestFile oldManifest : oldManifests) {
      Assert.assertTrue("New snapshot should contain old manifests",
          newManifests.remove(oldManifest));
    }

    Assert.assertEquals("Should create 1 new manifest and reuse old manifests",
        1, newManifests.size());
    ManifestFile manifest = newManifests.get(0);

    long id = snap.snapshotId();
    Iterator<String> newPaths = paths(newFiles).iterator();

    for (ManifestEntry entry : ManifestFiles.read(manifest, FILE_IO).entries()) {
      DataFile file = entry.file();
      if (sequenceNumber != null) {
        V1Assert.assertEquals("Sequence number should default to 0", 0, entry.sequenceNumber().longValue());
        V2Assert.assertEquals("Sequence number should match expected", sequenceNumber, entry.sequenceNumber());
      }
      Assert.assertEquals("Path should match expected", newPaths.next(), file.path().toString());
      Assert.assertEquals("File's snapshot ID should match", id, (long) entry.snapshotId());
    }

    Assert.assertFalse("Should find all files in the manifest", newPaths.hasNext());
  }

PARAMS COUNT: 4
IS VOID METHOD: true
FRAGMENT LENGTH: 1349
FRAGMENT LINE AVG SIZE: 46.51724137931034
DEPTHS:
1 2 2 2 2 3 3 2 2 2 2 2 2 2 2 2 2 3 3 4 4 3 3 3 2 2 2 1 1 
AREA: 66
AVG DEPTH: 2.2758620689655173
NUMBER OF LINES IN FRAGMENT: 29
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	package validateManifest(manifest ManifestFile, seqs Iterator<Long>, ids Iterator<Long>, expectedFiles Iterator<DataFile>) : void extracted from package validateManifest(manifest ManifestFile, ids Iterator<Long>, expectedFiles Iterator<DataFile>) : void in class org.apache.iceberg.TableTestBase
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/8db4a61a091b9e713153ba69ce1a85015b21538f/core/src/test/java/org/apache/iceberg/TableTestBase.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/8db4a61a091b9e713153ba69ce1a85015b21538f/core/src/test/java/org/apache/iceberg/TableTestBase.java#L282
DIRECTLY EXTRACTED OPERATION:
                        Iterator<Long> seqs,
                        Iterator<Long> ids,
                        Iterator<DataFile> expectedFiles) {
    for (ManifestEntry entry : ManifestFiles.read(manifest, FILE_IO).entries()) {
      DataFile file = entry.file();
      DataFile expected = expectedFiles.next();
      if (seqs != null) {
        V1Assert.assertEquals("Sequence number should default to 0", 0, entry.sequenceNumber().longValue());
        V2Assert.assertEquals("Sequence number should match expected", seqs.next(), entry.sequenceNumber());
      }
      Assert.assertEquals("Path should match expected",
          expected.path().toString(), file.path().toString());
      Assert.assertEquals("Snapshot ID should match expected ID",
          ids.next(), entry.snapshotId());
    }

    Assert.assertFalse("Should find all files in the manifest", expectedFiles.hasNext());
  }

PARAMS COUNT: 4
IS VOID METHOD: true
FRAGMENT LENGTH: 897
FRAGMENT LINE AVG SIZE: 47.21052631578947
DEPTHS:
0 1 1 2 3 3 3 4 4 3 3 3 3 3 2 2 2 1 1 
AREA: 44
AVG DEPTH: 2.3157894736842106
NUMBER OF LINES IN FRAGMENT: 19
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 55a4e5725387ac618015cfa586e5c4e25ed8ba9a
URL: https://github.com/apache/incubator-iceberg/commit/55a4e5725387ac618015cfa586e5c4e25ed8ba9a
DESCRIPTION: Extract Method	package writeManifest(snapshotId Long, files DataFile...) : ManifestFile extracted from package writeManifest(files DataFile...) : ManifestFile in class org.apache.iceberg.TableTestBase
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/55a4e5725387ac618015cfa586e5c4e25ed8ba9a/core/src/test/java/org/apache/iceberg/TableTestBase.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/55a4e5725387ac618015cfa586e5c4e25ed8ba9a/core/src/test/java/org/apache/iceberg/TableTestBase.java#L138
DIRECTLY EXTRACTED OPERATION:
    File manifestFile = temp.newFile("input.m0.avro");
    Assert.assertTrue(manifestFile.delete());
    OutputFile outputFile = table.ops().io().newOutputFile(manifestFile.getCanonicalPath());

    ManifestWriter writer = ManifestFiles.write(formatVersion, table.spec(), outputFile, snapshotId);
    try {
      for (DataFile file : files) {
        writer.add(file);
      }
    } finally {
      writer.close();
    }

    return writer.toManifestFile();
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 463
FRAGMENT LINE AVG SIZE: 28.9375
DEPTHS:
1 2 2 2 2 2 3 4 3 3 3 2 2 2 1 1 
AREA: 35
AVG DEPTH: 2.1875
NUMBER OF LINES IN FRAGMENT: 16
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	package writeManifest(snapshotId Long, fileName String, entries ManifestEntry...) : ManifestFile extracted from package writeManifest(fileName String, entries ManifestEntry...) : ManifestFile in class org.apache.iceberg.TableTestBase
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/55a4e5725387ac618015cfa586e5c4e25ed8ba9a/core/src/test/java/org/apache/iceberg/TableTestBase.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/55a4e5725387ac618015cfa586e5c4e25ed8ba9a/core/src/test/java/org/apache/iceberg/TableTestBase.java#L163
DIRECTLY EXTRACTED OPERATION:
    File manifestFile = temp.newFile(fileName);
    Assert.assertTrue(manifestFile.delete());
    OutputFile outputFile = table.ops().io().newOutputFile(manifestFile.getCanonicalPath());

    ManifestWriter writer = ManifestFiles.write(formatVersion, table.spec(), outputFile, snapshotId);
    try {
      for (ManifestEntry entry : entries) {
        writer.addEntry(entry);
      }
    } finally {
      writer.close();
    }

    return writer.toManifestFile();
  }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 470
FRAGMENT LINE AVG SIZE: 29.375
DEPTHS:
1 2 2 2 2 2 3 4 3 3 3 2 2 2 1 1 
AREA: 35
AVG DEPTH: 2.1875
NUMBER OF LINES IN FRAGMENT: 16
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: e9a9032fc686fc9082e5db2dcafde3a72f0abfa9
URL: https://github.com/apache/incubator-iceberg/commit/e9a9032fc686fc9082e5db2dcafde3a72f0abfa9
DESCRIPTION: Extract Method	public beforeField(type Type) : void extracted from public visit(type Type, visitor ParquetTypeVisitor<T>) : T in class org.apache.iceberg.parquet.ParquetTypeVisitor
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e9a9032fc686fc9082e5db2dcafde3a72f0abfa9/parquet/src/main/java/org/apache/iceberg/parquet/ParquetTypeVisitor.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e9a9032fc686fc9082e5db2dcafde3a72f0abfa9/parquet/src/main/java/org/apache/iceberg/parquet/ParquetTypeVisitor.java#L197
DIRECTLY EXTRACTED OPERATION:
    fieldNames.push(type.getName());
  }

PARAMS COUNT: 1
IS VOID METHOD: true
FRAGMENT LENGTH: 42
FRAGMENT LINE AVG SIZE: 14.0
DEPTHS:
1 1 1 
AREA: 3
AVG DEPTH: 1.0
NUMBER OF LINES IN FRAGMENT: 3
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public beforeField(type Type) : void extracted from public visit(type Type, visitor ParquetTypeVisitor<T>) : T in class org.apache.iceberg.parquet.ParquetTypeVisitor
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e9a9032fc686fc9082e5db2dcafde3a72f0abfa9/parquet/src/main/java/org/apache/iceberg/parquet/ParquetTypeVisitor.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e9a9032fc686fc9082e5db2dcafde3a72f0abfa9/parquet/src/main/java/org/apache/iceberg/parquet/ParquetTypeVisitor.java#L197
DIRECTLY EXTRACTED OPERATION:
    fieldNames.push(type.getName());
  }

PARAMS COUNT: 1
IS VOID METHOD: true
FRAGMENT LENGTH: 42
FRAGMENT LINE AVG SIZE: 14.0
DEPTHS:
1 1 1 
AREA: 3
AVG DEPTH: 1.0
NUMBER OF LINES IN FRAGMENT: 3
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public afterField(type Type) : void extracted from public visit(type Type, visitor ParquetTypeVisitor<T>) : T in class org.apache.iceberg.parquet.ParquetTypeVisitor
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e9a9032fc686fc9082e5db2dcafde3a72f0abfa9/parquet/src/main/java/org/apache/iceberg/parquet/ParquetTypeVisitor.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e9a9032fc686fc9082e5db2dcafde3a72f0abfa9/parquet/src/main/java/org/apache/iceberg/parquet/ParquetTypeVisitor.java#L201
DIRECTLY EXTRACTED OPERATION:
    fieldNames.pop();
  }

PARAMS COUNT: 1
IS VOID METHOD: true
FRAGMENT LENGTH: 27
FRAGMENT LINE AVG SIZE: 9.0
DEPTHS:
1 1 1 
AREA: 3
AVG DEPTH: 1.0
NUMBER OF LINES IN FRAGMENT: 3
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public afterField(type Type) : void extracted from public visit(type Type, visitor ParquetTypeVisitor<T>) : T in class org.apache.iceberg.parquet.ParquetTypeVisitor
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e9a9032fc686fc9082e5db2dcafde3a72f0abfa9/parquet/src/main/java/org/apache/iceberg/parquet/ParquetTypeVisitor.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e9a9032fc686fc9082e5db2dcafde3a72f0abfa9/parquet/src/main/java/org/apache/iceberg/parquet/ParquetTypeVisitor.java#L201
DIRECTLY EXTRACTED OPERATION:
    fieldNames.pop();
  }

PARAMS COUNT: 1
IS VOID METHOD: true
FRAGMENT LENGTH: 27
FRAGMENT LINE AVG SIZE: 9.0
DEPTHS:
1 1 1 
AREA: 3
AVG DEPTH: 1.0
NUMBER OF LINES IN FRAGMENT: 3
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private visitList(list GroupType, visitor ParquetTypeVisitor<T>) : T extracted from public visit(type Type, visitor ParquetTypeVisitor<T>) : T in class org.apache.iceberg.parquet.ParquetTypeVisitor
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e9a9032fc686fc9082e5db2dcafde3a72f0abfa9/parquet/src/main/java/org/apache/iceberg/parquet/ParquetTypeVisitor.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e9a9032fc686fc9082e5db2dcafde3a72f0abfa9/parquet/src/main/java/org/apache/iceberg/parquet/ParquetTypeVisitor.java#L63
DIRECTLY EXTRACTED OPERATION:
    Preconditions.checkArgument(!list.isRepetition(Type.Repetition.REPEATED),
        "Invalid list: top-level group is repeated: %s", list);
    Preconditions.checkArgument(list.getFieldCount() == 1,
        "Invalid list: does not contain single repeated field: %s", list);

    GroupType repeatedElement = list.getFields().get(0).asGroupType();
    Preconditions.checkArgument(repeatedElement.isRepetition(Type.Repetition.REPEATED),
        "Invalid list: inner group is not repeated");
    Preconditions.checkArgument(repeatedElement.getFieldCount() <= 1,
        "Invalid list: repeated group is not a single field: %s", list);

    visitor.beforeRepeatedElement(repeatedElement);
    try {
      T elementResult = null;
      if (repeatedElement.getFieldCount() > 0) {
        Type elementField = repeatedElement.getType(0);
        visitor.beforeElementField(elementField);
        try {
          elementResult = visit(elementField, visitor);
        } finally {
          visitor.afterElementField(elementField);
        }
      }

      return visitor.list(list, elementResult);

    } finally {
      visitor.afterRepeatedElement(repeatedElement);
    }
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 1170
FRAGMENT LINE AVG SIZE: 37.74193548387097
DEPTHS:
1 2 2 2 2 2 2 2 2 2 2 2 2 3 3 4 4 4 5 5 5 4 3 3 3 3 3 3 2 1 1 
AREA: 84
AVG DEPTH: 2.7096774193548385
NUMBER OF LINES IN FRAGMENT: 31
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private visitMap(map GroupType, visitor ParquetTypeVisitor<T>) : T extracted from public visit(type Type, visitor ParquetTypeVisitor<T>) : T in class org.apache.iceberg.parquet.ParquetTypeVisitor
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e9a9032fc686fc9082e5db2dcafde3a72f0abfa9/parquet/src/main/java/org/apache/iceberg/parquet/ParquetTypeVisitor.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e9a9032fc686fc9082e5db2dcafde3a72f0abfa9/parquet/src/main/java/org/apache/iceberg/parquet/ParquetTypeVisitor.java#L95
DIRECTLY EXTRACTED OPERATION:
    Preconditions.checkArgument(!map.isRepetition(Type.Repetition.REPEATED),
        "Invalid map: top-level group is repeated: %s", map);
    Preconditions.checkArgument(map.getFieldCount() == 1,
        "Invalid map: does not contain single repeated field: %s", map);

    GroupType repeatedKeyValue = map.getType(0).asGroupType();
    Preconditions.checkArgument(repeatedKeyValue.isRepetition(Type.Repetition.REPEATED),
        "Invalid map: inner group is not repeated");
    Preconditions.checkArgument(repeatedKeyValue.getFieldCount() <= 2,
        "Invalid map: repeated group does not have 2 fields");

    visitor.beforeRepeatedKeyValue(repeatedKeyValue);
    try {
      T keyResult = null;
      T valueResult = null;
      switch (repeatedKeyValue.getFieldCount()) {
        case 2:
          // if there are 2 fields, both key and value are projected
          Type keyType = repeatedKeyValue.getType(0);
          visitor.beforeKeyField(keyType);
          try {
            keyResult = visit(keyType, visitor);
          } finally {
            visitor.afterKeyField(keyType);
          }
          Type valueType = repeatedKeyValue.getType(1);
          visitor.beforeValueField(valueType);
          try {
            valueResult = visit(valueType, visitor);
          } finally {
            visitor.afterValueField(valueType);
          }
          break;

        case 1:
          // if there is just one, use the name to determine what it is
          Type keyOrValue = repeatedKeyValue.getType(0);
          if (keyOrValue.getName().equalsIgnoreCase("key")) {
            visitor.beforeKeyField(keyOrValue);
            try {
              keyResult = visit(keyOrValue, visitor);
            } finally {
              visitor.afterKeyField(keyOrValue);
            }
            // value result remains null
          } else {
            visitor.beforeValueField(keyOrValue);
            try {
              valueResult = visit(keyOrValue, visitor);
            } finally {
              visitor.afterValueField(keyOrValue);
            }
            // key result remains null
          }
          break;

        default:
          // both results will remain null
      }

      return visitor.map(map, keyResult, valueResult);

    } finally {
      visitor.afterRepeatedKeyValue(repeatedKeyValue);
    }
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 2339
FRAGMENT LINE AVG SIZE: 34.91044776119403
DEPTHS:
1 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 4 4 4 4 4 5 5 5 4 4 4 4 5 5 5 4 4 4 4 4 4 4 5 5 6 6 6 5 5 5 5 5 6 6 6 5 5 4 4 4 4 4 3 3 3 3 3 3 2 1 1 
AREA: 251
AVG DEPTH: 3.746268656716418
NUMBER OF LINES IN FRAGMENT: 67
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 699e68a919c708ccbb43179928e3e7fb4fec58f6
URL: https://github.com/apache/incubator-iceberg/commit/699e68a919c708ccbb43179928e3e7fb4fec58f6
DESCRIPTION: Extract Method	private newOption(fieldType Type, writer ParquetValueWriter<?>) : ParquetValueWriter<?> extracted from public struct(struct GroupType, fieldWriters List<ParquetValueWriter<?>>) : ParquetValueWriter<?> in class org.apache.iceberg.spark.data.SparkParquetWriters.WriteBuilder
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/699e68a919c708ccbb43179928e3e7fb4fec58f6/spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetWriters.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/699e68a919c708ccbb43179928e3e7fb4fec58f6/spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetWriters.java#L121
DIRECTLY EXTRACTED OPERATION:
      int maxD = type.getMaxDefinitionLevel(path(fieldType.getName()));
      return ParquetValueWriters.option(fieldType, maxD, writer);
    }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 145
FRAGMENT LINE AVG SIZE: 36.25
DEPTHS:
2 3 2 2 
AREA: 9
AVG DEPTH: 2.25
NUMBER OF LINES IN FRAGMENT: 4
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: b8037d24256cd7133d535cce0c8399acb138e30c
URL: https://github.com/apache/incubator-iceberg/commit/b8037d24256cd7133d535cce0c8399acb138e30c
DESCRIPTION: Extract Method	private updateVersionAndMetadata(newVersion int, metadataFile String) : void extracted from public refresh() : TableMetadata in class org.apache.iceberg.hadoop.HadoopTableOperations
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/b8037d24256cd7133d535cce0c8399acb138e30c/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/b8037d24256cd7133d535cce0c8399acb138e30c/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java#L83
DIRECTLY EXTRACTED OPERATION:
    // update if the current version is out of date
    if (version == null || version != newVersion) {
      this.version = newVersion;
      this.currentMetadata = checkUUID(currentMetadata, TableMetadataParser.read(io(), metadataFile));
    }
  }

PARAMS COUNT: 2
IS VOID METHOD: true
FRAGMENT LENGTH: 251
FRAGMENT LINE AVG SIZE: 35.857142857142854
DEPTHS:
1 2 3 3 2 1 1 
AREA: 13
AVG DEPTH: 1.8571428571428572
NUMBER OF LINES IN FRAGMENT: 7
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 189ea232bfbe0b4a01dda0419cf24a2ebff5dfe1
URL: https://github.com/apache/incubator-iceberg/commit/189ea232bfbe0b4a01dda0419cf24a2ebff5dfe1
DESCRIPTION: Extract Method	protected doUnlock(lockId long) : void extracted from private unlock(lockId Optional<Long>) : void in class org.apache.iceberg.hive.HiveTableOperations
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/189ea232bfbe0b4a01dda0419cf24a2ebff5dfe1/hive/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/189ea232bfbe0b4a01dda0419cf24a2ebff5dfe1/hive/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java#L307
DIRECTLY EXTRACTED OPERATION:
    metaClients.run(client -> {
      client.unlock(lockId);
      return null;
    });
  }
}
PARAMS COUNT: 1
IS VOID METHOD: true
FRAGMENT LENGTH: 94
FRAGMENT LINE AVG SIZE: 15.666666666666666
DEPTHS:
2 3 3 2 1 0 
AREA: 11
AVG DEPTH: 1.8333333333333333
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 79d4f0579dd988e8dfea9f40e53c69545afa0b58
URL: https://github.com/apache/incubator-iceberg/commit/79d4f0579dd988e8dfea9f40e53c69545afa0b58
DESCRIPTION: Extract Method	private writeAndValidateRecords(schema Schema, expected List<Record>) : void extracted from protected writeAndValidate(schema Schema) : void in class org.apache.iceberg.data.orc.TestGenericData
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/79d4f0579dd988e8dfea9f40e53c69545afa0b58/data/src/test/java/org/apache/iceberg/data/orc/TestGenericData.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/79d4f0579dd988e8dfea9f40e53c69545afa0b58/data/src/test/java/org/apache/iceberg/data/orc/TestGenericData.java#L121
DIRECTLY EXTRACTED OPERATION:
    File testFile = temp.newFile();
    Assert.assertTrue("Delete should succeed", testFile.delete());

    try (FileAppender<Record> writer = ORC.write(Files.localOutput(testFile))
        .schema(schema)
        .createWriterFunc(GenericOrcWriter::buildWriter)
        .build()) {
      for (Record rec : expected) {
        writer.add(rec);
      }
    }

    List<Record> rows;
    try (CloseableIterable<Record> reader = ORC.read(Files.localInput(testFile))
        .project(schema)
        .createReaderFunc(fileSchema -> GenericOrcReader.buildReader(schema, fileSchema))
        .build()) {
      rows = Lists.newArrayList(reader);
    }

    for (int i = 0; i < expected.size(); i += 1) {
      DataTestHelpers.assertEquals(schema.asStruct(), expected.get(i), rows.get(i));
    }
  }
}
PARAMS COUNT: 2
IS VOID METHOD: true
FRAGMENT LENGTH: 794
FRAGMENT LINE AVG SIZE: 31.76
DEPTHS:
1 2 2 2 2 2 2 3 4 3 2 2 2 2 2 2 2 3 2 2 2 3 2 1 0 
AREA: 52
AVG DEPTH: 2.08
NUMBER OF LINES IN FRAGMENT: 25
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private writeAndValidateRecords(schema Schema, expected Iterable<InternalRow>) : void extracted from protected writeAndValidate(schema Schema) : void in class org.apache.iceberg.spark.data.TestSparkOrcReader
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/79d4f0579dd988e8dfea9f40e53c69545afa0b58/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkOrcReader.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/79d4f0579dd988e8dfea9f40e53c69545afa0b58/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkOrcReader.java#L61
DIRECTLY EXTRACTED OPERATION:
    final File testFile = temp.newFile();
    Assert.assertTrue("Delete should succeed", testFile.delete());

    try (FileAppender<InternalRow> writer = ORC.write(Files.localOutput(testFile))
        .createWriterFunc(SparkOrcWriter::new)
        .schema(schema)
        .build()) {
      writer.addAll(expected);
    }

    try (CloseableIterable<InternalRow> reader = ORC.read(Files.localInput(testFile))
        .project(schema)
        .createReaderFunc(SparkOrcReader::new)
        .build()) {
      final Iterator<InternalRow> actualRows = reader.iterator();
      final Iterator<InternalRow> expectedRows = expected.iterator();
      while (expectedRows.hasNext()) {
        Assert.assertTrue("Should have expected number of rows", actualRows.hasNext());
        assertEquals(schema, expectedRows.next(), actualRows.next());
      }
      Assert.assertFalse("Should not have extra rows", actualRows.hasNext());
    }
  }
}
PARAMS COUNT: 2
IS VOID METHOD: true
FRAGMENT LENGTH: 931
FRAGMENT LINE AVG SIZE: 38.791666666666664
DEPTHS:
1 2 2 2 2 2 2 3 2 2 2 2 2 2 3 3 3 4 4 3 3 2 1 0 
AREA: 54
AVG DEPTH: 2.25
NUMBER OF LINES IN FRAGMENT: 24
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: a81d11e78fb447f68b2a3e8f98cf02b0c57c7f58
URL: https://github.com/apache/incubator-iceberg/commit/a81d11e78fb447f68b2a3e8f98cf02b0c57c7f58
DESCRIPTION: Extract Method	package newTableMetadata(schema Schema, spec PartitionSpec, location String, properties Map<String,String>, formatVersion int) : TableMetadata extracted from public newTableMetadata(schema Schema, spec PartitionSpec, location String, properties Map<String,String>) : TableMetadata in class org.apache.iceberg.TableMetadata
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/a81d11e78fb447f68b2a3e8f98cf02b0c57c7f58/core/src/main/java/org/apache/iceberg/TableMetadata.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/a81d11e78fb447f68b2a3e8f98cf02b0c57c7f58/core/src/main/java/org/apache/iceberg/TableMetadata.java#L58
DIRECTLY EXTRACTED OPERATION:
                                        PartitionSpec spec,
                                        String location,
                                        Map<String, String> properties,
                                        int formatVersion) {
    // reassign all column ids to ensure consistency
    AtomicInteger lastColumnId = new AtomicInteger(0);
    Schema freshSchema = TypeUtil.assignFreshIds(schema, lastColumnId::incrementAndGet);

    // rebuild the partition spec using the new column ids
    PartitionSpec.Builder specBuilder = PartitionSpec.builderFor(freshSchema)
        .withSpecId(INITIAL_SPEC_ID);
    for (PartitionField field : spec.fields()) {
      // look up the name of the source field in the old schema to get the new schema's id
      String sourceName = schema.findColumnName(field.sourceId());
      // reassign all partition fields with fresh partition field Ids to ensure consistency
      specBuilder.add(
          freshSchema.findField(sourceName).fieldId(),
          field.name(),
          field.transform().toString());
    }
    PartitionSpec freshSpec = specBuilder.build();

    return new TableMetadata(null, formatVersion, UUID.randomUUID().toString(), location,
        INITIAL_SEQUENCE_NUMBER, System.currentTimeMillis(),
        lastColumnId.get(), freshSchema, INITIAL_SPEC_ID, ImmutableList.of(freshSpec),
        ImmutableMap.copyOf(properties), -1, ImmutableList.of(),
        ImmutableList.of(), ImmutableList.of());
  }

PARAMS COUNT: 5
IS VOID METHOD: false
FRAGMENT LENGTH: 1480
FRAGMENT LINE AVG SIZE: 51.03448275862069
DEPTHS:
0 1 1 1 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 2 2 2 2 2 2 2 2 1 1 
AREA: 58
AVG DEPTH: 2.0
NUMBER OF LINES IN FRAGMENT: 29
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: af25cbedac34c09633a66a2c73d68f3f4822d64a
URL: https://github.com/apache/incubator-iceberg/commit/af25cbedac34c09633a66a2c73d68f3f4822d64a
DESCRIPTION: Extract Method	private applyResidualFiltering(iter CloseableIterable<T>, residual Expression, readSchema Schema) : CloseableIterable<T> extracted from private newAvroIterable(inputFile InputFile, task FileScanTask, readSchema Schema) : CloseableIterable<T> in class org.apache.iceberg.mr.mapreduce.IcebergInputFormat.IcebergRecordReader
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/af25cbedac34c09633a66a2c73d68f3f4822d64a/mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/af25cbedac34c09633a66a2c73d68f3f4822d64a/mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java#L446
DIRECTLY EXTRACTED OPERATION:
                                                        Schema readSchema) {
      boolean applyResidual = !context.getConfiguration().getBoolean(SKIP_RESIDUAL_FILTERING, false);

      if (applyResidual && residual != null && residual != Expressions.alwaysTrue()) {
        Evaluator filter = new Evaluator(readSchema.asStruct(), residual, caseSensitive);
        return CloseableIterable.filter(iter, record -> filter.eval((StructLike) record));
      } else {
        return iter;
      }
    }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 499
FRAGMENT LINE AVG SIZE: 45.36363636363637
DEPTHS:
2 3 3 3 4 4 4 4 3 2 2 
AREA: 34
AVG DEPTH: 3.090909090909091
NUMBER OF LINES IN FRAGMENT: 11
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private applyResidualFiltering(iter CloseableIterable<T>, residual Expression, readSchema Schema) : CloseableIterable<T> extracted from private newParquetIterable(inputFile InputFile, task FileScanTask, readSchema Schema) : CloseableIterable<T> in class org.apache.iceberg.mr.mapreduce.IcebergInputFormat.IcebergRecordReader
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/af25cbedac34c09633a66a2c73d68f3f4822d64a/mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/af25cbedac34c09633a66a2c73d68f3f4822d64a/mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java#L446
DIRECTLY EXTRACTED OPERATION:
                                                        Schema readSchema) {
      boolean applyResidual = !context.getConfiguration().getBoolean(SKIP_RESIDUAL_FILTERING, false);

      if (applyResidual && residual != null && residual != Expressions.alwaysTrue()) {
        Evaluator filter = new Evaluator(readSchema.asStruct(), residual, caseSensitive);
        return CloseableIterable.filter(iter, record -> filter.eval((StructLike) record));
      } else {
        return iter;
      }
    }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 499
FRAGMENT LINE AVG SIZE: 45.36363636363637
DEPTHS:
2 3 3 3 4 4 4 4 3 2 2 
AREA: 34
AVG DEPTH: 3.090909090909091
NUMBER OF LINES IN FRAGMENT: 11
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private applyResidualFiltering(iter CloseableIterable<T>, residual Expression, readSchema Schema) : CloseableIterable<T> extracted from private newOrcIterable(inputFile InputFile, task FileScanTask, readSchema Schema) : CloseableIterable<T> in class org.apache.iceberg.mr.mapreduce.IcebergInputFormat.IcebergRecordReader
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/af25cbedac34c09633a66a2c73d68f3f4822d64a/mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/af25cbedac34c09633a66a2c73d68f3f4822d64a/mr/src/main/java/org/apache/iceberg/mr/mapreduce/IcebergInputFormat.java#L446
DIRECTLY EXTRACTED OPERATION:
                                                        Schema readSchema) {
      boolean applyResidual = !context.getConfiguration().getBoolean(SKIP_RESIDUAL_FILTERING, false);

      if (applyResidual && residual != null && residual != Expressions.alwaysTrue()) {
        Evaluator filter = new Evaluator(readSchema.asStruct(), residual, caseSensitive);
        return CloseableIterable.filter(iter, record -> filter.eval((StructLike) record));
      } else {
        return iter;
      }
    }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 499
FRAGMENT LINE AVG SIZE: 45.36363636363637
DEPTHS:
2 3 3 3 4 4 4 4 3 2 2 
AREA: 34
AVG DEPTH: 3.090909090909091
NUMBER OF LINES IN FRAGMENT: 11
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: b6cdc694b6b5a1040d6a2ffb4e72446fe52309c4
URL: https://github.com/apache/incubator-iceberg/commit/b6cdc694b6b5a1040d6a2ffb4e72446fe52309c4
DESCRIPTION: Extract Method	package add(sourceId int, fieldId int, name String, transform String) : Builder extracted from package add(sourceId int, name String, transform String) : Builder in class org.apache.iceberg.PartitionSpec.Builder
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/b6cdc694b6b5a1040d6a2ffb4e72446fe52309c4/api/src/main/java/org/apache/iceberg/PartitionSpec.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/b6cdc694b6b5a1040d6a2ffb4e72446fe52309c4/api/src/main/java/org/apache/iceberg/PartitionSpec.java#L464
DIRECTLY EXTRACTED OPERATION:
      Types.NestedField column = schema.findField(sourceId);
      checkAndAddPartitionName(name, column.fieldId());
      Preconditions.checkNotNull(column, "Cannot find source column: %s", sourceId);
      fields.add(new PartitionField(sourceId, fieldId, name, Transforms.fromString(column.type(), transform)));
      lastAssignedFieldId.getAndAccumulate(fieldId, Math::max);
      return this;
    }

PARAMS COUNT: 4
IS VOID METHOD: false
FRAGMENT LENGTH: 404
FRAGMENT LINE AVG SIZE: 50.5
DEPTHS:
2 3 3 3 3 3 2 2 
AREA: 21
AVG DEPTH: 2.625
NUMBER OF LINES IN FRAGMENT: 8
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 3a35c0764a51008fa5c5ecea109c87f92106dbcf
URL: https://github.com/apache/incubator-iceberg/commit/3a35c0764a51008fa5c5ecea109c87f92106dbcf
DESCRIPTION: Extract Method	public constantsMap(task FileScanTask, convertConstant BiFunction<Type,Object,Object>) : Map<Integer,?> extracted from public constantsMap(task FileScanTask) : Map<Integer,?> in class org.apache.iceberg.util.PartitionUtil
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/3a35c0764a51008fa5c5ecea109c87f92106dbcf/core/src/main/java/org/apache/iceberg/util/PartitionUtil.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/3a35c0764a51008fa5c5ecea109c87f92106dbcf/core/src/main/java/org/apache/iceberg/util/PartitionUtil.java#L41
DIRECTLY EXTRACTED OPERATION:
    return constantsMap(task.spec(), task.file().partition(), convertConstant);
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 85
FRAGMENT LINE AVG SIZE: 28.333333333333332
DEPTHS:
1 1 1 
AREA: 3
AVG DEPTH: 1.0
NUMBER OF LINES IN FRAGMENT: 3
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public buildReader(expectedSchema Schema, fileSchema MessageType, idToConstant Map<Integer,?>) : ParquetValueReader<GenericRecord> extracted from public buildReader(expectedSchema Schema, fileSchema MessageType) : ParquetValueReader<GenericRecord> in class org.apache.iceberg.data.parquet.GenericParquetReaders
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/3a35c0764a51008fa5c5ecea109c87f92106dbcf/data/src/main/java/org/apache/iceberg/data/parquet/GenericParquetReaders.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/3a35c0764a51008fa5c5ecea109c87f92106dbcf/data/src/main/java/org/apache/iceberg/data/parquet/GenericParquetReaders.java#L73
DIRECTLY EXTRACTED OPERATION:
  public static ParquetValueReader<GenericRecord> buildReader(Schema expectedSchema,
                                                              MessageType fileSchema,
                                                              Map<Integer, ?> idToConstant) {
    if (ParquetSchemaUtil.hasIds(fileSchema)) {
      return (ParquetValueReader<GenericRecord>)
          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,
              new ReadBuilder(fileSchema, idToConstant));
    } else {
      return (ParquetValueReader<GenericRecord>)
          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,
              new FallbackReadBuilder(fileSchema, idToConstant));
    }
  }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 713
FRAGMENT LINE AVG SIZE: 50.92857142857143
DEPTHS:
0 1 1 2 3 3 3 3 3 3 3 2 1 1 
AREA: 29
AVG DEPTH: 2.0714285714285716
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public buildReader(expectedSchema Schema, fileSchema MessageType, idToConstant Map<Integer,?>) : ParquetValueReader<InternalRow> extracted from public buildReader(expectedSchema Schema, fileSchema MessageType) : ParquetValueReader<InternalRow> in class org.apache.iceberg.spark.data.SparkParquetReaders
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/3a35c0764a51008fa5c5ecea109c87f92106dbcf/spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetReaders.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/3a35c0764a51008fa5c5ecea109c87f92106dbcf/spark/src/main/java/org/apache/iceberg/spark/data/SparkParquetReaders.java#L75
DIRECTLY EXTRACTED OPERATION:
  public static ParquetValueReader<InternalRow> buildReader(Schema expectedSchema,
                                                            MessageType fileSchema,
                                                            Map<Integer, ?> idToConstant) {
    if (ParquetSchemaUtil.hasIds(fileSchema)) {
      return (ParquetValueReader<InternalRow>)
          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,
              new ReadBuilder(fileSchema, idToConstant));
    } else {
      return (ParquetValueReader<InternalRow>)
          TypeWithSchemaVisitor.visit(expectedSchema.asStruct(), fileSchema,
              new FallbackReadBuilder(fileSchema, idToConstant));
    }
  }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 703
FRAGMENT LINE AVG SIZE: 50.214285714285715
DEPTHS:
0 1 1 2 3 3 3 3 3 3 3 2 1 1 
AREA: 29
AVG DEPTH: 2.0714285714285716
NUMBER OF LINES IN FRAGMENT: 14
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: b47e97781b82109432612a0f36ebb0d0ce270d8b
URL: https://github.com/apache/incubator-iceberg/commit/b47e97781b82109432612a0f36ebb0d0ce270d8b
DESCRIPTION: Extract Method	public create(expectedSchema Schema, readSchema Schema, idToConstant Map<Integer,?>) : DataReader<D> extracted from public create(expectedSchema Schema, readSchema Schema) : DataReader<D> in class org.apache.iceberg.data.avro.DataReader
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/b47e97781b82109432612a0f36ebb0d0ce270d8b/data/src/main/java/org/apache/iceberg/data/avro/DataReader.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/b47e97781b82109432612a0f36ebb0d0ce270d8b/data/src/main/java/org/apache/iceberg/data/avro/DataReader.java#L52
DIRECTLY EXTRACTED OPERATION:
                                         Map<Integer, ?> idToConstant) {
    return new DataReader<>(expectedSchema, readSchema, idToConstant);
  }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 149
FRAGMENT LINE AVG SIZE: 37.25
DEPTHS:
1 2 1 1 
AREA: 5
AVG DEPTH: 1.25
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 898c122118ba99847216c80ca7e0d12821133a8a
URL: https://github.com/apache/incubator-iceberg/commit/898c122118ba99847216c80ca7e0d12821133a8a
DESCRIPTION: Extract Method	private writeData(records Iterable<Record>, schema Schema, location String) : void extracted from private writeAndValidateWithLocations(table Table, location File, expectedDataDir File) : void in class org.apache.iceberg.spark.source.TestDataFrameWrites
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/898c122118ba99847216c80ca7e0d12821133a8a/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWrites.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/898c122118ba99847216c80ca7e0d12821133a8a/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWrites.java#L192
DIRECTLY EXTRACTED OPERATION:
    Dataset<Row> df = createDataset(records, schema);
    DataFrameWriter<?> writer = df.write().format("iceberg").mode("append");
    writer.save(location);
  }

PARAMS COUNT: 3
IS VOID METHOD: true
FRAGMENT LENGTH: 163
FRAGMENT LINE AVG SIZE: 32.6
DEPTHS:
1 2 2 1 1 
AREA: 7
AVG DEPTH: 1.4
NUMBER OF LINES IN FRAGMENT: 5
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private convert(struct Types.StructType, row Row) : Record extracted from private convert(schema Schema, row Row) : Record in class org.apache.iceberg.spark.source.TestSparkReadProjection
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/898c122118ba99847216c80ca7e0d12821133a8a/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadProjection.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/898c122118ba99847216c80ca7e0d12821133a8a/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadProjection.java#L231
DIRECTLY EXTRACTED OPERATION:
    Record record = GenericRecord.create(struct);
    List<Types.NestedField> fields = struct.fields();
    for (int i = 0; i < fields.size(); i += 1) {
      Types.NestedField field = fields.get(i);

      Type fieldType = field.type();

      switch (fieldType.typeId()) {
        case STRUCT:
          record.set(i, convert(fieldType.asStructType(), row.getStruct(i)));
          break;
        case LIST:
          record.set(i, convert(fieldType.asListType(), row.getList(i)));
          break;
        case MAP:
          record.set(i, convert(fieldType.asMapType(), row.getJavaMap(i)));
          break;
        default:
          record.set(i, convert(fieldType, row.get(i)));
      }
    }
    return record;
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 724
FRAGMENT LINE AVG SIZE: 30.166666666666668
DEPTHS:
1 2 2 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 3 2 2 1 1 
AREA: 73
AVG DEPTH: 3.0416666666666665
NUMBER OF LINES IN FRAGMENT: 24
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: d0ea946edd71907358de517b5a879fa720693855
URL: https://github.com/apache/incubator-iceberg/commit/d0ea946edd71907358de517b5a879fa720693855
DESCRIPTION: Extract Method	private newFiles(numFiles int, sizeInBytes long, fileFormat FileFormat) : List<DataFile> extracted from private newFiles(numFiles int, sizeInBytes long) : List<DataFile> in class org.apache.iceberg.TestSplitPlanning
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/d0ea946edd71907358de517b5a879fa720693855/core/src/test/java/org/apache/iceberg/TestSplitPlanning.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/d0ea946edd71907358de517b5a879fa720693855/core/src/test/java/org/apache/iceberg/TestSplitPlanning.java#L174
DIRECTLY EXTRACTED OPERATION:
    List<DataFile> files = Lists.newArrayList();
    for (int fileNum = 0; fileNum < numFiles; fileNum++) {
      files.add(newFile(sizeInBytes, fileFormat));
    }
    return files;
  }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 188
FRAGMENT LINE AVG SIZE: 26.857142857142858
DEPTHS:
1 2 3 2 2 1 1 
AREA: 12
AVG DEPTH: 1.7142857142857142
NUMBER OF LINES IN FRAGMENT: 7
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: bd4f236b834d484908b4ea205aa62bb29692a648
URL: https://github.com/apache/incubator-iceberg/commit/bd4f236b834d484908b4ea205aa62bb29692a648
DESCRIPTION: Extract Method	private checkUUID(currentMetadata TableMetadata, newMetadata TableMetadata) : TableMetadata extracted from public refresh() : TableMetadata in class org.apache.iceberg.hadoop.HadoopTableOperations
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/bd4f236b834d484908b4ea205aa62bb29692a648/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/bd4f236b834d484908b4ea205aa62bb29692a648/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java#L364
DIRECTLY EXTRACTED OPERATION:
    String newUUID = newMetadata.uuid();
    if (currentMetadata != null && currentMetadata.uuid() != null && newUUID != null) {
      Preconditions.checkState(newUUID.equals(currentMetadata.uuid()),
          "Table UUID does not match: current=%s != refreshed=%s", currentMetadata.uuid(), newUUID);
    }
    return newMetadata;
  }
}
PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 337
FRAGMENT LINE AVG SIZE: 42.125
DEPTHS:
1 2 3 3 2 2 1 0 
AREA: 14
AVG DEPTH: 1.75
NUMBER OF LINES IN FRAGMENT: 8
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 87c3cf620203758d572658d269e2bf8ebb3c1e9e
URL: https://github.com/apache/incubator-iceberg/commit/87c3cf620203758d572658d269e2bf8ebb3c1e9e
DESCRIPTION: Extract Method	public fromPath(path Path, fs FileSystem, conf Configuration) : HadoopInputFile extracted from public fromPath(path Path, conf Configuration) : HadoopInputFile in class org.apache.iceberg.hadoop.HadoopInputFile
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/87c3cf620203758d572658d269e2bf8ebb3c1e9e/core/src/main/java/org/apache/iceberg/hadoop/HadoopInputFile.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/87c3cf620203758d572658d269e2bf8ebb3c1e9e/core/src/main/java/org/apache/iceberg/hadoop/HadoopInputFile.java#L87
DIRECTLY EXTRACTED OPERATION:
    return new HadoopInputFile(fs, path, conf);
  }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 53
FRAGMENT LINE AVG SIZE: 17.666666666666668
DEPTHS:
1 1 1 
AREA: 3
AVG DEPTH: 1.0
NUMBER OF LINES IN FRAGMENT: 3
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public fromPath(path Path, length long, fs FileSystem, conf Configuration) : HadoopInputFile extracted from public fromPath(path Path, length long, conf Configuration) : HadoopInputFile in class org.apache.iceberg.hadoop.HadoopInputFile
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/87c3cf620203758d572658d269e2bf8ebb3c1e9e/core/src/main/java/org/apache/iceberg/hadoop/HadoopInputFile.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/87c3cf620203758d572658d269e2bf8ebb3c1e9e/core/src/main/java/org/apache/iceberg/hadoop/HadoopInputFile.java#L91
DIRECTLY EXTRACTED OPERATION:
    return new HadoopInputFile(fs, path, length, conf);
  }

PARAMS COUNT: 4
IS VOID METHOD: false
FRAGMENT LENGTH: 61
FRAGMENT LINE AVG SIZE: 20.333333333333332
DEPTHS:
1 1 1 
AREA: 3
AVG DEPTH: 1.0
NUMBER OF LINES IN FRAGMENT: 3
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public fromStatus(stat FileStatus, fs FileSystem, conf Configuration) : HadoopInputFile extracted from public fromStatus(stat FileStatus, conf Configuration) : HadoopInputFile in class org.apache.iceberg.hadoop.HadoopInputFile
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/87c3cf620203758d572658d269e2bf8ebb3c1e9e/core/src/main/java/org/apache/iceberg/hadoop/HadoopInputFile.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/87c3cf620203758d572658d269e2bf8ebb3c1e9e/core/src/main/java/org/apache/iceberg/hadoop/HadoopInputFile.java#L104
DIRECTLY EXTRACTED OPERATION:
    return new HadoopInputFile(fs, stat, conf);
  }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 53
FRAGMENT LINE AVG SIZE: 17.666666666666668
DEPTHS:
1 1 1 
AREA: 3
AVG DEPTH: 1.0
NUMBER OF LINES IN FRAGMENT: 3
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public fromPath(path Path, fs FileSystem, conf Configuration) : OutputFile extracted from public fromPath(path Path, conf Configuration) : OutputFile in class org.apache.iceberg.hadoop.HadoopOutputFile
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/87c3cf620203758d572658d269e2bf8ebb3c1e9e/core/src/main/java/org/apache/iceberg/hadoop/HadoopOutputFile.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/87c3cf620203758d572658d269e2bf8ebb3c1e9e/core/src/main/java/org/apache/iceberg/hadoop/HadoopOutputFile.java#L61
DIRECTLY EXTRACTED OPERATION:
    return new HadoopOutputFile(fs, path, conf);
  }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 54
FRAGMENT LINE AVG SIZE: 18.0
DEPTHS:
1 1 1 
AREA: 3
AVG DEPTH: 1.0
NUMBER OF LINES IN FRAGMENT: 3
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: b35899f492a8a20fe7431e0c6a467a45fba3d654
URL: https://github.com/apache/incubator-iceberg/commit/b35899f492a8a20fe7431e0c6a467a45fba3d654
DESCRIPTION: Extract Method	public writeCompatibilityErrors(readSchema Schema, writeSchema Schema, checkOrdering boolean) : List<String> extracted from public writeCompatibilityErrors(readSchema Schema, writeSchema Schema) : List<String> in class org.apache.iceberg.types.CheckCompatibility
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/b35899f492a8a20fe7431e0c6a467a45fba3d654/api/src/main/java/org/apache/iceberg/types/CheckCompatibility.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/b35899f492a8a20fe7431e0c6a467a45fba3d654/api/src/main/java/org/apache/iceberg/types/CheckCompatibility.java#L45
DIRECTLY EXTRACTED OPERATION:
   * Returns a list of compatibility errors for writing with the given write schema.
   * This includes nullability: writing optional (nullable) values to a required field is an error
   * Optionally this method allows case where input schema has different ordering than table schema.
   * @param readSchema a read schema
   * @param writeSchema a write schema
   * @param checkOrdering If false, allow input schema to have different ordering than table schema
   * @return a list of error details, or an empty list if there are no compatibility problems
   */
  public static List<String> writeCompatibilityErrors(Schema readSchema, Schema writeSchema, boolean checkOrdering) {
    return TypeUtil.visit(readSchema, new CheckCompatibility(writeSchema, checkOrdering, true));
  }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 781
FRAGMENT LINE AVG SIZE: 65.08333333333333
DEPTHS:
0 1 1 1 1 1 1 1 1 2 1 1 
AREA: 12
AVG DEPTH: 1.0
NUMBER OF LINES IN FRAGMENT: 12
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: c663570cf2b98412d6f63d38e447af866f7e0ed2
URL: https://github.com/apache/incubator-iceberg/commit/c663570cf2b98412d6f63d38e447af866f7e0ed2
DESCRIPTION: Extract Method	private validateSnapshotIdsRefinement(newFromSnapshotId long, newToSnapshotId long) : void extracted from public appendsBetween(newFromSnapshotId long, newToSnapshotId long) : TableScan in class org.apache.iceberg.IncrementalDataTableScan
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/c663570cf2b98412d6f63d38e447af866f7e0ed2/core/src/main/java/org/apache/iceberg/IncrementalDataTableScan.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/c663570cf2b98412d6f63d38e447af866f7e0ed2/core/src/main/java/org/apache/iceberg/IncrementalDataTableScan.java#L137
DIRECTLY EXTRACTED OPERATION:
    Set<Long> snapshotIdsRange = Sets.newHashSet(
        SnapshotUtil.snapshotIdsBetween(table(), fromSnapshotId, toSnapshotId));
    // since snapshotIdsBetween return ids in range (fromSnapshotId, toSnapshotId]
    snapshotIdsRange.add(fromSnapshotId);
    Preconditions.checkArgument(
        snapshotIdsRange.contains(newFromSnapshotId),
        "from snapshot id %s not in existing snapshot ids range (%s, %s]",
        newFromSnapshotId, fromSnapshotId, newToSnapshotId);
    Preconditions.checkArgument(
        snapshotIdsRange.contains(newToSnapshotId),
        "to snapshot id %s not in existing snapshot ids range (%s, %s]",
        newToSnapshotId, fromSnapshotId, toSnapshotId);
  }

PARAMS COUNT: 2
IS VOID METHOD: true
FRAGMENT LENGTH: 698
FRAGMENT LINE AVG SIZE: 49.857142857142854
DEPTHS:
1 2 2 2 2 2 2 2 2 2 2 2 1 1 
AREA: 25
AVG DEPTH: 1.7857142857142858
NUMBER OF LINES IN FRAGMENT: 14
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: a0ff334f2a9b0e1a429ccf69a85a12e8c255ed33
URL: https://github.com/apache/incubator-iceberg/commit/a0ff334f2a9b0e1a429ccf69a85a12e8c255ed33
DESCRIPTION: Extract Method	protected refresh() : TableMetadata extracted from public apply() : Snapshot in class org.apache.iceberg.SnapshotProducer
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/a0ff334f2a9b0e1a429ccf69a85a12e8c255ed33/core/src/main/java/org/apache/iceberg/SnapshotProducer.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/a0ff334f2a9b0e1a429ccf69a85a12e8c255ed33/core/src/main/java/org/apache/iceberg/SnapshotProducer.java#L228
DIRECTLY EXTRACTED OPERATION:
    this.base = ops.refresh();
    return base;
  }

IS VOID METHOD: false
FRAGMENT LENGTH: 53
FRAGMENT LINE AVG SIZE: 13.25
DEPTHS:
1 2 1 1 
AREA: 5
AVG DEPTH: 1.25
NUMBER OF LINES IN FRAGMENT: 4
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: c3dc9824b381e5e479e356be5e0f4fcf61a9fc37
URL: https://github.com/apache/incubator-iceberg/commit/c3dc9824b381e5e479e356be5e0f4fcf61a9fc37
DESCRIPTION: Extract Method	private copyManifest(manifest ManifestFile) : ManifestFile extracted from public appendManifest(manifest ManifestFile) : FastAppend in class org.apache.iceberg.FastAppend
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/c3dc9824b381e5e479e356be5e0f4fcf61a9fc37/core/src/main/java/org/apache/iceberg/FastAppend.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/c3dc9824b381e5e479e356be5e0f4fcf61a9fc37/core/src/main/java/org/apache/iceberg/FastAppend.java#L111
DIRECTLY EXTRACTED OPERATION:
    try (ManifestReader reader = ManifestReader.read(manifest, ops.io(), ops.current().specsById())) {
      OutputFile newManifestPath = manifestPath(manifestCount.getAndIncrement());
      return ManifestWriter.copyAppendManifest(reader, newManifestPath, snapshotId(), summaryBuilder);
    } catch (IOException e) {
      throw new RuntimeIOException(e, "Failed to close manifest: %s", manifest);
    }
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 410
FRAGMENT LINE AVG SIZE: 51.25
DEPTHS:
2 3 3 3 3 2 1 1 
AREA: 18
AVG DEPTH: 2.25
NUMBER OF LINES IN FRAGMENT: 8
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private copyManifest(manifest ManifestFile) : ManifestFile extracted from protected add(manifest ManifestFile) : void in class org.apache.iceberg.MergingSnapshotProducer
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/c3dc9824b381e5e479e356be5e0f4fcf61a9fc37/core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/c3dc9824b381e5e479e356be5e0f4fcf61a9fc37/core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java#L231
DIRECTLY EXTRACTED OPERATION:
    try (ManifestReader reader = ManifestReader.read(manifest, ops.io(), ops.current().specsById())) {
      OutputFile newManifestPath = manifestPath(manifestCount.getAndIncrement());
      return ManifestWriter.copyAppendManifest(reader, newManifestPath, snapshotId(), appendedManifestsSummary);
    } catch (IOException e) {
      throw new RuntimeIOException(e, "Failed to close manifest: %s", manifest);
    }
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 420
FRAGMENT LINE AVG SIZE: 52.5
DEPTHS:
2 3 3 3 3 2 1 1 
AREA: 18
AVG DEPTH: 2.25
NUMBER OF LINES IN FRAGMENT: 8
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 2562649b097a89b2495d3a9118fd3b64ce82367a
URL: https://github.com/apache/incubator-iceberg/commit/2562649b097a89b2495d3a9118fd3b64ce82367a
DESCRIPTION: Extract Method	private entries(entryFn BiFunction<ManifestFile,CloseableIterable<ManifestEntry>,CloseableIterable<T>>) : Iterable<CloseableIterable<T>> extracted from public entries() : CloseableIterable<ManifestEntry> in class org.apache.iceberg.ManifestGroup
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/2562649b097a89b2495d3a9118fd3b64ce82367a/core/src/main/java/org/apache/iceberg/ManifestGroup.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/2562649b097a89b2495d3a9118fd3b64ce82367a/core/src/main/java/org/apache/iceberg/ManifestGroup.java#L165
DIRECTLY EXTRACTED OPERATION:
      BiFunction<ManifestFile, CloseableIterable<ManifestEntry>, CloseableIterable<T>> entryFn) {
    LoadingCache<Integer, ManifestEvaluator> evalCache = specsById == null ?
        null : Caffeine.newBuilder().build(specId -> {
          PartitionSpec spec = specsById.get(specId);
          return ManifestEvaluator.forPartitionFilter(
              Expressions.and(partitionFilter, Projections.inclusive(spec, caseSensitive).project(dataFilter)),
              spec, caseSensitive);
        });

    Evaluator evaluator = new Evaluator(DataFile.getType(EMPTY_STRUCT), fileFilter, caseSensitive);

    Iterable<ManifestFile> matchingManifests = evalCache == null ? manifests : Iterables.filter(manifests,
        manifest -> evalCache.get(manifest.partitionSpecId()).eval(manifest));

    if (ignoreDeleted) {
      // only scan manifests that have entries other than deletes
      // remove any manifests that don't have any existing or added files. if either the added or
      // existing files count is missing, the manifest must be scanned.
      matchingManifests = Iterables.filter(matchingManifests,
          manifest -> manifest.hasAddedFiles() || manifest.hasExistingFiles());
    }

    if (ignoreExisting) {
      // only scan manifests that have entries other than existing
      // remove any manifests that don't have any deleted or added files. if either the added or
      // deleted files count is missing, the manifest must be scanned.
      matchingManifests = Iterables.filter(matchingManifests,
          manifest -> manifest.hasAddedFiles() || manifest.hasDeletedFiles());
    }

    matchingManifests = Iterables.filter(matchingManifests, manifestPredicate::test);

    Iterable<CloseableIterable<T>> readers = Iterables.transform(
        matchingManifests,
        manifest -> {
          ManifestReader reader = ManifestReader.read(
              io.newInputFile(manifest.path()),
              specsById);

          FilteredManifest filtered = reader
              .filterRows(dataFilter)
              .filterPartitions(partitionFilter)
              .caseSensitive(caseSensitive)
              .select(columns);

          CloseableIterable<ManifestEntry> entries = filtered.allEntries();
          if (ignoreDeleted) {
            entries = filtered.liveEntries();
          }

          if (ignoreExisting) {
            entries = CloseableIterable.filter(entries,
                entry -> entry.status() != ManifestEntry.Status.EXISTING);
          }

          if (fileFilter != null && fileFilter != Expressions.alwaysTrue()) {
            entries = CloseableIterable.filter(entries,
                entry -> evaluator.eval((GenericDataFile) entry.file()));
          }

          entries = CloseableIterable.filter(entries, manifestEntryPredicate);
          return entryFn.apply(manifest, entries);
        });

    return readers;
  }
}
PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 2880
FRAGMENT LINE AVG SIZE: 42.985074626865675
DEPTHS:
1 2 2 3 3 3 3 2 2 2 2 2 2 2 2 3 3 3 3 3 2 2 2 3 3 3 3 3 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 4 3 3 3 4 4 3 3 3 4 4 3 3 3 3 2 2 2 1 0 
AREA: 176
AVG DEPTH: 2.626865671641791
NUMBER OF LINES IN FRAGMENT: 67
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: c73b6574eda1466c4ad4c6addb3f44b23928c681
URL: https://github.com/apache/incubator-iceberg/commit/c73b6574eda1466c4ad4c6addb3f44b23928c681
DESCRIPTION: Extract Method	private dropStats() : boolean extracted from public iterator() : Iterator<DataFile> in class org.apache.iceberg.FilteredManifest
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/c73b6574eda1466c4ad4c6addb3f44b23928c681/core/src/main/java/org/apache/iceberg/FilteredManifest.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/c73b6574eda1466c4ad4c6addb3f44b23928c681/core/src/main/java/org/apache/iceberg/FilteredManifest.java#L177
DIRECTLY EXTRACTED OPERATION:
    // Make sure we only drop all stats if we had projected all stats
    // We do not drop stats even if we had partially added some stats columns
    return rowFilter != Expressions.alwaysTrue() &&
        !columns.containsAll(ManifestReader.ALL_COLUMNS) &&
        Sets.intersection(Sets.newHashSet(columns), STATS_COLUMNS).isEmpty();
  }

IS VOID METHOD: false
FRAGMENT LENGTH: 343
FRAGMENT LINE AVG SIZE: 49.0
DEPTHS:
1 2 2 2 2 1 1 
AREA: 11
AVG DEPTH: 1.5714285714285714
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 686fcb752dd5a7204094266cd3c4363431dddb43
URL: https://github.com/apache/incubator-iceberg/commit/686fcb752dd5a7204094266cd3c4363431dddb43
DESCRIPTION: Extract Method	public eval(struct StructLike) : Boolean extracted from public test(struct StructLike) : boolean in class org.apache.iceberg.expressions.BoundPredicate
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/686fcb752dd5a7204094266cd3c4363431dddb43/api/src/main/java/org/apache/iceberg/expressions/BoundPredicate.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/686fcb752dd5a7204094266cd3c4363431dddb43/api/src/main/java/org/apache/iceberg/expressions/BoundPredicate.java#L35
DIRECTLY EXTRACTED OPERATION:
  public Boolean eval(StructLike struct) {
    return test(term().eval(struct));
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 86
FRAGMENT LINE AVG SIZE: 21.5
DEPTHS:
1 2 1 1 
AREA: 5
AVG DEPTH: 1.25
NUMBER OF LINES IN FRAGMENT: 4
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private predicate(op Operation, expr UnboundTerm<T>, values Iterable<T>) : UnboundPredicate<T> extracted from public predicate(op Operation, name String, values Iterable<T>) : UnboundPredicate<T> in class org.apache.iceberg.expressions.Expressions
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/686fcb752dd5a7204094266cd3c4363431dddb43/api/src/main/java/org/apache/iceberg/expressions/Expressions.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/686fcb752dd5a7204094266cd3c4363431dddb43/api/src/main/java/org/apache/iceberg/expressions/Expressions.java#L238
DIRECTLY EXTRACTED OPERATION:
    return new UnboundPredicate<>(op, expr, values);
  }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 58
FRAGMENT LINE AVG SIZE: 19.333333333333332
DEPTHS:
1 1 1 
AREA: 3
AVG DEPTH: 1.0
NUMBER OF LINES IN FRAGMENT: 3
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 17effe101056ce6b9b154e60b31589cc0d759b72
URL: https://github.com/apache/incubator-iceberg/commit/17effe101056ce6b9b154e60b31589cc0d759b72
DESCRIPTION: Extract Method	public wrap(catalog Catalog, caseSensitive boolean) : Catalog extracted from public wrap(catalog Catalog) : Catalog in class org.apache.iceberg.CachingCatalog
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/17effe101056ce6b9b154e60b31589cc0d759b72/core/src/main/java/org/apache/iceberg/CachingCatalog.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/17effe101056ce6b9b154e60b31589cc0d759b72/core/src/main/java/org/apache/iceberg/CachingCatalog.java#L39
DIRECTLY EXTRACTED OPERATION:
    return new CachingCatalog(catalog, caseSensitive);
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 60
FRAGMENT LINE AVG SIZE: 20.0
DEPTHS:
1 1 1 
AREA: 3
AVG DEPTH: 1.0
NUMBER OF LINES IN FRAGMENT: 3
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private canonicalizeIdentifier(tableIdentifier TableIdentifier) : TableIdentifier extracted from public dropTable(ident TableIdentifier, purge boolean) : boolean in class org.apache.iceberg.CachingCatalog
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/17effe101056ce6b9b154e60b31589cc0d759b72/core/src/main/java/org/apache/iceberg/CachingCatalog.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/17effe101056ce6b9b154e60b31589cc0d759b72/core/src/main/java/org/apache/iceberg/CachingCatalog.java#L55
DIRECTLY EXTRACTED OPERATION:
    if (caseSensitive) {
      return tableIdentifier;
    } else {
      return tableIdentifier.toLowerCase();
    }
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 123
FRAGMENT LINE AVG SIZE: 17.571428571428573
DEPTHS:
2 3 3 3 2 1 1 
AREA: 15
AVG DEPTH: 2.142857142857143
NUMBER OF LINES IN FRAGMENT: 7
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private canonicalizeIdentifier(tableIdentifier TableIdentifier) : TableIdentifier extracted from public renameTable(from TableIdentifier, to TableIdentifier) : void in class org.apache.iceberg.CachingCatalog
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/17effe101056ce6b9b154e60b31589cc0d759b72/core/src/main/java/org/apache/iceberg/CachingCatalog.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/17effe101056ce6b9b154e60b31589cc0d759b72/core/src/main/java/org/apache/iceberg/CachingCatalog.java#L55
DIRECTLY EXTRACTED OPERATION:
    if (caseSensitive) {
      return tableIdentifier;
    } else {
      return tableIdentifier.toLowerCase();
    }
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 123
FRAGMENT LINE AVG SIZE: 17.571428571428573
DEPTHS:
2 3 3 3 2 1 1 
AREA: 15
AVG DEPTH: 2.142857142857143
NUMBER OF LINES IN FRAGMENT: 7
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 94fdc7321407c22b143b9eab83d29179d12d1a6d
URL: https://github.com/apache/incubator-iceberg/commit/94fdc7321407c22b143b9eab83d29179d12d1a6d
DESCRIPTION: Extract Method	private bindUnaryOperation(ref BoundReference<T>, isRequired boolean) : Expression extracted from public bind(struct StructType, caseSensitive boolean) : Expression in class org.apache.iceberg.expressions.UnboundPredicate
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/94fdc7321407c22b143b9eab83d29179d12d1a6d/api/src/main/java/org/apache/iceberg/expressions/UnboundPredicate.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/94fdc7321407c22b143b9eab83d29179d12d1a6d/api/src/main/java/org/apache/iceberg/expressions/UnboundPredicate.java#L123
DIRECTLY EXTRACTED OPERATION:
    switch (op()) {
      case IS_NULL:
        if (isRequired) {
          return Expressions.alwaysFalse();
        }
        return new BoundUnaryPredicate<>(Operation.IS_NULL, ref);
      case NOT_NULL:
        if (isRequired) {
          return Expressions.alwaysTrue();
        }
        return new BoundUnaryPredicate<>(Operation.NOT_NULL, ref);
      default:
        throw new ValidationException("Operation must be IS_NULL or NOT_NULL");
    }
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 459
FRAGMENT LINE AVG SIZE: 28.6875
DEPTHS:
2 3 3 4 3 3 3 3 4 3 3 3 3 2 1 1 
AREA: 44
AVG DEPTH: 2.75
NUMBER OF LINES IN FRAGMENT: 16
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private bindLiteralOperation(ref BoundReference<T>, type Type) : Expression extracted from public bind(struct StructType, caseSensitive boolean) : Expression in class org.apache.iceberg.expressions.UnboundPredicate
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/94fdc7321407c22b143b9eab83d29179d12d1a6d/api/src/main/java/org/apache/iceberg/expressions/UnboundPredicate.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/94fdc7321407c22b143b9eab83d29179d12d1a6d/api/src/main/java/org/apache/iceberg/expressions/UnboundPredicate.java#L140
DIRECTLY EXTRACTED OPERATION:
    Literal<T> lit = literal().to(type);

    if (lit == null) {
      throw new ValidationException(String.format(
          "Invalid value for conversion to type %s: %s (%s)",
          type, literal().value(), literal().value().getClass().getName()));

    } else if (lit == Literals.aboveMax()) {
      switch (op()) {
        case LT:
        case LT_EQ:
        case NOT_EQ:
          return Expressions.alwaysTrue();
        case GT:
        case GT_EQ:
        case EQ:
          return Expressions.alwaysFalse();
      }
    } else if (lit == Literals.belowMin()) {
      switch (op()) {
        case GT:
        case GT_EQ:
        case NOT_EQ:
          return Expressions.alwaysTrue();
        case LT:
        case LT_EQ:
        case EQ:
          return Expressions.alwaysFalse();
      }
    }

    return new BoundLiteralPredicate<>(op(), ref, lit);
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 872
FRAGMENT LINE AVG SIZE: 25.647058823529413
DEPTHS:
1 2 2 3 3 3 3 3 3 4 4 4 4 4 4 4 4 3 3 3 4 4 4 4 4 4 4 4 3 2 2 2 1 1 
AREA: 107
AVG DEPTH: 3.1470588235294117
NUMBER OF LINES IN FRAGMENT: 34
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 96fb5ef6edb4111793e475ea6163a646959c7453
URL: https://github.com/apache/incubator-iceberg/commit/96fb5ef6edb4111793e475ea6163a646959c7453
DESCRIPTION: Extract Method	public createReader(readSchema StructType, options DataSourceOptions) : DataSourceReader extracted from public createReader(options DataSourceOptions) : DataSourceReader in class org.apache.iceberg.spark.source.IcebergSource
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/96fb5ef6edb4111793e475ea6163a646959c7453/spark/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/96fb5ef6edb4111793e475ea6163a646959c7453/spark/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java#L70
DIRECTLY EXTRACTED OPERATION:
  public DataSourceReader createReader(StructType readSchema, DataSourceOptions options) {
    Configuration conf = new Configuration(lazyBaseConf());
    Table table = getTableAndResolveHadoopConfiguration(options, conf);
    String caseSensitive = lazySparkSession().conf().get("spark.sql.caseSensitive", "true");

    Reader reader = new Reader(table, Boolean.valueOf(caseSensitive), options);
    if (readSchema != null) {
      // convert() will fail if readSchema contains fields not in table.schema()
      SparkSchemaUtil.convert(table.schema(), readSchema);
      reader.pruneColumns(readSchema);
    }

    return reader;
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 637
FRAGMENT LINE AVG SIZE: 42.46666666666667
DEPTHS:
1 2 2 2 2 2 2 3 3 3 2 2 2 1 1 
AREA: 30
AVG DEPTH: 2.0
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 70c1940209b9c444363322bf547b65d9320e0cea
URL: https://github.com/apache/incubator-iceberg/commit/70c1940209b9c444363322bf547b65d9320e0cea
DESCRIPTION: Extract Method	private cleanUncommitted(manifests Iterable<ManifestFile>, committedManifests Set<ManifestFile>) : void extracted from protected cleanUncommitted(committed Set<ManifestFile>) : void in class org.apache.iceberg.BaseRewriteManifests
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/70c1940209b9c444363322bf547b65d9320e0cea/core/src/main/java/org/apache/iceberg/BaseRewriteManifests.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/70c1940209b9c444363322bf547b65d9320e0cea/core/src/main/java/org/apache/iceberg/BaseRewriteManifests.java#L302
DIRECTLY EXTRACTED OPERATION:
    for (ManifestFile manifest : manifests) {
      if (!committedManifests.contains(manifest)) {
        deleteFile(manifest.path());
      }
    }
  }

PARAMS COUNT: 2
IS VOID METHOD: true
FRAGMENT LENGTH: 154
FRAGMENT LINE AVG SIZE: 22.0
DEPTHS:
2 3 4 3 2 1 1 
AREA: 16
AVG DEPTH: 2.2857142857142856
NUMBER OF LINES IN FRAGMENT: 7
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	package copyManifest(reader ManifestReader, outputFile OutputFile, snapshotId long, summaryBuilder SnapshotSummary.Builder, allowedEntryStatuses Set<ManifestEntry.Status>) : ManifestFile extracted from package copyAppendManifest(reader ManifestReader, outputFile OutputFile, snapshotId long, summaryBuilder SnapshotSummary.Builder) : ManifestFile in class org.apache.iceberg.ManifestWriter
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/70c1940209b9c444363322bf547b65d9320e0cea/core/src/main/java/org/apache/iceberg/ManifestWriter.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/70c1940209b9c444363322bf547b65d9320e0cea/core/src/main/java/org/apache/iceberg/ManifestWriter.java#L44
DIRECTLY EXTRACTED OPERATION:
                                   SnapshotSummary.Builder summaryBuilder,
                                   Set<ManifestEntry.Status> allowedEntryStatuses) {
    ManifestWriter writer = new ManifestWriter(reader.spec(), outputFile, snapshotId);
    boolean threw = true;
    try {
      for (ManifestEntry entry : reader.entries()) {
        Preconditions.checkArgument(
            allowedEntryStatuses.contains(entry.status()),
            "Invalid manifest entry status: %s (allowed statuses: %s)",
            entry.status(), allowedEntryStatuses);
        switch (entry.status()) {
          case ADDED:
            summaryBuilder.addedFile(reader.spec(), entry.file());
            writer.add(entry);
            break;
          case EXISTING:
            writer.existing(entry);
            break;
          case DELETED:
            summaryBuilder.deletedFile(reader.spec(), entry.file());
            writer.delete(entry);
            break;
        }
      }

      threw = false;

    } finally {
      try {
        writer.close();
      } catch (IOException e) {
        if (!threw) {
          throw new RuntimeIOException(e, "Failed to close manifest: %s", outputFile);
        }
      }
    }

    return writer.toManifestFile();
  }

PARAMS COUNT: 5
IS VOID METHOD: false
FRAGMENT LENGTH: 1254
FRAGMENT LINE AVG SIZE: 31.35
DEPTHS:
0 1 2 2 2 3 4 4 4 4 4 5 5 5 5 5 5 5 5 5 5 5 4 3 3 3 3 3 3 4 4 4 5 4 3 2 2 2 1 1 
AREA: 139
AVG DEPTH: 3.475
NUMBER OF LINES IN FRAGMENT: 40
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 42c1e63333c340210752e879820eedd307f227d4
URL: https://github.com/apache/incubator-iceberg/commit/42c1e63333c340210752e879820eedd307f227d4
DESCRIPTION: Extract Method	private internalAddColumn(parent String, name String, isOptional boolean, type Type, doc String) : void extracted from public addColumn(parent String, name String, type Type, doc String) : UpdateSchema in class org.apache.iceberg.SchemaUpdate
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/42c1e63333c340210752e879820eedd307f227d4/core/src/main/java/org/apache/iceberg/SchemaUpdate.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/42c1e63333c340210752e879820eedd307f227d4/core/src/main/java/org/apache/iceberg/SchemaUpdate.java#L109
DIRECTLY EXTRACTED OPERATION:
    int parentId = TABLE_ROOT_ID;
    if (parent != null) {
      Types.NestedField parentField = schema.findField(parent);
      Preconditions.checkArgument(parentField != null, "Cannot find parent struct: %s", parent);
      Type parentType = parentField.type();
      if (parentType.isNestedType()) {
        Type.NestedType nested = parentType.asNestedType();
        if (nested.isMapType()) {
          // fields are added to the map value type
          parentField = nested.asMapType().fields().get(1);
        } else if (nested.isListType()) {
          // fields are added to the element type
          parentField = nested.asListType().fields().get(0);
        }
      }
      Preconditions.checkArgument(
          parentField.type().isNestedType() && parentField.type().asNestedType().isStructType(),
          "Cannot add to non-struct column: %s: %s", parent, parentField.type());
      parentId = parentField.fieldId();
      Preconditions.checkArgument(!deletes.contains(parentId),
          "Cannot add to a column that will be deleted: %s", parent);
      Preconditions.checkArgument(schema.findField(parent + "." + name) == null,
          "Cannot add column, name already exists: %s.%s", parent, name);
    } else {
      Preconditions.checkArgument(schema.findField(name) == null,
          "Cannot add column, name already exists: %s", name);
    }

    // assign new IDs in order
    int newId = assignNewColumnId();
    adds.put(parentId, Types.NestedField.of(newId, isOptional, name,
        TypeUtil.assignFreshIds(type, this::assignNewColumnId), doc));
  }

PARAMS COUNT: 5
IS VOID METHOD: true
FRAGMENT LENGTH: 1585
FRAGMENT LINE AVG SIZE: 46.61764705882353
DEPTHS:
1 2 3 3 3 3 4 4 5 5 5 5 5 4 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 1 1 
AREA: 102
AVG DEPTH: 3.0
NUMBER OF LINES IN FRAGMENT: 34
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 860cdcf428b901ea546957187d517089924e0917
URL: https://github.com/apache/incubator-iceberg/commit/860cdcf428b901ea546957187d517089924e0917
DESCRIPTION: Extract Method	private predicate(op Operation, name String, lits Collection<Literal<T>>) : UnboundPredicate<T> extracted from public predicate(op Operation, name String, value T) : UnboundPredicate<T> in class org.apache.iceberg.expressions.Expressions
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/860cdcf428b901ea546957187d517089924e0917/api/src/main/java/org/apache/iceberg/expressions/Expressions.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/860cdcf428b901ea546957187d517089924e0917/api/src/main/java/org/apache/iceberg/expressions/Expressions.java#L147
DIRECTLY EXTRACTED OPERATION:
    return new UnboundPredicate<>(op, ref(name), lits);
  }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 61
FRAGMENT LINE AVG SIZE: 20.333333333333332
DEPTHS:
1 1 1 
AREA: 3
AVG DEPTH: 1.0
NUMBER OF LINES IN FRAGMENT: 3
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private predicate(op Operation, name String, lits Collection<Literal<T>>) : UnboundPredicate<T> extracted from public predicate(op Operation, name String, lit Literal<T>) : UnboundPredicate<T> in class org.apache.iceberg.expressions.Expressions
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/860cdcf428b901ea546957187d517089924e0917/api/src/main/java/org/apache/iceberg/expressions/Expressions.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/860cdcf428b901ea546957187d517089924e0917/api/src/main/java/org/apache/iceberg/expressions/Expressions.java#L147
DIRECTLY EXTRACTED OPERATION:
    return new UnboundPredicate<>(op, ref(name), lits);
  }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 61
FRAGMENT LINE AVG SIZE: 20.333333333333332
DEPTHS:
1 1 1 
AREA: 3
AVG DEPTH: 1.0
NUMBER OF LINES IN FRAGMENT: 3
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 9f1598e7f9c3409e2ce38365f928ad6d5d58e485
URL: https://github.com/apache/incubator-iceberg/commit/9f1598e7f9c3409e2ce38365f928ad6d5d58e485
DESCRIPTION: Extract Method	private getId(schema Schema, propertyName String, nameMapping NameMapping, names List<String>) : Integer extracted from private getId(schema Schema, propertyName String) : int in class org.apache.iceberg.avro.AvroSchemaUtil
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/9f1598e7f9c3409e2ce38365f928ad6d5d58e485/core/src/main/java/org/apache/iceberg/avro/AvroSchemaUtil.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/9f1598e7f9c3409e2ce38365f928ad6d5d58e485/core/src/main/java/org/apache/iceberg/avro/AvroSchemaUtil.java#L208
DIRECTLY EXTRACTED OPERATION:
    if (schema.getType() == UNION) {
      return getId(fromOption(schema), propertyName, nameMapping, names);
    }

    Object id = schema.getObjectProp(propertyName);
    if (id != null) {
      return toInt(id);
    } else if (nameMapping != null) {
      MappedField mappedField = nameMapping.find(names);
      if (mappedField != null) {
        return mappedField.id();
      }
    }

    return null;
  }

PARAMS COUNT: 4
IS VOID METHOD: false
FRAGMENT LENGTH: 414
FRAGMENT LINE AVG SIZE: 24.352941176470587
DEPTHS:
2 3 2 2 2 2 3 3 3 3 4 3 2 2 2 1 1 
AREA: 40
AVG DEPTH: 2.3529411764705883
NUMBER OF LINES IN FRAGMENT: 17
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	package getFieldId(field Schema.Field, nameMapping NameMapping, parentFieldNames Iterable<String>) : Integer extracted from public getFieldId(field Schema.Field) : int in class org.apache.iceberg.avro.AvroSchemaUtil
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/9f1598e7f9c3409e2ce38365f928ad6d5d58e485/core/src/main/java/org/apache/iceberg/avro/AvroSchemaUtil.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/9f1598e7f9c3409e2ce38365f928ad6d5d58e485/core/src/main/java/org/apache/iceberg/avro/AvroSchemaUtil.java#L281
DIRECTLY EXTRACTED OPERATION:
    Object id = field.getObjectProp(FIELD_ID_PROP);
    if (id != null) {
      return toInt(id);
    } else if (nameMapping != null) {
      List<String> names = Lists.newArrayList(parentFieldNames);
      names.add(field.name());
      MappedField mappedField = nameMapping.find(names);
      if (mappedField != null) {
        return mappedField.id();
      }
    }

    return null;
  }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 392
FRAGMENT LINE AVG SIZE: 26.133333333333333
DEPTHS:
1 2 3 3 3 3 3 3 4 3 2 2 2 1 1 
AREA: 36
AVG DEPTH: 2.4
NUMBER OF LINES IN FRAGMENT: 15
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private arrayWithId(array Schema, elementId Integer) : Schema extracted from public array(array Schema, element Schema) : Schema in class org.apache.iceberg.avro.PruneColumns
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/9f1598e7f9c3409e2ce38365f928ad6d5d58e485/core/src/main/java/org/apache/iceberg/avro/PruneColumns.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/9f1598e7f9c3409e2ce38365f928ad6d5d58e485/core/src/main/java/org/apache/iceberg/avro/PruneColumns.java#L201
DIRECTLY EXTRACTED OPERATION:
    if (!AvroSchemaUtil.hasProperty(array, AvroSchemaUtil.ELEMENT_ID_PROP)) {
      Schema result = Schema.createArray(array.getElementType());
      result.addProp(AvroSchemaUtil.ELEMENT_ID_PROP, elementId);
      return result;
    }
    return array;
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 259
FRAGMENT LINE AVG SIZE: 32.375
DEPTHS:
2 3 3 3 2 2 1 1 
AREA: 17
AVG DEPTH: 2.125
NUMBER OF LINES IN FRAGMENT: 8
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private complexMapWithIds(map Schema, keyId Integer, valueId Integer) : Schema extracted from public array(array Schema, element Schema) : Schema in class org.apache.iceberg.avro.PruneColumns
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/9f1598e7f9c3409e2ce38365f928ad6d5d58e485/core/src/main/java/org/apache/iceberg/avro/PruneColumns.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/9f1598e7f9c3409e2ce38365f928ad6d5d58e485/core/src/main/java/org/apache/iceberg/avro/PruneColumns.java#L210
DIRECTLY EXTRACTED OPERATION:
    Schema keyValue = map.getElementType();
    if (!AvroSchemaUtil.hasFieldId(keyValue.getField("key")) ||
        !AvroSchemaUtil.hasFieldId(keyValue.getField("value"))) {
      return AvroSchemaUtil.createMap(
          keyId, keyValue.getField("key").schema(),
          valueId, keyValue.getField("value").schema());
    }
    return map;
  }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 349
FRAGMENT LINE AVG SIZE: 34.9
DEPTHS:
1 2 2 3 3 3 2 2 1 1 
AREA: 20
AVG DEPTH: 2.0
NUMBER OF LINES IN FRAGMENT: 10
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 84dd40dc873544c4842bed20bafd921181465f49
URL: https://github.com/apache/incubator-iceberg/commit/84dd40dc873544c4842bed20bafd921181465f49
DESCRIPTION: Extract Method	private checkDataFile(expected DataFile, actual DataFile) : void extracted from public testDataFileKryoSerialization() : void in class org.apache.iceberg.TestDataFileSerialization
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/84dd40dc873544c4842bed20bafd921181465f49/spark/src/test/java/org/apache/iceberg/TestDataFileSerialization.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/84dd40dc873544c4842bed20bafd921181465f49/spark/src/test/java/org/apache/iceberg/TestDataFileSerialization.java#L131
DIRECTLY EXTRACTED OPERATION:
    Assert.assertEquals("Should match the serialized record path",
        expected.path(), actual.path());
    Assert.assertEquals("Should match the serialized record format",
        expected.format(), actual.format());
    Assert.assertEquals("Should match the serialized record partition",
        expected.partition().get(0, Object.class), actual.partition().get(0, Object.class));
    Assert.assertEquals("Should match the serialized record count",
        expected.recordCount(), actual.recordCount());
    Assert.assertEquals("Should match the serialized record size",
        expected.fileSizeInBytes(), actual.fileSizeInBytes());
    Assert.assertEquals("Should match the serialized record value counts",
        expected.valueCounts(), actual.valueCounts());
    Assert.assertEquals("Should match the serialized record null value counts",
        expected.nullValueCounts(), actual.nullValueCounts());
    Assert.assertEquals("Should match the serialized record lower bounds",
        expected.lowerBounds(), actual.lowerBounds());
    Assert.assertEquals("Should match the serialized record upper bounds",
        expected.upperBounds(), actual.upperBounds());
    Assert.assertEquals("Should match the serialized record key metadata",
        expected.keyMetadata(), actual.keyMetadata());
    Assert.assertEquals("Should match the serialized record offsets",
        expected.splitOffsets(), actual.splitOffsets());
    Assert.assertEquals("Should match the serialized record offsets",
        expected.keyMetadata(), actual.keyMetadata());
  }

PARAMS COUNT: 2
IS VOID METHOD: true
FRAGMENT LENGTH: 1560
FRAGMENT LINE AVG SIZE: 60.0
DEPTHS:
1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 
AREA: 49
AVG DEPTH: 1.8846153846153846
NUMBER OF LINES IN FRAGMENT: 26
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 4bb65c18e9dd89698891be99cd9ac0efab85cf73
URL: https://github.com/apache/incubator-iceberg/commit/4bb65c18e9dd89698891be99cd9ac0efab85cf73
DESCRIPTION: Extract Method	private checkAndAddPartitionName(name String, identitySourceColumnId Integer) : void extracted from private checkAndAddPartitionName(name String) : void in class org.apache.iceberg.PartitionSpec.Builder
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/4bb65c18e9dd89698891be99cd9ac0efab85cf73/api/src/main/java/org/apache/iceberg/PartitionSpec.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/4bb65c18e9dd89698891be99cd9ac0efab85cf73/api/src/main/java/org/apache/iceberg/PartitionSpec.java#L338
DIRECTLY EXTRACTED OPERATION:
      Types.NestedField schemaField = schema.findField(name);
      if (identitySourceColumnId != null) {
        // for identity transform case we allow  conflicts between partition and schema field name as
        //   long as they are sourced from the same schema field
        Preconditions.checkArgument(schemaField == null || schemaField.fieldId() == identitySourceColumnId,
            "Cannot create identity partition sourced from different field in schema: %s", name);
      } else {
        // for all other transforms we don't allow conflicts between partition name and schema field name
        Preconditions.checkArgument(schemaField == null,
            "Cannot create partition from name that exists in schema: %s", name);
      }
      Preconditions.checkArgument(name != null && !name.isEmpty(),
          "Cannot use empty or null partition name: %s", name);
      Preconditions.checkArgument(!partitionNames.contains(name),
          "Cannot use partition name more than once: %s", name);
      partitionNames.add(name);
    }

PARAMS COUNT: 2
IS VOID METHOD: true
FRAGMENT LENGTH: 1048
FRAGMENT LINE AVG SIZE: 58.22222222222222
DEPTHS:
2 3 4 4 4 4 4 4 4 4 3 3 3 3 3 3 2 2 
AREA: 59
AVG DEPTH: 3.2777777777777777
NUMBER OF LINES IN FRAGMENT: 18
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	package identity(sourceName String, targetName String) : Builder extracted from public identity(sourceName String) : Builder in class org.apache.iceberg.PartitionSpec.Builder
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/4bb65c18e9dd89698891be99cd9ac0efab85cf73/api/src/main/java/org/apache/iceberg/PartitionSpec.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/4bb65c18e9dd89698891be99cd9ac0efab85cf73/api/src/main/java/org/apache/iceberg/PartitionSpec.java#L375
DIRECTLY EXTRACTED OPERATION:
      Types.NestedField sourceColumn = findSourceColumn(sourceName);
      checkAndAddPartitionName(targetName, sourceColumn.fieldId());
      fields.add(new PartitionField(
          sourceColumn.fieldId(), targetName, Transforms.identity(sourceColumn.type())));
      return this;
    }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 290
FRAGMENT LINE AVG SIZE: 41.42857142857143
DEPTHS:
2 3 3 3 3 2 2 
AREA: 18
AVG DEPTH: 2.5714285714285716
NUMBER OF LINES IN FRAGMENT: 7
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public year(sourceName String, targetName String) : Builder extracted from public year(sourceName String) : Builder in class org.apache.iceberg.PartitionSpec.Builder
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/4bb65c18e9dd89698891be99cd9ac0efab85cf73/api/src/main/java/org/apache/iceberg/PartitionSpec.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/4bb65c18e9dd89698891be99cd9ac0efab85cf73/api/src/main/java/org/apache/iceberg/PartitionSpec.java#L387
DIRECTLY EXTRACTED OPERATION:
      checkAndAddPartitionName(targetName);
      Types.NestedField sourceColumn = findSourceColumn(sourceName);
      PartitionField field = new PartitionField(
          sourceColumn.fieldId(), targetName, Transforms.year(sourceColumn.type()));
      checkForRedundantPartitions(field);
      fields.add(field);
      return this;
    }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 340
FRAGMENT LINE AVG SIZE: 37.77777777777778
DEPTHS:
2 3 3 3 3 3 3 2 2 
AREA: 24
AVG DEPTH: 2.6666666666666665
NUMBER OF LINES IN FRAGMENT: 9
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public month(sourceName String, targetName String) : Builder extracted from public month(sourceName String) : Builder in class org.apache.iceberg.PartitionSpec.Builder
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/4bb65c18e9dd89698891be99cd9ac0efab85cf73/api/src/main/java/org/apache/iceberg/PartitionSpec.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/4bb65c18e9dd89698891be99cd9ac0efab85cf73/api/src/main/java/org/apache/iceberg/PartitionSpec.java#L401
DIRECTLY EXTRACTED OPERATION:
      checkAndAddPartitionName(targetName);
      Types.NestedField sourceColumn = findSourceColumn(sourceName);
      PartitionField field = new PartitionField(
          sourceColumn.fieldId(), targetName, Transforms.month(sourceColumn.type()));
      checkForRedundantPartitions(field);
      fields.add(field);
      return this;
    }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 341
FRAGMENT LINE AVG SIZE: 37.888888888888886
DEPTHS:
2 3 3 3 3 3 3 2 2 
AREA: 24
AVG DEPTH: 2.6666666666666665
NUMBER OF LINES IN FRAGMENT: 9
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public day(sourceName String, targetName String) : Builder extracted from public day(sourceName String) : Builder in class org.apache.iceberg.PartitionSpec.Builder
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/4bb65c18e9dd89698891be99cd9ac0efab85cf73/api/src/main/java/org/apache/iceberg/PartitionSpec.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/4bb65c18e9dd89698891be99cd9ac0efab85cf73/api/src/main/java/org/apache/iceberg/PartitionSpec.java#L415
DIRECTLY EXTRACTED OPERATION:
      checkAndAddPartitionName(targetName);
      Types.NestedField sourceColumn = findSourceColumn(sourceName);
      PartitionField field = new PartitionField(
          sourceColumn.fieldId(), targetName, Transforms.day(sourceColumn.type()));
      checkForRedundantPartitions(field);
      fields.add(field);
      return this;
    }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 339
FRAGMENT LINE AVG SIZE: 37.666666666666664
DEPTHS:
2 3 3 3 3 3 3 2 2 
AREA: 24
AVG DEPTH: 2.6666666666666665
NUMBER OF LINES IN FRAGMENT: 9
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public hour(sourceName String, targetName String) : Builder extracted from public hour(sourceName String) : Builder in class org.apache.iceberg.PartitionSpec.Builder
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/4bb65c18e9dd89698891be99cd9ac0efab85cf73/api/src/main/java/org/apache/iceberg/PartitionSpec.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/4bb65c18e9dd89698891be99cd9ac0efab85cf73/api/src/main/java/org/apache/iceberg/PartitionSpec.java#L429
DIRECTLY EXTRACTED OPERATION:
      checkAndAddPartitionName(targetName);
      Types.NestedField sourceColumn = findSourceColumn(sourceName);
      PartitionField field = new PartitionField(
          sourceColumn.fieldId(), targetName, Transforms.hour(sourceColumn.type()));
      checkForRedundantPartitions(field);
      fields.add(field);
      return this;
    }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 340
FRAGMENT LINE AVG SIZE: 37.77777777777778
DEPTHS:
2 3 3 3 3 3 3 2 2 
AREA: 24
AVG DEPTH: 2.6666666666666665
NUMBER OF LINES IN FRAGMENT: 9
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public bucket(sourceName String, numBuckets int, targetName String) : Builder extracted from public bucket(sourceName String, numBuckets int) : Builder in class org.apache.iceberg.PartitionSpec.Builder
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/4bb65c18e9dd89698891be99cd9ac0efab85cf73/api/src/main/java/org/apache/iceberg/PartitionSpec.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/4bb65c18e9dd89698891be99cd9ac0efab85cf73/api/src/main/java/org/apache/iceberg/PartitionSpec.java#L443
DIRECTLY EXTRACTED OPERATION:
      checkAndAddPartitionName(targetName);
      Types.NestedField sourceColumn = findSourceColumn(sourceName);
      fields.add(new PartitionField(
          sourceColumn.fieldId(), targetName, Transforms.bucket(sourceColumn.type(), numBuckets)));
      return this;
    }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 276
FRAGMENT LINE AVG SIZE: 39.42857142857143
DEPTHS:
2 3 3 3 3 2 2 
AREA: 18
AVG DEPTH: 2.5714285714285716
NUMBER OF LINES IN FRAGMENT: 7
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public truncate(sourceName String, width int, targetName String) : Builder extracted from public truncate(sourceName String, width int) : Builder in class org.apache.iceberg.PartitionSpec.Builder
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/4bb65c18e9dd89698891be99cd9ac0efab85cf73/api/src/main/java/org/apache/iceberg/PartitionSpec.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/4bb65c18e9dd89698891be99cd9ac0efab85cf73/api/src/main/java/org/apache/iceberg/PartitionSpec.java#L455
DIRECTLY EXTRACTED OPERATION:
      checkAndAddPartitionName(targetName);
      Types.NestedField sourceColumn = findSourceColumn(sourceName);
      fields.add(new PartitionField(
          sourceColumn.fieldId(), targetName, Transforms.truncate(sourceColumn.type(), width)));
      return this;
    }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 273
FRAGMENT LINE AVG SIZE: 39.0
DEPTHS:
2 3 3 3 3 2 2 
AREA: 18
AVG DEPTH: 2.5714285714285716
NUMBER OF LINES IN FRAGMENT: 7
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: c2435d64709b9657768f35ae5048c9ef4a352716
URL: https://github.com/apache/incubator-iceberg/commit/c2435d64709b9657768f35ae5048c9ef4a352716
DESCRIPTION: Extract Method	protected closeCurrent() : void extracted from public commit() : WriterCommitMessage in class org.apache.iceberg.spark.source.Writer.BaseWriter
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/c2435d64709b9657768f35ae5048c9ef4a352716/spark/src/main/java/org/apache/iceberg/spark/source/Writer.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/c2435d64709b9657768f35ae5048c9ef4a352716/spark/src/main/java/org/apache/iceberg/spark/source/Writer.java#L422
DIRECTLY EXTRACTED OPERATION:
      if (currentAppender != null) {
        currentAppender.close();
        // metrics are only valid after the appender is closed
        Metrics metrics = currentAppender.metrics();
        List<Long> splitOffsets = currentAppender.splitOffsets();
        this.currentAppender = null;

        if (metrics.recordCount() == 0L) {
          fileIo.deleteFile(currentFile.encryptingOutputFile());
        } else {
          DataFile dataFile = DataFiles.builder(spec)
              .withEncryptedOutputFile(currentFile)
              .withPartition(spec.fields().size() == 0 ? null : currentKey) // set null if unpartitioned
              .withMetrics(metrics)
              .withSplitOffsets(splitOffsets)
              .build();
          completedFiles.add(dataFile);
        }

        this.currentFile = null;
      }
    }

IS VOID METHOD: true
FRAGMENT LENGTH: 831
FRAGMENT LINE AVG SIZE: 36.130434782608695
DEPTHS:
3 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 4 4 4 3 2 2 
AREA: 95
AVG DEPTH: 4.130434782608695
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	protected closeCurrent() : void extracted from public abort() : void in class org.apache.iceberg.spark.source.Writer.BaseWriter
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/c2435d64709b9657768f35ae5048c9ef4a352716/spark/src/main/java/org/apache/iceberg/spark/source/Writer.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/c2435d64709b9657768f35ae5048c9ef4a352716/spark/src/main/java/org/apache/iceberg/spark/source/Writer.java#L422
DIRECTLY EXTRACTED OPERATION:
      if (currentAppender != null) {
        currentAppender.close();
        // metrics are only valid after the appender is closed
        Metrics metrics = currentAppender.metrics();
        List<Long> splitOffsets = currentAppender.splitOffsets();
        this.currentAppender = null;

        if (metrics.recordCount() == 0L) {
          fileIo.deleteFile(currentFile.encryptingOutputFile());
        } else {
          DataFile dataFile = DataFiles.builder(spec)
              .withEncryptedOutputFile(currentFile)
              .withPartition(spec.fields().size() == 0 ? null : currentKey) // set null if unpartitioned
              .withMetrics(metrics)
              .withSplitOffsets(splitOffsets)
              .build();
          completedFiles.add(dataFile);
        }

        this.currentFile = null;
      }
    }

IS VOID METHOD: true
FRAGMENT LENGTH: 831
FRAGMENT LINE AVG SIZE: 36.130434782608695
DEPTHS:
3 4 4 4 4 4 4 4 5 5 5 5 5 5 5 5 5 4 4 4 3 2 2 
AREA: 95
AVG DEPTH: 4.130434782608695
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: cb173335edf88053a614edecf7cdd4eb5509cc2d
URL: https://github.com/apache/incubator-iceberg/commit/cb173335edf88053a614edecf7cdd4eb5509cc2d
DESCRIPTION: Extract Method	private deletedFiles(manifests Iterable<ManifestFile>) : Set<CharSequenceWrapper> extracted from public apply(base TableMetadata) : List<ManifestFile> in class org.apache.iceberg.MergingSnapshotProducer
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/cb173335edf88053a614edecf7cdd4eb5509cc2d/core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/cb173335edf88053a614edecf7cdd4eb5509cc2d/core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java#L302
DIRECTLY EXTRACTED OPERATION:
    Set<CharSequenceWrapper> deletedFiles = Sets.newHashSet();

    for (ManifestFile manifest : manifests) {
      PartitionSpec manifestSpec = ops.current().spec(manifest.partitionSpecId());
      Iterable<DataFile> manifestDeletes = filteredManifestToDeletedFiles.get(manifest);
      if (manifestDeletes != null) {
        for (DataFile file : manifestDeletes) {
          summaryBuilder.deletedFile(manifestSpec, file);
          deletedFiles.add(CharSequenceWrapper.wrap(file.path()));
        }
      }
    }

    return deletedFiles;
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 547
FRAGMENT LINE AVG SIZE: 34.1875
DEPTHS:
1 2 2 3 3 3 4 5 5 4 3 2 2 2 1 1 
AREA: 43
AVG DEPTH: 2.6875
NUMBER OF LINES IN FRAGMENT: 16
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private groupManifestsByPartitionSpec(groups Map<Integer,List<ManifestFile>>, filtered Iterable<ManifestFile>) : void extracted from public apply(base TableMetadata) : List<ManifestFile> in class org.apache.iceberg.MergingSnapshotProducer
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/cb173335edf88053a614edecf7cdd4eb5509cc2d/core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/cb173335edf88053a614edecf7cdd4eb5509cc2d/core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java#L319
DIRECTLY EXTRACTED OPERATION:
    for (ManifestFile manifest : filtered) {
      List<ManifestFile> group = groups.get(manifest.partitionSpecId());
      if (group != null) {
        group.add(manifest);
      } else {
        group = Lists.newArrayList();
        group.add(manifest);
        groups.put(manifest.partitionSpecId(), group);
      }
    }
  }

PARAMS COUNT: 2
IS VOID METHOD: true
FRAGMENT LENGTH: 330
FRAGMENT LINE AVG SIZE: 27.5
DEPTHS:
2 3 3 4 4 4 4 4 3 2 1 1 
AREA: 35
AVG DEPTH: 2.9166666666666665
NUMBER OF LINES IN FRAGMENT: 12
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 8c0f53bbaee86dfc0806c649484dce400f39b2bb
URL: https://github.com/apache/incubator-iceberg/commit/8c0f53bbaee86dfc0806c649484dce400f39b2bb
DESCRIPTION: Extract Method	private deleteMetadataFiles(manifestsToDelete Set<String>, manifestListsToDelete Set<String>) : void extracted from public commit() : void in class org.apache.iceberg.RemoveSnapshots
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/8c0f53bbaee86dfc0806c649484dce400f39b2bb/core/src/main/java/org/apache/iceberg/RemoveSnapshots.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/8c0f53bbaee86dfc0806c649484dce400f39b2bb/core/src/main/java/org/apache/iceberg/RemoveSnapshots.java#L244
DIRECTLY EXTRACTED OPERATION:
    LOG.warn("Manifests to delete: {}", Joiner.on(", ").join(manifestsToDelete));

    Tasks.foreach(manifestsToDelete)
        .noRetry().suppressFailureWhenFinished()
        .onFailure((manifest, exc) -> LOG.warn("Delete failed for manifest: {}", manifest, exc))
        .run(deleteFunc::accept);

    Tasks.foreach(manifestListsToDelete)
        .noRetry().suppressFailureWhenFinished()
        .onFailure((list, exc) -> LOG.warn("Delete failed for manifest list: {}", list, exc))
        .run(deleteFunc::accept);
  }

PARAMS COUNT: 2
IS VOID METHOD: true
FRAGMENT LENGTH: 524
FRAGMENT LINE AVG SIZE: 40.30769230769231
DEPTHS:
1 2 2 2 2 2 2 2 2 2 2 1 1 
AREA: 23
AVG DEPTH: 1.7692307692307692
NUMBER OF LINES IN FRAGMENT: 13
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private deleteDataFiles(manifestsToScan Set<String>, manifestsToRevert Set<String>, validIds Set<Long>) : void extracted from public commit() : void in class org.apache.iceberg.RemoveSnapshots
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/8c0f53bbaee86dfc0806c649484dce400f39b2bb/core/src/main/java/org/apache/iceberg/RemoveSnapshots.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/8c0f53bbaee86dfc0806c649484dce400f39b2bb/core/src/main/java/org/apache/iceberg/RemoveSnapshots.java#L258
DIRECTLY EXTRACTED OPERATION:
    Set<String> filesToDelete = findFilesToDelete(manifestsToScan, manifestsToRevert, validIds);
    Tasks.foreach(filesToDelete)
        .noRetry().suppressFailureWhenFinished()
        .onFailure((file, exc) -> LOG.warn("Delete failed for data file: {}", file, exc))
        .run(file -> deleteFunc.accept(file));
  }

PARAMS COUNT: 3
IS VOID METHOD: true
FRAGMENT LENGTH: 321
FRAGMENT LINE AVG SIZE: 45.857142857142854
DEPTHS:
1 2 2 2 2 1 1 
AREA: 11
AVG DEPTH: 1.5714285714285714
NUMBER OF LINES IN FRAGMENT: 7
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private cleanExpiredSnapshots() : void extracted from public commit() : void in class org.apache.iceberg.RemoveSnapshots
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/8c0f53bbaee86dfc0806c649484dce400f39b2bb/core/src/main/java/org/apache/iceberg/RemoveSnapshots.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/8c0f53bbaee86dfc0806c649484dce400f39b2bb/core/src/main/java/org/apache/iceberg/RemoveSnapshots.java#L129
DIRECTLY EXTRACTED OPERATION:
    // clean up the expired snapshots:
    // 1. Get a list of the snapshots that were removed
    // 2. Delete any data files that were deleted by those snapshots and are not in the table
    // 3. Delete any manifests that are no longer used by current snapshots
    // 4. Delete the manifest lists

    TableMetadata current = ops.refresh();

    Set<Long> validIds = Sets.newHashSet();
    for (Snapshot snapshot : current.snapshots()) {
      validIds.add(snapshot.snapshotId());
    }

    Set<Long> expiredIds = Sets.newHashSet();
    for (Snapshot snapshot : base.snapshots()) {
      long snapshotId = snapshot.snapshotId();
      if (!validIds.contains(snapshotId)) {
        // the snapshot was expired
        LOG.info("Expired snapshot: {}", snapshot);
        expiredIds.add(snapshotId);
      }
    }

    if (expiredIds.isEmpty()) {
      // if no snapshots were expired, skip cleanup
      return;
    }

    LOG.info("Committed snapshot changes; cleaning up expired manifests and data files.");

    cleanExpiredFiles(current.snapshots(), validIds, expiredIds);
  }

IS VOID METHOD: true
FRAGMENT LENGTH: 1085
FRAGMENT LINE AVG SIZE: 32.878787878787875
DEPTHS:
1 2 2 2 2 2 2 2 2 2 3 2 2 2 2 3 3 4 4 4 3 2 2 2 3 3 2 2 2 2 2 1 1 
AREA: 75
AVG DEPTH: 2.272727272727273
NUMBER OF LINES IN FRAGMENT: 33
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 33e340fd1fc9c2ed06468abcc5dea8426abb54fc
URL: https://github.com/apache/incubator-iceberg/commit/33e340fd1fc9c2ed06468abcc5dea8426abb54fc
DESCRIPTION: Extract Method	public toByteBuffer(typeId Type.TypeID, value Object) : ByteBuffer extracted from public toByteBuffer(type Type, value Object) : ByteBuffer in class org.apache.iceberg.types.Conversions
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/33e340fd1fc9c2ed06468abcc5dea8426abb54fc/api/src/main/java/org/apache/iceberg/types/Conversions.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/33e340fd1fc9c2ed06468abcc5dea8426abb54fc/api/src/main/java/org/apache/iceberg/types/Conversions.java#L84
DIRECTLY EXTRACTED OPERATION:
    if (value == null) {
      return null;
    }

    switch (typeId) {
      case BOOLEAN:
        return ByteBuffer.allocate(1).put(0, (Boolean) value ? (byte) 0x01 : (byte) 0x00);
      case INTEGER:
      case DATE:
        return ByteBuffer.allocate(4).order(ByteOrder.LITTLE_ENDIAN).putInt(0, (int) value);
      case LONG:
      case TIME:
      case TIMESTAMP:
        return ByteBuffer.allocate(8).order(ByteOrder.LITTLE_ENDIAN).putLong(0, (long) value);
      case FLOAT:
        return ByteBuffer.allocate(4).order(ByteOrder.LITTLE_ENDIAN).putFloat(0, (float) value);
      case DOUBLE:
        return ByteBuffer.allocate(8).order(ByteOrder.LITTLE_ENDIAN).putDouble(0, (double) value);
      case STRING:
        CharBuffer buffer = CharBuffer.wrap((CharSequence) value);
        try {
          return ENCODER.get().encode(buffer);
        } catch (CharacterCodingException e) {
          throw new RuntimeIOException(e, "Failed to encode value as UTF-8: " + value);
        }
      case UUID:
        UUID uuid = (UUID) value;
        return ByteBuffer.allocate(16).order(ByteOrder.BIG_ENDIAN)
            .putLong(0, uuid.getMostSignificantBits())
            .putLong(8, uuid.getLeastSignificantBits());
      case FIXED:
      case BINARY:
        return (ByteBuffer) value;
      case DECIMAL:
        return ByteBuffer.wrap(((BigDecimal) value).unscaledValue().toByteArray());
      default:
        throw new UnsupportedOperationException("Cannot serialize type: " + typeId);
    }
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 1507
FRAGMENT LINE AVG SIZE: 37.675
DEPTHS:
2 3 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 3 3 3 3 3 3 3 3 3 3 3 3 3 2 1 1 
AREA: 114
AVG DEPTH: 2.85
NUMBER OF LINES IN FRAGMENT: 40
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: e7197a7e9cda676ddce402a258ca6e3ebfb2dcad
URL: https://github.com/apache/incubator-iceberg/commit/e7197a7e9cda676ddce402a258ca6e3ebfb2dcad
DESCRIPTION: Extract Method	private containsNullsOnly(id Integer) : boolean extracted from public notNull(ref BoundReference<T>) : Boolean in class org.apache.iceberg.expressions.InclusiveMetricsEvaluator.MetricsEvalVisitor
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e7197a7e9cda676ddce402a258ca6e3ebfb2dcad/api/src/main/java/org/apache/iceberg/expressions/InclusiveMetricsEvaluator.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e7197a7e9cda676ddce402a258ca6e3ebfb2dcad/api/src/main/java/org/apache/iceberg/expressions/InclusiveMetricsEvaluator.java#L276
DIRECTLY EXTRACTED OPERATION:
      return valueCounts != null && valueCounts.containsKey(id) &&
          nullCounts != null && nullCounts.containsKey(id) &&
          valueCounts.get(id) - nullCounts.get(id) == 0;
    }
  }
PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 196
FRAGMENT LINE AVG SIZE: 39.2
DEPTHS:
2 3 3 2 1 
AREA: 11
AVG DEPTH: 2.2
NUMBER OF LINES IN FRAGMENT: 5
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private containsNullsOnly(id Integer) : boolean extracted from public isNull(ref BoundReference<T>) : Boolean in class org.apache.iceberg.expressions.StrictMetricsEvaluator.MetricsEvalVisitor
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e7197a7e9cda676ddce402a258ca6e3ebfb2dcad/api/src/main/java/org/apache/iceberg/expressions/StrictMetricsEvaluator.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e7197a7e9cda676ddce402a258ca6e3ebfb2dcad/api/src/main/java/org/apache/iceberg/expressions/StrictMetricsEvaluator.java#L322
DIRECTLY EXTRACTED OPERATION:
      return valueCounts != null && valueCounts.containsKey(id) &&
          nullCounts != null && nullCounts.containsKey(id) &&
          valueCounts.get(id) - nullCounts.get(id) == 0;
    }
  }
PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 196
FRAGMENT LINE AVG SIZE: 39.2
DEPTHS:
2 3 3 2 1 
AREA: 11
AVG DEPTH: 2.2
NUMBER OF LINES IN FRAGMENT: 5
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 92b5b450bbf3c392785b45b97446f88ed2d5b6c5
URL: https://github.com/apache/incubator-iceberg/commit/92b5b450bbf3c392785b45b97446f88ed2d5b6c5
DESCRIPTION: Extract Method	protected refreshFromMetadataLocation(newLocation String, shouldRetry Predicate<Exception>, numRetries int) : void extracted from protected refreshFromMetadataLocation(newLocation String, numRetries int) : void in class org.apache.iceberg.BaseMetastoreTableOperations
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/92b5b450bbf3c392785b45b97446f88ed2d5b6c5/core/src/main/java/org/apache/iceberg/BaseMetastoreTableOperations.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/92b5b450bbf3c392785b45b97446f88ed2d5b6c5/core/src/main/java/org/apache/iceberg/BaseMetastoreTableOperations.java#L93
DIRECTLY EXTRACTED OPERATION:
                                             int numRetries) {
    // use null-safe equality check because new tables have a null metadata location
    if (!Objects.equal(currentMetadataLocation, newLocation)) {
      LOG.info("Refreshing table metadata from new version: {}", newLocation);

      AtomicReference<TableMetadata> newMetadata = new AtomicReference<>();
      Tasks.foreach(newLocation)
          .retry(numRetries).exponentialBackoff(100, 5000, 600000, 4.0 /* 100, 400, 1600, ... */)
          .throwFailureWhenFinished()
          .shouldRetryTest(shouldRetry)
          .run(metadataLocation -> newMetadata.set(
              TableMetadataParser.read(this, io().newInputFile(metadataLocation))));

      String newUUID = newMetadata.get().uuid();
      if (currentMetadata != null) {
        Preconditions.checkState(newUUID == null || newUUID.equals(currentMetadata.uuid()),
            "Table UUID does not match: current=%s != refreshed=%s", currentMetadata.uuid(), newUUID);
      }

      this.currentMetadata = newMetadata.get();
      this.currentMetadataLocation = newLocation;
      this.version = parseVersion(newLocation);
    }
    this.shouldRefresh = false;
  }

PARAMS COUNT: 3
IS VOID METHOD: true
FRAGMENT LENGTH: 1194
FRAGMENT LINE AVG SIZE: 45.92307692307692
DEPTHS:
1 2 2 3 3 3 3 3 3 3 3 3 3 3 3 4 4 3 3 3 3 3 2 2 1 1 
AREA: 70
AVG DEPTH: 2.6923076923076925
NUMBER OF LINES IN FRAGMENT: 26
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public internalWrite(metadata TableMetadata, outputFile OutputFile, overwrite boolean) : void extracted from public write(metadata TableMetadata, outputFile OutputFile) : void in class org.apache.iceberg.TableMetadataParser
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/92b5b450bbf3c392785b45b97446f88ed2d5b6c5/core/src/main/java/org/apache/iceberg/TableMetadataParser.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/92b5b450bbf3c392785b45b97446f88ed2d5b6c5/core/src/main/java/org/apache/iceberg/TableMetadataParser.java#L107
DIRECTLY EXTRACTED OPERATION:
      TableMetadata metadata, OutputFile outputFile, boolean overwrite) {
    boolean isGzip = Codec.fromFileName(outputFile.location()) == Codec.GZIP;
    OutputStream stream = overwrite ? outputFile.createOrOverwrite() : outputFile.create();
    try (OutputStreamWriter writer = new OutputStreamWriter(isGzip ? new GZIPOutputStream(stream) : stream)) {
      JsonGenerator generator = JsonUtil.factory().createGenerator(writer);
      generator.useDefaultPrettyPrinter();
      toJson(metadata, generator);
      generator.flush();
    } catch (IOException e) {
      throw new RuntimeIOException(e, "Failed to write json to file: %s", outputFile);
    }
  }

PARAMS COUNT: 3
IS VOID METHOD: true
FRAGMENT LENGTH: 662
FRAGMENT LINE AVG SIZE: 50.92307692307692
DEPTHS:
1 2 2 2 3 3 3 3 3 3 2 1 1 
AREA: 29
AVG DEPTH: 2.230769230769231
NUMBER OF LINES IN FRAGMENT: 13
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: ce226cf8ba851641f789d36bd2077925e63ef65b
URL: https://github.com/apache/incubator-iceberg/commit/ce226cf8ba851641f789d36bd2077925e63ef65b
DESCRIPTION: Extract Method	private commitCreateTransaction() : void extracted from public commitTransaction() : void in class org.apache.iceberg.BaseTransaction
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/ce226cf8ba851641f789d36bd2077925e63ef65b/core/src/main/java/org/apache/iceberg/BaseTransaction.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/ce226cf8ba851641f789d36bd2077925e63ef65b/core/src/main/java/org/apache/iceberg/BaseTransaction.java#L213
DIRECTLY EXTRACTED OPERATION:
    // fix up the snapshot log, which should not contain intermediate snapshots
    TableMetadata createMetadata = current.removeSnapshotLogEntries(intermediateSnapshotIds);

    // this operation creates the table. if the commit fails, this cannot retry because another
    // process has created the same table.
    try {
      ops.commit(null, createMetadata);

    } catch (RuntimeException e) {
      // the commit failed and no files were committed. clean up each update.
      Tasks.foreach(updates)
          .suppressFailureWhenFinished()
          .run(update -> {
            if (update instanceof SnapshotProducer) {
              ((SnapshotProducer) update).cleanAll();
            }
          });

      throw e;

    } finally {
      // create table never needs to retry because the table has no previous state. because retries are not a
      // concern, it is safe to delete all of the deleted files from individual operations
      Tasks.foreach(deletedFiles)
          .suppressFailureWhenFinished()
          .onFailure((file, exc) -> LOG.warn("Failed to delete uncommitted file: {}", file, exc))
          .run(ops.io()::deleteFile);
    }
  }

IS VOID METHOD: true
FRAGMENT LENGTH: 1167
FRAGMENT LINE AVG SIZE: 38.9
DEPTHS:
1 2 2 2 2 2 3 3 3 3 3 3 3 4 5 4 3 3 3 3 3 3 3 3 3 3 3 2 1 1 
AREA: 82
AVG DEPTH: 2.7333333333333334
NUMBER OF LINES IN FRAGMENT: 30
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private commitReplaceTransaction() : void extracted from public commitTransaction() : void in class org.apache.iceberg.BaseTransaction
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/ce226cf8ba851641f789d36bd2077925e63ef65b/core/src/main/java/org/apache/iceberg/BaseTransaction.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/ce226cf8ba851641f789d36bd2077925e63ef65b/core/src/main/java/org/apache/iceberg/BaseTransaction.java#L244
DIRECTLY EXTRACTED OPERATION:
    // fix up the snapshot log, which should not contain intermediate snapshots
    TableMetadata replaceMetadata = current.removeSnapshotLogEntries(intermediateSnapshotIds);

    try {
      Tasks.foreach(ops)
          .retry(base.propertyAsInt(COMMIT_NUM_RETRIES, COMMIT_NUM_RETRIES_DEFAULT))
          .exponentialBackoff(
              base.propertyAsInt(COMMIT_MIN_RETRY_WAIT_MS, COMMIT_MIN_RETRY_WAIT_MS_DEFAULT),
              base.propertyAsInt(COMMIT_MAX_RETRY_WAIT_MS, COMMIT_MAX_RETRY_WAIT_MS_DEFAULT),
              base.propertyAsInt(COMMIT_TOTAL_RETRY_TIME_MS, COMMIT_TOTAL_RETRY_TIME_MS_DEFAULT),
              2.0 /* exponential */)
          .onlyRetryOn(CommitFailedException.class)
          .run(underlyingOps -> {
            // because this is a replace table, it will always completely replace the table
            // metadata. even if it was just updated.
            if (base != underlyingOps.refresh()) {
              this.base = underlyingOps.current(); // just refreshed
            }

            underlyingOps.commit(base, replaceMetadata);
          });

    } catch (RuntimeException e) {
      // the commit failed and no files were committed. clean up each update.
      Tasks.foreach(updates)
          .suppressFailureWhenFinished()
          .run(update -> {
            if (update instanceof SnapshotProducer) {
              ((SnapshotProducer) update).cleanAll();
            }
          });

      throw e;

    } finally {
      // replace table never needs to retry because the table state is completely replaced. because retries are not
      // a concern, it is safe to delete all of the deleted files from individual operations
      Tasks.foreach(deletedFiles)
          .suppressFailureWhenFinished()
          .onFailure((file, exc) -> LOG.warn("Failed to delete uncommitted file: {}", file, exc))
          .run(ops.io()::deleteFile);
    }
  }

IS VOID METHOD: true
FRAGMENT LENGTH: 1899
FRAGMENT LINE AVG SIZE: 43.15909090909091
DEPTHS:
1 2 2 2 3 3 3 3 3 3 3 3 3 4 4 4 5 4 4 4 3 3 3 3 3 3 3 4 5 4 3 3 3 3 3 3 3 3 3 3 3 2 1 1 
AREA: 134
AVG DEPTH: 3.0454545454545454
NUMBER OF LINES IN FRAGMENT: 44
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private commitSimpleTransaction() : void extracted from public commitTransaction() : void in class org.apache.iceberg.BaseTransaction
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/ce226cf8ba851641f789d36bd2077925e63ef65b/core/src/main/java/org/apache/iceberg/BaseTransaction.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/ce226cf8ba851641f789d36bd2077925e63ef65b/core/src/main/java/org/apache/iceberg/BaseTransaction.java#L289
DIRECTLY EXTRACTED OPERATION:
    // if there were no changes, don't try to commit
    if (base == current) {
      return;
    }

    // this is always set to the latest commit attempt's snapshot id.
    AtomicLong currentSnapshotId = new AtomicLong(-1L);

    try {
      Tasks.foreach(ops)
          .retry(base.propertyAsInt(COMMIT_NUM_RETRIES, COMMIT_NUM_RETRIES_DEFAULT))
          .exponentialBackoff(
              base.propertyAsInt(COMMIT_MIN_RETRY_WAIT_MS, COMMIT_MIN_RETRY_WAIT_MS_DEFAULT),
              base.propertyAsInt(COMMIT_MAX_RETRY_WAIT_MS, COMMIT_MAX_RETRY_WAIT_MS_DEFAULT),
              base.propertyAsInt(COMMIT_TOTAL_RETRY_TIME_MS, COMMIT_TOTAL_RETRY_TIME_MS_DEFAULT),
              2.0 /* exponential */)
          .onlyRetryOn(CommitFailedException.class)
          .run(underlyingOps -> {
            if (base != underlyingOps.refresh()) {
              this.base = underlyingOps.current(); // just refreshed
              this.current = base;
              for (PendingUpdate update : updates) {
                // re-commit each update in the chain to apply it and update current
                update.commit();
              }
            }

            currentSnapshotId.set(current.currentSnapshot().snapshotId());

            // fix up the snapshot log, which should not contain intermediate snapshots
            underlyingOps.commit(base, current.removeSnapshotLogEntries(intermediateSnapshotIds));
          });

    } catch (RuntimeException e) {
      // the commit failed and no files were committed. clean up each update.
      Tasks.foreach(updates)
          .suppressFailureWhenFinished()
          .run(update -> {
            if (update instanceof SnapshotProducer) {
              ((SnapshotProducer) update).cleanAll();
            }
          });

      // delete all files that were cleaned up
      Tasks.foreach(deletedFiles)
          .suppressFailureWhenFinished()
          .onFailure((file, exc) -> LOG.warn("Failed to delete uncommitted file: {}", file, exc))
          .run(ops.io()::deleteFile);

      throw e;
    }

    // the commit succeeded

    try {
      intermediateSnapshotIds.add(currentSnapshotId.get());

      // clean up the data files that were deleted by each operation. first, get the list of committed manifests to
      // ensure that no committed manifest is deleted. a manifest could be deleted in one successful operation
      // commit, but reused in another successful commit of that operation if the whole transaction is retried.
      Set<String> committedFiles = committedFiles(ops, intermediateSnapshotIds);
      if (committedFiles != null) {
        // delete all of the files that were deleted in the most recent set of operation commits
        Tasks.foreach(deletedFiles)
            .suppressFailureWhenFinished()
            .onFailure((file, exc) -> LOG.warn("Failed to delete uncommitted file: {}", file, exc))
            .run(path -> {
              if (!committedFiles.contains(path)) {
                ops.io().deleteFile(path);
              }
            });
      } else {
        LOG.warn("Failed to load metadata for a committed snapshot, skipping clean-up");
      }

    } catch (RuntimeException e) {
      LOG.warn("Failed to load committed metadata, skipping clean-up", e);
    }
  }

IS VOID METHOD: true
FRAGMENT LENGTH: 3271
FRAGMENT LINE AVG SIZE: 40.8875
DEPTHS:
1 2 3 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 4 5 5 5 6 6 5 4 4 4 4 4 4 3 3 3 3 3 3 3 4 5 4 3 3 3 3 3 3 3 3 3 2 2 2 2 2 3 3 3 3 3 3 3 4 4 4 4 4 5 6 5 4 4 4 3 3 3 3 2 1 1 
AREA: 261
AVG DEPTH: 3.2625
NUMBER OF LINES IN FRAGMENT: 80
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: e4c6fb979003b1629961b0c950ec6acec994de3b
URL: https://github.com/apache/incubator-iceberg/commit/e4c6fb979003b1629961b0c950ec6acec994de3b
DESCRIPTION: Extract Method	public snapshot() : Snapshot extracted from public planFiles() : CloseableIterable<FileScanTask> in class org.apache.iceberg.BaseTableScan
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e4c6fb979003b1629961b0c950ec6acec994de3b/core/src/main/java/org/apache/iceberg/BaseTableScan.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e4c6fb979003b1629961b0c950ec6acec994de3b/core/src/main/java/org/apache/iceberg/BaseTableScan.java#L200
DIRECTLY EXTRACTED OPERATION:
  public Snapshot snapshot() {
    return snapshotId != null ?
        ops.current().snapshot(snapshotId) :
        ops.current().currentSnapshot();
  }

IS VOID METHOD: false
FRAGMENT LENGTH: 154
FRAGMENT LINE AVG SIZE: 25.666666666666668
DEPTHS:
1 2 2 2 1 1 
AREA: 9
AVG DEPTH: 1.5
NUMBER OF LINES IN FRAGMENT: 6
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: bdd7fa470434d484ba5c8eb27a73055976be2804
URL: https://github.com/apache/incubator-iceberg/commit/bdd7fa470434d484ba5c8eb27a73055976be2804
DESCRIPTION: Extract Method	private metadataFilePath(metadataVersion int, codec TableMetadataParser.Codec) : Path extracted from private metadataFile(metadataVersion int) : Path in class org.apache.iceberg.hadoop.HadoopTableOperations
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/bdd7fa470434d484ba5c8eb27a73055976be2804/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/bdd7fa470434d484ba5c8eb27a73055976be2804/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java#L185
DIRECTLY EXTRACTED OPERATION:
    return metadataPath("v" + metadataVersion + TableMetadataParser.getFileExtension(codec));
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 99
FRAGMENT LINE AVG SIZE: 33.0
DEPTHS:
1 1 1 
AREA: 3
AVG DEPTH: 1.0
NUMBER OF LINES IN FRAGMENT: 3
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 3287fee28359d303df28fef6fc9ea7b95cfa348f
URL: https://github.com/apache/incubator-iceberg/commit/3287fee28359d303df28fef6fc9ea7b95cfa348f
DESCRIPTION: Extract Method	public fileMetrics(file InputFile, statsTruncateLength int) : Metrics extracted from public fileMetrics(file InputFile) : Metrics in class org.apache.iceberg.parquet.ParquetUtil
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/3287fee28359d303df28fef6fc9ea7b95cfa348f/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/3287fee28359d303df28fef6fc9ea7b95cfa348f/parquet/src/main/java/org/apache/iceberg/parquet/ParquetUtil.java#L66
DIRECTLY EXTRACTED OPERATION:
    try (ParquetFileReader reader = ParquetFileReader.open(ParquetIO.file(file))) {
      return footerMetrics(reader.getFooter(), statsTruncateLength);
    } catch (IOException e) {
      throw new RuntimeIOException(e, "Failed to read footer of file: %s", file);
    }
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 276
FRAGMENT LINE AVG SIZE: 39.42857142857143
DEPTHS:
2 3 3 3 2 1 1 
AREA: 15
AVG DEPTH: 2.142857142857143
NUMBER OF LINES IN FRAGMENT: 7
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: b5c23ebe3d2e9c3585b85cdd4c5b5e453a39a3b8
URL: https://github.com/apache/incubator-iceberg/commit/b5c23ebe3d2e9c3585b85cdd4c5b5e453a39a3b8
DESCRIPTION: Extract Method	private append(messages WriterCommitMessage[]) : void extracted from public commit(messages WriterCommitMessage[]) : void in class org.apache.iceberg.spark.source.Writer
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/b5c23ebe3d2e9c3585b85cdd4c5b5e453a39a3b8/spark/src/main/java/org/apache/iceberg/spark/source/Writer.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/b5c23ebe3d2e9c3585b85cdd4c5b5e453a39a3b8/spark/src/main/java/org/apache/iceberg/spark/source/Writer.java#L127
DIRECTLY EXTRACTED OPERATION:
    AppendFiles append = table.newAppend();

    int numFiles = 0;
    for (DataFile file : files(messages)) {
      numFiles += 1;
      append.appendFile(file);
    }

    commitOperation(append, numFiles, "append");
  }

PARAMS COUNT: 1
IS VOID METHOD: true
FRAGMENT LENGTH: 224
FRAGMENT LINE AVG SIZE: 20.363636363636363
DEPTHS:
1 2 2 2 3 3 2 2 2 1 1 
AREA: 21
AVG DEPTH: 1.9090909090909092
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 57e9bd09b0d3ff0d67d320f232946d150b877a41
URL: https://github.com/apache/incubator-iceberg/commit/57e9bd09b0d3ff0d67d320f232946d150b877a41
DESCRIPTION: Extract Method	private getFileFormat(tableProperties Map<String,String>, options DataSourceOptions) : FileFormat extracted from public createWriter(jobId String, dfStruct StructType, mode SaveMode, options DataSourceOptions) : Optional<DataSourceWriter> in class org.apache.iceberg.spark.source.IcebergSource
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/57e9bd09b0d3ff0d67d320f232946d150b877a41/spark/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/57e9bd09b0d3ff0d67d320f232946d150b877a41/spark/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java#L141
DIRECTLY EXTRACTED OPERATION:
    Optional<String> formatOption = options.get("write-format");
    String format = formatOption.orElse(tableProperties.getOrDefault(DEFAULT_FILE_FORMAT, DEFAULT_FILE_FORMAT_DEFAULT));
    return FileFormat.valueOf(format.toUpperCase(Locale.ENGLISH));
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 258
FRAGMENT LINE AVG SIZE: 51.6
DEPTHS:
1 2 2 1 1 
AREA: 7
AVG DEPTH: 1.4
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private validateWriteSchema(tableSchema Schema, dsStruct StructType) : void extracted from public createWriter(jobId String, dfStruct StructType, mode SaveMode, options DataSourceOptions) : Optional<DataSourceWriter> in class org.apache.iceberg.spark.source.IcebergSource
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/57e9bd09b0d3ff0d67d320f232946d150b877a41/spark/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/57e9bd09b0d3ff0d67d320f232946d150b877a41/spark/src/main/java/org/apache/iceberg/spark/source/IcebergSource.java#L147
DIRECTLY EXTRACTED OPERATION:
    Schema dsSchema = SparkSchemaUtil.convert(tableSchema, dsStruct);
    List<String> errors = CheckCompatibility.writeCompatibilityErrors(tableSchema, dsSchema);
    if (!errors.isEmpty()) {
      StringBuilder sb = new StringBuilder();
      sb.append("Cannot write incompatible dataset to table with schema:\n")
          .append(tableSchema)
          .append("\nProblems:");
      for (String error : errors) {
        sb.append("\n* ").append(error);
      }
      throw new IllegalArgumentException(sb.toString());
    }
  }
}
PARAMS COUNT: 2
IS VOID METHOD: true
FRAGMENT LENGTH: 535
FRAGMENT LINE AVG SIZE: 38.214285714285715
DEPTHS:
1 2 2 3 3 3 3 3 4 3 3 2 1 0 
AREA: 33
AVG DEPTH: 2.357142857142857
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 6ac23e03735d4514480a9f0155200faf7179f21b
URL: https://github.com/apache/incubator-iceberg/commit/6ac23e03735d4514480a9f0155200faf7179f21b
DESCRIPTION: Extract Method	package addEntry(entry ManifestEntry) : void extracted from public add(entry ManifestEntry) : void in class org.apache.iceberg.ManifestWriter
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/6ac23e03735d4514480a9f0155200faf7179f21b/core/src/main/java/org/apache/iceberg/ManifestWriter.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/6ac23e03735d4514480a9f0155200faf7179f21b/core/src/main/java/org/apache/iceberg/ManifestWriter.java#L99
DIRECTLY EXTRACTED OPERATION:
    switch (entry.status()) {
      case ADDED:
        addedFiles += 1;
        break;
      case EXISTING:
        existingFiles += 1;
        break;
      case DELETED:
        deletedFiles += 1;
        break;
    }
    stats.update(entry.file().partition());
    writer.add(entry);
  }

PARAMS COUNT: 1
IS VOID METHOD: true
FRAGMENT LENGTH: 292
FRAGMENT LINE AVG SIZE: 19.466666666666665
DEPTHS:
2 3 3 3 3 3 3 3 3 3 2 2 2 1 1 
AREA: 37
AVG DEPTH: 2.466666666666667
NUMBER OF LINES IN FRAGMENT: 15
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private cleanUncommittedAppends(committed Set<ManifestFile>) : void extracted from protected cleanUncommitted(committed Set<ManifestFile>) : void in class org.apache.iceberg.MergingSnapshotProducer
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/6ac23e03735d4514480a9f0155200faf7179f21b/core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/6ac23e03735d4514480a9f0155200faf7179f21b/core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java#L343
DIRECTLY EXTRACTED OPERATION:
    if (cachedNewManifest != null && !committed.contains(cachedNewManifest)) {
      deleteFile(cachedNewManifest.path());
      this.cachedNewManifest = null;
    }

    for (ManifestFile manifest : appendManifests) {
      if (!committed.contains(manifest)) {
        deleteFile(manifest.path());
      }
    }
  }

PARAMS COUNT: 1
IS VOID METHOD: true
FRAGMENT LENGTH: 318
FRAGMENT LINE AVG SIZE: 26.5
DEPTHS:
2 3 3 2 2 2 3 4 3 2 1 1 
AREA: 28
AVG DEPTH: 2.3333333333333335
NUMBER OF LINES IN FRAGMENT: 12
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 6d11edd196c6ba7813af2145035787ac2b41ffda
URL: https://github.com/apache/incubator-iceberg/commit/6d11edd196c6ba7813af2145035787ac2b41ffda
DESCRIPTION: Extract Method	private newOrcWriter(file OutputFile, columnIds ColumnIdMap, options OrcFile.WriterOptions, metadata Map<String,byte[]>) : Writer extracted from package OrcFileAppender(schema Schema, file OutputFile, options OrcFile.WriterOptions, metadata Map<String,byte[]>) in class org.apache.iceberg.orc.OrcFileAppender
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/6d11edd196c6ba7813af2145035787ac2b41ffda/orc/src/main/java/org/apache/iceberg/orc/OrcFileAppender.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/6d11edd196c6ba7813af2145035787ac2b41ffda/orc/src/main/java/org/apache/iceberg/orc/OrcFileAppender.java#L131
DIRECTLY EXTRACTED OPERATION:
                                     ColumnIdMap columnIds,
                                     OrcFile.WriterOptions options, Map<String, byte[]> metadata) {
    final Path locPath = new Path(file.location());
    final Writer writer;

    try {
      writer = OrcFile.createWriter(locPath, options);
    } catch (IOException e) {
      throw new RuntimeException("Can't create file " + locPath, e);
    }

    writer.addUserMetadata(COLUMN_NUMBERS_ATTRIBUTE, columnIds.serialize());
    metadata.forEach((key,value) -> writer.addUserMetadata(key, ByteBuffer.wrap(value)));

    return writer;
  }

PARAMS COUNT: 4
IS VOID METHOD: false
FRAGMENT LENGTH: 601
FRAGMENT LINE AVG SIZE: 35.35294117647059
DEPTHS:
0 1 2 2 2 2 3 3 3 2 2 2 2 2 2 1 1 
AREA: 32
AVG DEPTH: 1.8823529411764706
NUMBER OF LINES IN FRAGMENT: 17
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: a534853b1d40bebd4227943f1abe5e29264675a2
URL: https://github.com/apache/incubator-iceberg/commit/a534853b1d40bebd4227943f1abe5e29264675a2
DESCRIPTION: Extract Method	package writeRecords(schema Schema, properties Map<String,String>, createWriterFunc Function<MessageType,ParquetValueWriter<?>>, records GenericData.Record...) : File extracted from private writeRecords(schema Schema, records Record...) : File in class org.apache.iceberg.parquet.BaseParquetWritingTest
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/a534853b1d40bebd4227943f1abe5e29264675a2/parquet/src/test/java/org/apache/iceberg/parquet/BaseParquetWritingTest.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/a534853b1d40bebd4227943f1abe5e29264675a2/parquet/src/test/java/org/apache/iceberg/parquet/BaseParquetWritingTest.java#L51
DIRECTLY EXTRACTED OPERATION:
      Schema schema, Map<String, String> properties,
      Function<MessageType, ParquetValueWriter<?>> createWriterFunc,
      GenericData.Record... records) throws IOException {
    File tmpFolder = temp.newFolder("parquet");
    String filename = UUID.randomUUID().toString();
    File file = new File(tmpFolder, FileFormat.PARQUET.addExtension(filename));
    try (FileAppender<GenericData.Record> writer = Parquet.write(localOutput(file))
        .schema(schema)
        .setAll(properties)
        .createWriterFunc(createWriterFunc)
        .build()) {
      writer.addAll(Lists.newArrayList(records));
    }
    return file;
  }
}
PARAMS COUNT: 4
IS VOID METHOD: false
FRAGMENT LENGTH: 639
FRAGMENT LINE AVG SIZE: 39.9375
DEPTHS:
0 1 1 2 2 2 2 2 2 2 2 3 2 2 1 0 
AREA: 26
AVG DEPTH: 1.625
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 5d781f19ad0bfdba2523c8483d51bf3cf31c7d81
URL: https://github.com/apache/incubator-iceberg/commit/5d781f19ad0bfdba2523c8483d51bf3cf31c7d81
DESCRIPTION: Extract Method	private filterManifests(metricsEvaluator StrictMetricsEvaluator, manifests List<ManifestFile>) : ManifestFile[] extracted from public apply(base TableMetadata) : List<ManifestFile> in class org.apache.iceberg.MergingSnapshotUpdate
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/5d781f19ad0bfdba2523c8483d51bf3cf31c7d81/core/src/main/java/org/apache/iceberg/MergingSnapshotUpdate.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/5d781f19ad0bfdba2523c8483d51bf3cf31c7d81/core/src/main/java/org/apache/iceberg/MergingSnapshotUpdate.java#L240
DIRECTLY EXTRACTED OPERATION:
      throws IOException {
    ManifestFile[] filtered = new ManifestFile[manifests.size()];
    // open all of the manifest files in parallel, use index to avoid reordering
    Tasks.range(filtered.length)
        .stopOnFailure().throwFailureWhenFinished()
        .executeWith(ThreadPools.getWorkerPool())
        .run(index -> {
          ManifestFile manifest = filterManifest(metricsEvaluator, manifests.get(index));
          filtered[index] = manifest;
        }, IOException.class);
    return filtered;
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 518
FRAGMENT LINE AVG SIZE: 39.84615384615385
DEPTHS:
1 2 2 2 2 2 2 3 3 2 2 1 1 
AREA: 25
AVG DEPTH: 1.9230769230769231
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private groupManifestsByPartitionSpec(groups Map<Integer,List<ManifestFile>>, deletedFiles Set<CharSequenceWrapper>, filtered ManifestFile[]) : void extracted from public apply(base TableMetadata) : List<ManifestFile> in class org.apache.iceberg.MergingSnapshotUpdate
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/5d781f19ad0bfdba2523c8483d51bf3cf31c7d81/core/src/main/java/org/apache/iceberg/MergingSnapshotUpdate.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/5d781f19ad0bfdba2523c8483d51bf3cf31c7d81/core/src/main/java/org/apache/iceberg/MergingSnapshotUpdate.java#L254
DIRECTLY EXTRACTED OPERATION:
      Map<Integer, List<ManifestFile>> groups,
      Set<CharSequenceWrapper> deletedFiles, ManifestFile[] filtered) {
    for (ManifestFile manifest : filtered) {
      PartitionSpec manifestSpec = ops.current().spec(manifest.partitionSpecId());
      Iterable<DataFile> manifestDeletes = filteredManifestToDeletedFiles.get(manifest);
      if (manifestDeletes != null) {
        for (DataFile file : manifestDeletes) {
          summaryBuilder.deletedFile(manifestSpec, file);
          deletedFiles.add(CharSequenceWrapper.wrap(file.path()));
        }
      }

      List<ManifestFile> group = groups.get(manifest.partitionSpecId());
      if (group != null) {
        group.add(manifest);
      } else {
        group = Lists.newArrayList();
        group.add(manifest);
        groups.put(manifest.partitionSpecId(), group);
      }
    }
  }

PARAMS COUNT: 3
IS VOID METHOD: true
FRAGMENT LENGTH: 850
FRAGMENT LINE AVG SIZE: 36.95652173913044
DEPTHS:
0 1 2 3 3 3 4 5 5 4 3 3 3 3 4 4 4 4 4 3 2 1 1 
AREA: 69
AVG DEPTH: 3.0
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private extractStrictDeleteExpression(reader ManifestReader) : Evaluator extracted from private filterManifest(deleteExpression Expression, metricsEvaluator StrictMetricsEvaluator, manifest ManifestFile) : ManifestFile in class org.apache.iceberg.MergingSnapshotUpdate
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/5d781f19ad0bfdba2523c8483d51bf3cf31c7d81/core/src/main/java/org/apache/iceberg/MergingSnapshotUpdate.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/5d781f19ad0bfdba2523c8483d51bf3cf31c7d81/core/src/main/java/org/apache/iceberg/MergingSnapshotUpdate.java#L452
DIRECTLY EXTRACTED OPERATION:
    Expression strictExpr = Projections
        .strict(reader.spec())
        .project(deleteExpression);
    return new Evaluator(reader.spec().partitionType(), strictExpr);
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 181
FRAGMENT LINE AVG SIZE: 30.166666666666668
DEPTHS:
1 2 2 2 1 1 
AREA: 9
AVG DEPTH: 1.5
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private extractInclusiveDeleteExpression(reader ManifestReader) : Evaluator extracted from private filterManifest(deleteExpression Expression, metricsEvaluator StrictMetricsEvaluator, manifest ManifestFile) : ManifestFile in class org.apache.iceberg.MergingSnapshotUpdate
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/5d781f19ad0bfdba2523c8483d51bf3cf31c7d81/core/src/main/java/org/apache/iceberg/MergingSnapshotUpdate.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/5d781f19ad0bfdba2523c8483d51bf3cf31c7d81/core/src/main/java/org/apache/iceberg/MergingSnapshotUpdate.java#L459
DIRECTLY EXTRACTED OPERATION:
    Expression inclusiveExpr = Projections
        .inclusive(reader.spec())
        .project(deleteExpression);
    return new Evaluator(reader.spec().partitionType(), inclusiveExpr);
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 190
FRAGMENT LINE AVG SIZE: 31.666666666666668
DEPTHS:
1 2 2 2 1 1 
AREA: 9
AVG DEPTH: 1.5
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private manifestHasDeletedFiles(metricsEvaluator StrictMetricsEvaluator, reader ManifestReader, pathWrapper CharSequenceWrapper, partitionWrapper StructLikeWrapper) : boolean extracted from private filterManifest(deleteExpression Expression, metricsEvaluator StrictMetricsEvaluator, manifest ManifestFile) : ManifestFile in class org.apache.iceberg.MergingSnapshotUpdate
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/5d781f19ad0bfdba2523c8483d51bf3cf31c7d81/core/src/main/java/org/apache/iceberg/MergingSnapshotUpdate.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/5d781f19ad0bfdba2523c8483d51bf3cf31c7d81/core/src/main/java/org/apache/iceberg/MergingSnapshotUpdate.java#L370
DIRECTLY EXTRACTED OPERATION:
      StrictMetricsEvaluator metricsEvaluator, ManifestReader reader,
      CharSequenceWrapper pathWrapper, StructLikeWrapper partitionWrapper) {
    Evaluator inclusive = extractInclusiveDeleteExpression(reader);
    Evaluator strict = extractStrictDeleteExpression(reader);
    boolean hasDeletedFiles = false;
    for (ManifestEntry entry : reader.entries()) {
      DataFile file = entry.file();
      boolean fileDelete = deletePaths.contains(pathWrapper.set(file.path())) ||
          dropPartitions.contains(partitionWrapper.set(file.partition()));
      if (fileDelete || inclusive.eval(file.partition())) {
        ValidationException.check(
            fileDelete || strict.eval(file.partition()) || metricsEvaluator.eval(file),
            "Cannot delete file where some, but not all, rows match filter %s: %s",
            this.deleteExpression, file.path());

        hasDeletedFiles = true;
        if (failAnyDelete) {
          throw new DeleteException(writeSpec().partitionToPath(file.partition()));
        }
        break; // as soon as a deleted file is detected, stop scanning
      }
    }
    return hasDeletedFiles;
  }

PARAMS COUNT: 4
IS VOID METHOD: false
FRAGMENT LENGTH: 1147
FRAGMENT LINE AVG SIZE: 45.88
DEPTHS:
0 1 2 2 2 2 3 3 3 3 4 4 4 4 4 4 4 5 4 4 3 2 2 1 1 
AREA: 71
AVG DEPTH: 2.84
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private filterManifestWithDeletedFiles(metricsEvaluator StrictMetricsEvaluator, manifest ManifestFile, reader ManifestReader, pathWrapper CharSequenceWrapper, partitionWrapper StructLikeWrapper) : ManifestFile extracted from private filterManifest(deleteExpression Expression, metricsEvaluator StrictMetricsEvaluator, manifest ManifestFile) : ManifestFile in class org.apache.iceberg.MergingSnapshotUpdate
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/5d781f19ad0bfdba2523c8483d51bf3cf31c7d81/core/src/main/java/org/apache/iceberg/MergingSnapshotUpdate.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/5d781f19ad0bfdba2523c8483d51bf3cf31c7d81/core/src/main/java/org/apache/iceberg/MergingSnapshotUpdate.java#L396
DIRECTLY EXTRACTED OPERATION:
      StrictMetricsEvaluator metricsEvaluator, ManifestFile manifest, ManifestReader reader,
      CharSequenceWrapper pathWrapper, StructLikeWrapper partitionWrapper) throws IOException {
    Evaluator inclusive = extractInclusiveDeleteExpression(reader);
    Evaluator strict = extractStrictDeleteExpression(reader);
    // when this point is reached, there is at least one file that will be deleted in the
    // manifest. produce a copy of the manifest with all deleted files removed.
    List<DataFile> deletedFiles = Lists.newArrayList();
    Set<CharSequenceWrapper> deletedPaths = Sets.newHashSet();
    OutputFile filteredCopy = manifestPath(manifestCount.getAndIncrement());
    ManifestWriter writer = new ManifestWriter(reader.spec(), filteredCopy, snapshotId());
    try {
      reader.entries().forEach(entry -> {
        DataFile file = entry.file();
        boolean fileDelete = deletePaths.contains(pathWrapper.set(file.path())) ||
            dropPartitions.contains(partitionWrapper.set(file.partition()));
        if (entry.status() != Status.DELETED) {
          if (fileDelete || inclusive.eval(file.partition())) {
            ValidationException.check(
                fileDelete || strict.eval(file.partition()) || metricsEvaluator.eval(file),
                "Cannot delete file where some, but not all, rows match filter %s: %s",
                this.deleteExpression, file.path());

            writer.delete(entry);

            CharSequenceWrapper wrapper = CharSequenceWrapper.wrap(entry.file().path());
            if (deletedPaths.contains(wrapper)) {
              LOG.warn("Deleting a duplicate path from manifest {}: {}",
                  manifest.path(), wrapper.get());
              summaryBuilder.incrementDuplicateDeletes();
            } else {
              // only add the file to deletes if it is a new delete
              // this keeps the snapshot summary accurate for non-duplicate data
              deletedFiles.add(entry.file().copy());
            }
            deletedPaths.add(wrapper);

          } else {
            writer.addExisting(entry);
          }
        }
      });
    } finally {
      writer.close();
    }

    // return the filtered manifest as a reader
    ManifestFile filtered = writer.toManifestFile();

    // update caches
    filteredManifests.put(manifest, filtered);
    filteredManifestToDeletedFiles.put(filtered, deletedFiles);

    return filtered;
  }

PARAMS COUNT: 5
IS VOID METHOD: false
FRAGMENT LENGTH: 2440
FRAGMENT LINE AVG SIZE: 44.36363636363637
DEPTHS:
0 1 2 2 2 2 2 2 2 2 2 3 4 4 4 4 5 6 6 6 6 6 6 6 6 6 7 7 7 7 7 7 7 6 6 6 6 6 5 4 3 3 3 2 2 2 2 2 2 2 2 2 2 1 1 
AREA: 216
AVG DEPTH: 3.9272727272727272
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private getFilesToDelete(currentIds Set<Long>, allManifests Set<ManifestFile>) : Set<String> extracted from public commit() : void in class org.apache.iceberg.RemoveSnapshots
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/5d781f19ad0bfdba2523c8483d51bf3cf31c7d81/core/src/main/java/org/apache/iceberg/RemoveSnapshots.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/5d781f19ad0bfdba2523c8483d51bf3cf31c7d81/core/src/main/java/org/apache/iceberg/RemoveSnapshots.java#L184
DIRECTLY EXTRACTED OPERATION:
    Set<String> filesToDelete = new ConcurrentSet<>();
    Tasks.foreach(allManifests)
        .noRetry().suppressFailureWhenFinished()
        .executeWith(ThreadPools.getWorkerPool())
        .onFailure((item, exc) ->
            LOG.warn("Failed to get deleted files: this may cause orphaned data files", exc)
        ).run(manifest -> {
          if (manifest.deletedFilesCount() != null && manifest.deletedFilesCount() == 0) {
            return;
          }

          // the manifest has deletes, scan it to find files to delete
          try (ManifestReader reader = ManifestReader.read(
              ops.io().newInputFile(manifest.path()), ops.current()::spec)) {
            for (ManifestEntry entry : reader.entries()) {
              // if the snapshot ID of the DELETE entry is no longer valid, the data can be deleted
              if (entry.status() == ManifestEntry.Status.DELETED &&
                  !currentIds.contains(entry.snapshotId())) {
                // use toString to ensure the path will not change (Utf8 is reused)
                filesToDelete.add(entry.file().path().toString());
              }
            }
          } catch (IOException e) {
            throw new RuntimeIOException(e, "Failed to read manifest file: " + manifest.path());
          }
        });
    return filesToDelete;
  }
}
PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 1333
FRAGMENT LINE AVG SIZE: 45.96551724137931
DEPTHS:
1 2 2 2 2 2 2 3 4 3 3 3 3 3 4 5 5 5 6 6 5 4 4 4 3 2 2 1 0 
AREA: 91
AVG DEPTH: 3.1379310344827585
NUMBER OF LINES IN FRAGMENT: 29
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private computeTopPartitionMetrics(rowFilter Expression, manifests Iterable<ManifestFile>, filterByTimestamp boolean, snapshotsInTimeRange Set<Long>) : Map<String,PartitionMetrics> extracted from public build() : Map<String,PartitionMetrics> in class org.apache.iceberg.ScanSummary.Builder
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/5d781f19ad0bfdba2523c8483d51bf3cf31c7d81/core/src/main/java/org/apache/iceberg/ScanSummary.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/5d781f19ad0bfdba2523c8483d51bf3cf31c7d81/core/src/main/java/org/apache/iceberg/ScanSummary.java#L211
DIRECTLY EXTRACTED OPERATION:
        Expression rowFilter,
        Iterable<ManifestFile> manifests,
        boolean filterByTimestamp,
        Set<Long> snapshotsInTimeRange) {
      TopN<String, PartitionMetrics> topN = new TopN<>(
          limit, throwIfLimited, Comparators.charSequences());

      try (CloseableIterable<ManifestEntry> entries = new ManifestGroup(ops, manifests)
          .filterData(rowFilter)
          .ignoreDeleted()
          .select(SCAN_SUMMARY_COLUMNS)
          .entries()) {

        PartitionSpec spec = table.spec();
        for (ManifestEntry entry : entries) {
          Long timestamp = snapshotTimestamps.get(entry.snapshotId());

          // if filtering, skip timestamps that are outside the range
          if (filterByTimestamp && !snapshotsInTimeRange.contains(entry.snapshotId())) {
            continue;
          }

          String partition = spec.partitionToPath(entry.file().partition());
          topN.update(partition, metrics -> (metrics == null ? new PartitionMetrics() : metrics)
              .updateFromFile(entry.file(), timestamp));
        }

      } catch (IOException e) {
        throw new RuntimeIOException(e);
      }

      return topN.get();
    }
  }
PARAMS COUNT: 4
IS VOID METHOD: false
FRAGMENT LENGTH: 1196
FRAGMENT LINE AVG SIZE: 35.1764705882353
DEPTHS:
1 2 2 2 3 3 3 3 3 3 3 3 4 4 4 5 5 5 5 6 5 5 5 5 5 4 4 4 4 3 3 3 2 1 
AREA: 122
AVG DEPTH: 3.588235294117647
NUMBER OF LINES IN FRAGMENT: 34
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private newBin() : Bin<T> extracted from public next() : List<T> in class org.apache.iceberg.util.BinPacking.PackingIterator
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/5d781f19ad0bfdba2523c8483d51bf3cf31c7d81/core/src/main/java/org/apache/iceberg/util/BinPacking.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/5d781f19ad0bfdba2523c8483d51bf3cf31c7d81/core/src/main/java/org/apache/iceberg/util/BinPacking.java#L148
DIRECTLY EXTRACTED OPERATION:
      return new Bin<>(targetWeight);
    }

IS VOID METHOD: false
FRAGMENT LENGTH: 45
FRAGMENT LINE AVG SIZE: 15.0
DEPTHS:
2 2 2 
AREA: 6
AVG DEPTH: 2.0
NUMBER OF LINES IN FRAGMENT: 3
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private tryRunOnFailure(item I, failure Exception) : void extracted from private runSingleThreaded(task Task<I,E>, exceptionClass Class<E>) : boolean in class org.apache.iceberg.util.Tasks.Builder
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/5d781f19ad0bfdba2523c8483d51bf3cf31c7d81/core/src/main/java/org/apache/iceberg/util/Tasks.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/5d781f19ad0bfdba2523c8483d51bf3cf31c7d81/core/src/main/java/org/apache/iceberg/util/Tasks.java#L272
DIRECTLY EXTRACTED OPERATION:
      try {
        onFailure.run(item, failure);
      } catch (Exception failException) {
        failure.addSuppressed(failException);
        LOG.error("Failed to clean up on failure", failException);
        // keep going
      }
    }

PARAMS COUNT: 2
IS VOID METHOD: true
FRAGMENT LENGTH: 242
FRAGMENT LINE AVG SIZE: 26.88888888888889
DEPTHS:
3 4 4 4 4 4 3 2 2 
AREA: 30
AVG DEPTH: 3.3333333333333335
NUMBER OF LINES IN FRAGMENT: 9
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private tryRunOnFailure(item I, failure Exception) : void extracted from private runParallel(task Task<I,E>, exceptionClass Class<E>) : boolean in class org.apache.iceberg.util.Tasks.Builder
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/5d781f19ad0bfdba2523c8483d51bf3cf31c7d81/core/src/main/java/org/apache/iceberg/util/Tasks.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/5d781f19ad0bfdba2523c8483d51bf3cf31c7d81/core/src/main/java/org/apache/iceberg/util/Tasks.java#L272
DIRECTLY EXTRACTED OPERATION:
      try {
        onFailure.run(item, failure);
      } catch (Exception failException) {
        failure.addSuppressed(failException);
        LOG.error("Failed to clean up on failure", failException);
        // keep going
      }
    }

PARAMS COUNT: 2
IS VOID METHOD: true
FRAGMENT LENGTH: 242
FRAGMENT LINE AVG SIZE: 26.88888888888889
DEPTHS:
3 4 4 4 4 4 3 2 2 
AREA: 30
AVG DEPTH: 3.3333333333333335
NUMBER OF LINES IN FRAGMENT: 9
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 84675a8ed5d600921adb886d50d42acc6d5252f8
URL: https://github.com/apache/incubator-iceberg/commit/84675a8ed5d600921adb886d50d42acc6d5252f8
DESCRIPTION: Extract Method	private renameToFinal(fs FileSystem, src Path, dst Path) : void extracted from public commit(base TableMetadata, metadata TableMetadata) : void in class org.apache.iceberg.hadoop.HadoopTableOperations
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/84675a8ed5d600921adb886d50d42acc6d5252f8/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/84675a8ed5d600921adb886d50d42acc6d5252f8/core/src/main/java/org/apache/iceberg/hadoop/HadoopTableOperations.java#L207
DIRECTLY EXTRACTED OPERATION:
   * Renames the source file to destination, using the provided file system. If the rename failed,
   * an attempt will be made to delete the source file.
   *
   * @param fs the filesystem used for the rename
   * @param src the source file
   * @param dst the destination file
   */
  private void renameToFinal(FileSystem fs, Path src, Path dst) {
    try {
      if (!fs.rename(src, dst)) {
        CommitFailedException cfe = new CommitFailedException(
            "Failed to commit changes using rename: %s", dst);
        RuntimeException re = tryDelete(src);
        if (re != null) {
          cfe.addSuppressed(re);
        }
        throw cfe;
      }
    } catch (IOException e) {
      CommitFailedException cfe = new CommitFailedException(e,
          "Failed to commit changes using rename: %s", dst);
      RuntimeException re = tryDelete(src);
      if (re != null) {
        cfe.addSuppressed(re);
      }
      throw cfe;
    }
  }

PARAMS COUNT: 3
IS VOID METHOD: true
FRAGMENT LENGTH: 952
FRAGMENT LINE AVG SIZE: 32.827586206896555
DEPTHS:
0 1 1 1 1 1 1 1 2 3 4 4 4 4 5 4 4 3 3 3 3 3 3 4 3 3 2 1 1 
AREA: 73
AVG DEPTH: 2.5172413793103448
NUMBER OF LINES IN FRAGMENT: 29
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 439a20d077f1746564f2e203e5904cd314ea8550
URL: https://github.com/apache/incubator-iceberg/commit/439a20d077f1746564f2e203e5904cd314ea8550
DESCRIPTION: Extract Method	private testDisallowPrimitiveToMap(from PrimitiveType, fromSchema Schema) : void extracted from public testPrimitiveTypes() : void in class org.apache.iceberg.types.TestReadabilityChecks
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/439a20d077f1746564f2e203e5904cd314ea8550/api/src/test/java/org/apache/iceberg/types/TestReadabilityChecks.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/439a20d077f1746564f2e203e5904cd314ea8550/api/src/test/java/org/apache/iceberg/types/TestReadabilityChecks.java#L76
DIRECTLY EXTRACTED OPERATION:
    Schema mapSchema = new Schema(required(1, "map_field",
        Types.MapType.ofRequired(2, 3, Types.StringType.get(), from)));

    List<String> errors = CheckCompatibility.writeCompatibilityErrors(mapSchema, fromSchema);
    Assert.assertEquals("Should produce 1 error message", 1, errors.size());

    Assert.assertTrue("Should complain that primitive to map is not allowed",
        errors.get(0).contains("cannot be read as a map"));
  }

PARAMS COUNT: 2
IS VOID METHOD: true
FRAGMENT LENGTH: 447
FRAGMENT LINE AVG SIZE: 44.7
DEPTHS:
1 2 2 2 2 2 2 2 1 1 
AREA: 17
AVG DEPTH: 1.7
NUMBER OF LINES IN FRAGMENT: 10
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private testDisallowPrimitiveToList(from PrimitiveType, fromSchema Schema) : void extracted from public testPrimitiveTypes() : void in class org.apache.iceberg.types.TestReadabilityChecks
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/439a20d077f1746564f2e203e5904cd314ea8550/api/src/test/java/org/apache/iceberg/types/TestReadabilityChecks.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/439a20d077f1746564f2e203e5904cd314ea8550/api/src/test/java/org/apache/iceberg/types/TestReadabilityChecks.java#L87
DIRECTLY EXTRACTED OPERATION:

    Schema listSchema = new Schema(required(1, "list_field", Types.ListType.ofRequired(2, from)));

    List<String> errors = CheckCompatibility.writeCompatibilityErrors(listSchema, fromSchema);
    Assert.assertEquals("Should produce 1 error message", 1, errors.size());
    Assert.assertTrue("Should complain that primitive to list is not allowed",
        errors.get(0).contains("cannot be read as a list"));
  }

PARAMS COUNT: 2
IS VOID METHOD: true
FRAGMENT LENGTH: 418
FRAGMENT LINE AVG SIZE: 46.44444444444444
DEPTHS:
1 2 2 2 2 2 2 1 1 
AREA: 15
AVG DEPTH: 1.6666666666666667
NUMBER OF LINES IN FRAGMENT: 9
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private testDisallowPrimitiveToStruct(from PrimitiveType, fromSchema Schema) : void extracted from public testPrimitiveTypes() : void in class org.apache.iceberg.types.TestReadabilityChecks
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/439a20d077f1746564f2e203e5904cd314ea8550/api/src/test/java/org/apache/iceberg/types/TestReadabilityChecks.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/439a20d077f1746564f2e203e5904cd314ea8550/api/src/test/java/org/apache/iceberg/types/TestReadabilityChecks.java#L97
DIRECTLY EXTRACTED OPERATION:
    Schema structSchema = new Schema(required(1, "struct_field", Types.StructType.of(
        required(2, "from", from))
    ));

    List<String> errors = CheckCompatibility.writeCompatibilityErrors(structSchema, fromSchema);
    Assert.assertEquals("Should produce 1 error message", 1, errors.size());
    Assert.assertTrue("Should complain that primitive to struct is not allowed",
        errors.get(0).contains("cannot be read as a struct"));
  }

PARAMS COUNT: 2
IS VOID METHOD: true
FRAGMENT LENGTH: 453
FRAGMENT LINE AVG SIZE: 45.3
DEPTHS:
1 2 2 2 2 2 2 2 1 1 
AREA: 17
AVG DEPTH: 1.7
NUMBER OF LINES IN FRAGMENT: 10
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: c8df297c27e9da954e4ce5bb6ea9d0d86b8bc78b
URL: https://github.com/apache/incubator-iceberg/commit/c8df297c27e9da954e4ce5bb6ea9d0d86b8bc78b
DESCRIPTION: Extract Method	private pack(items List<Integer>, targetWeight long, lookback int, largestBinFirst boolean) : List<List<Integer>> extracted from private pack(items List<Integer>, targetWeight long, lookback int) : List<List<Integer>> in class org.apache.iceberg.util.TestBinPacking
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/c8df297c27e9da954e4ce5bb6ea9d0d86b8bc78b/core/src/test/java/org/apache/iceberg/util/TestBinPacking.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/c8df297c27e9da954e4ce5bb6ea9d0d86b8bc78b/core/src/test/java/org/apache/iceberg/util/TestBinPacking.java#L184
DIRECTLY EXTRACTED OPERATION:
    ListPacker<Integer> packer = new ListPacker<>(targetWeight, lookback, largestBinFirst);
    return packer.pack(items, Integer::longValue);
  }

PARAMS COUNT: 4
IS VOID METHOD: false
FRAGMENT LENGTH: 148
FRAGMENT LINE AVG SIZE: 37.0
DEPTHS:
1 2 1 1 
AREA: 5
AVG DEPTH: 1.25
NUMBER OF LINES IN FRAGMENT: 4
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 5e37ff424a7be3a5ac107421d41e5df8c0dbe33c
URL: https://github.com/apache/incubator-iceberg/commit/5e37ff424a7be3a5ac107421d41e5df8c0dbe33c
DESCRIPTION: Extract Method	private internalSelect(names Collection<String>, caseSensitive boolean) : Schema extracted from public select(names Collection<String>) : Schema in class com.netflix.iceberg.Schema
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/5e37ff424a7be3a5ac107421d41e5df8c0dbe33c/api/src/main/java/com/netflix/iceberg/Schema.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/5e37ff424a7be3a5ac107421d41e5df8c0dbe33c/api/src/main/java/com/netflix/iceberg/Schema.java#L246
DIRECTLY EXTRACTED OPERATION:
    if (names.contains(ALL_COLUMNS)) {
      return this;
    }

    Set<Integer> selected = Sets.newHashSet();
    for (String name : names) {
      Integer id;
      if (caseSensitive) {
        id = lazyNameToId().get(name);
      } else {
        id = lazyLowerCaseNameToId().get(name.toLowerCase(Locale.ROOT));
      }

      if (id != null) {
        selected.add(id);
      }
    }

    return TypeUtil.select(this, selected);
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 439
FRAGMENT LINE AVG SIZE: 20.904761904761905
DEPTHS:
2 3 2 2 2 2 3 3 4 4 4 3 3 3 4 3 2 2 2 1 1 
AREA: 55
AVG DEPTH: 2.619047619047619
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private lazyColumnProjection() : Schema extracted from public schema() : Schema in class com.netflix.iceberg.BaseTableScan
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/5e37ff424a7be3a5ac107421d41e5df8c0dbe33c/core/src/main/java/com/netflix/iceberg/BaseTableScan.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/5e37ff424a7be3a5ac107421d41e5df8c0dbe33c/core/src/main/java/com/netflix/iceberg/BaseTableScan.java#L258
DIRECTLY EXTRACTED OPERATION:
   * To be able to make refinements {@link #select(Collection)} and {@link #caseSensitive(boolean)} in any order,
   * we resolve the schema to be projected lazily here.
   *
   * @return the Schema to project
   */
  private Schema lazyColumnProjection() {
    if (selectedColumns != null ) {
      Set<Integer> requiredFieldIds = Sets.newHashSet();

      // all of the filter columns are required
      requiredFieldIds.addAll(
          Binder.boundReferences(table.schema().asStruct(), Collections.singletonList(rowFilter), caseSensitive));

      // all of the projection columns are required
      Set<Integer> selectedIds;
      if (caseSensitive) {
        selectedIds = TypeUtil.getProjectedIds(table.schema().select(selectedColumns));
      } else {
        selectedIds = TypeUtil.getProjectedIds(table.schema().caseInsensitiveSelect(selectedColumns));
      }
      requiredFieldIds.addAll(selectedIds);

      return TypeUtil.select(table.schema(), requiredFieldIds);
    }

    return schema;
  }
}
IS VOID METHOD: false
FRAGMENT LENGTH: 1013
FRAGMENT LINE AVG SIZE: 36.17857142857143
DEPTHS:
0 1 1 1 1 1 2 3 3 3 3 3 3 3 3 3 4 4 4 3 3 3 3 2 2 2 1 0 
AREA: 65
AVG DEPTH: 2.3214285714285716
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 022fe36ba4afa7f22231df7a544eaae915a5153b
URL: https://github.com/apache/incubator-iceberg/commit/022fe36ba4afa7f22231df7a544eaae915a5153b
DESCRIPTION: Extract Method	public bind(struct StructType, expr Expression, caseSensitive boolean) : Expression extracted from public bind(struct StructType, expr Expression) : Expression in class com.netflix.iceberg.expressions.Binder
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/022fe36ba4afa7f22231df7a544eaae915a5153b/api/src/main/java/com/netflix/iceberg/expressions/Binder.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/022fe36ba4afa7f22231df7a544eaae915a5153b/api/src/main/java/com/netflix/iceberg/expressions/Binder.java#L39
DIRECTLY EXTRACTED OPERATION:
   * Replaces all unbound/named references with bound references to fields in the given struct.
   * <p>
   * When a reference is resolved, any literal used in a predicate for that field is converted to
   * the field's type using {@link Literal#to(Type)}. If automatic conversion to that type isn't
   * allowed, a {@link ValidationException validation exception} is thrown.
   * <p>
   * The result expression may be simplified when constructed. For example, {@code isNull("a")} is
   * replaced with {@code alwaysFalse()} when {@code "a"} is resolved to a required field.
   * <p>
   * The expression cannot contain references that are already bound, or an
   * {@link IllegalStateException} will be thrown.
   *
   * @param struct The {@link StructType struct type} to resolve references by name.
   * @param expr An {@link Expression expression} to rewrite with bound references.
   * @param caseSensitive A boolean flag to control whether the bind should enforce case sensitivity.
   * @return the expression rewritten with bound references
   * @throws ValidationException if literals do not match bound references
   * @throws IllegalStateException if any references are already bound
   */
  public static Expression bind(StructType struct,
                                Expression expr,
                                boolean caseSensitive) {
    return ExpressionVisitors.visit(expr, new BindVisitor(struct, caseSensitive));
  }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 1444
FRAGMENT LINE AVG SIZE: 57.76
DEPTHS:
0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 
AREA: 25
AVG DEPTH: 1.0
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public bind(struct Types.StructType, caseSensitive boolean) : Expression extracted from public bind(struct Types.StructType) : Expression in class com.netflix.iceberg.expressions.UnboundPredicate
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/022fe36ba4afa7f22231df7a544eaae915a5153b/api/src/main/java/com/netflix/iceberg/expressions/UnboundPredicate.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/022fe36ba4afa7f22231df7a544eaae915a5153b/api/src/main/java/com/netflix/iceberg/expressions/UnboundPredicate.java#L60
DIRECTLY EXTRACTED OPERATION:
   * Bind this UnboundPredicate.
   *
   * @param struct The {@link Types.StructType struct type} to resolve references by name.
   * @param caseSensitive A boolean flag to control whether the bind should enforce case sensitivity.
   * @return an {@link Expression}
   * @throws ValidationException if literals do not match bound references, or if comparison on expression is invalid
   */
  public Expression bind(Types.StructType struct, boolean caseSensitive) {
    Types.NestedField field;
    if (caseSensitive) {
      field = struct.field(ref().name());
    } else {
      field = struct.caseInsensitiveField(ref().name());
    }

    ValidationException.check(field != null,
        "Cannot find field '%s' in struct: %s", ref().name(), struct);

    if (literal() == null) {
      switch (op()) {
        case IS_NULL:
          if (field.isRequired()) {
            return Expressions.alwaysFalse();
          }
          return new BoundPredicate<>(IS_NULL, new BoundReference<>(struct, field.fieldId()));
        case NOT_NULL:
          if (field.isRequired()) {
            return Expressions.alwaysTrue();
          }
          return new BoundPredicate<>(NOT_NULL, new BoundReference<>(struct, field.fieldId()));
        default:
          throw new ValidationException("Operation must be IS_NULL or NOT_NULL");
      }
    }

    Literal<T> lit = literal().to(field.type());
    if (lit == null) {
      throw new ValidationException(String.format(
          "Invalid value for comparison inclusive type %s: %s (%s)",
          field.type(), literal().value(), literal().value().getClass().getName()));

    } else if (lit == Literals.aboveMax()) {
      switch (op()) {
        case LT:
        case LT_EQ:
        case NOT_EQ:
          return Expressions.alwaysTrue();
        case GT:
        case GT_EQ:
        case EQ:
          return Expressions.alwaysFalse();
//        case IN:
//          break;
//        case NOT_IN:
//          break;
      }
    } else if (lit == Literals.belowMin()) {
      switch (op()) {
        case GT:
        case GT_EQ:
        case NOT_EQ:
          return Expressions.alwaysTrue();
        case LT:
        case LT_EQ:
        case EQ:
          return Expressions.alwaysFalse();
//        case IN:
//          break;
//        case NOT_IN:
//          break;
      }
    }
    return new BoundPredicate<>(op(), new BoundReference<>(struct, field.fieldId()), lit);
  }
}
PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 2432
FRAGMENT LINE AVG SIZE: 32.42666666666667
DEPTHS:
0 1 1 1 1 1 1 1 2 2 3 3 3 2 2 2 2 2 2 3 4 4 5 4 4 4 4 5 4 4 4 4 3 2 2 2 2 3 3 3 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 3 2 2 1 0 
AREA: 227
AVG DEPTH: 3.026666666666667
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 0342f23a37d25c4e80c0cc0d82acae2c673e062d
URL: https://github.com/apache/incubator-iceberg/commit/0342f23a37d25c4e80c0cc0d82acae2c673e062d
DESCRIPTION: Extract Method	private createTableFolder() : File extracted from protected writeAndValidate(schema Schema) : void in class com.netflix.iceberg.spark.source.TestDataFrameWrites
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/0342f23a37d25c4e80c0cc0d82acae2c673e062d/spark/src/test/java/com/netflix/iceberg/spark/source/TestDataFrameWrites.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/0342f23a37d25c4e80c0cc0d82acae2c673e062d/spark/src/test/java/com/netflix/iceberg/spark/source/TestDataFrameWrites.java#L112
DIRECTLY EXTRACTED OPERATION:
    File parent = temp.newFolder("parquet");
    File location = new File(parent, "test");
    Assert.assertTrue("Mkdir should succeed", location.mkdirs());
    return location;
  }

IS VOID METHOD: false
FRAGMENT LENGTH: 183
FRAGMENT LINE AVG SIZE: 30.5
DEPTHS:
1 2 2 2 1 1 
AREA: 9
AVG DEPTH: 1.5
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private createTable(schema Schema, location File) : Table extracted from protected writeAndValidate(schema Schema) : void in class com.netflix.iceberg.spark.source.TestDataFrameWrites
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/0342f23a37d25c4e80c0cc0d82acae2c673e062d/spark/src/test/java/com/netflix/iceberg/spark/source/TestDataFrameWrites.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/0342f23a37d25c4e80c0cc0d82acae2c673e062d/spark/src/test/java/com/netflix/iceberg/spark/source/TestDataFrameWrites.java#L119
DIRECTLY EXTRACTED OPERATION:
    HadoopTables tables = new HadoopTables(CONF);
    return tables.create(schema, PartitionSpec.unpartitioned(), location.toString());
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 141
FRAGMENT LINE AVG SIZE: 35.25
DEPTHS:
1 2 1 1 
AREA: 5
AVG DEPTH: 1.25
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private writeAndValidateWithLocations(table Table, location File, expectedDataDir File) : void extracted from protected writeAndValidate(schema Schema) : void in class com.netflix.iceberg.spark.source.TestDataFrameWrites
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/0342f23a37d25c4e80c0cc0d82acae2c673e062d/spark/src/test/java/com/netflix/iceberg/spark/source/TestDataFrameWrites.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/0342f23a37d25c4e80c0cc0d82acae2c673e062d/spark/src/test/java/com/netflix/iceberg/spark/source/TestDataFrameWrites.java#L124
DIRECTLY EXTRACTED OPERATION:
    Schema tableSchema = table.schema(); // use the table schema because ids are reassigned

    table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();

    List<Record> expected = RandomData.generateList(tableSchema, 100, 0L);
    Dataset<Row> df = createDataset(expected, tableSchema);
    DataFrameWriter<?> writer = df.write().format("iceberg").mode("append");

    writer.save(location.toString());

    table.refresh();

    Dataset<Row> result = spark.read()
        .format("iceberg")
        .load(location.toString());

    List<Row> actual = result.collectAsList();

    Assert.assertEquals("Result size should match expected", expected.size(), actual.size());
    for (int i = 0; i < expected.size(); i += 1) {
      assertEqualsSafe(tableSchema.asStruct(), expected.get(i), actual.get(i));
    }

    table.currentSnapshot().addedFiles().forEach(dataFile ->
        Assert.assertTrue(
            String.format(
                "File should have the parent directory %s, but has: %s.",
                expectedDataDir.getAbsolutePath(),
                dataFile.path()),
            URI.create(dataFile.path().toString()).getPath().startsWith(expectedDataDir.getAbsolutePath())));
  }

PARAMS COUNT: 3
IS VOID METHOD: true
FRAGMENT LENGTH: 1228
FRAGMENT LINE AVG SIZE: 38.375
DEPTHS:
1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 1 1 
AREA: 62
AVG DEPTH: 1.9375
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: fd8a162e4d39aaeaca8da61a96ed62d4f391dfb9
URL: https://github.com/apache/incubator-iceberg/commit/fd8a162e4d39aaeaca8da61a96ed62d4f391dfb9
DESCRIPTION: Extract Method	package toJsonFields(spec PartitionSpec, generator JsonGenerator) : void extracted from public toJson(spec PartitionSpec, generator JsonGenerator) : void in class com.netflix.iceberg.PartitionSpecParser
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/fd8a162e4d39aaeaca8da61a96ed62d4f391dfb9/core/src/main/java/com/netflix/iceberg/PartitionSpecParser.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/fd8a162e4d39aaeaca8da61a96ed62d4f391dfb9/core/src/main/java/com/netflix/iceberg/PartitionSpecParser.java#L102
DIRECTLY EXTRACTED OPERATION:
    generator.writeStartArray();
    for (PartitionField field : spec.fields()) {
      generator.writeStartObject();
      generator.writeStringField(NAME, field.name());
      generator.writeStringField(TRANSFORM, field.transform().toString());
      generator.writeNumberField(SOURCE_ID, field.sourceId());
      generator.writeEndObject();
    }
    generator.writeEndArray();
  }

PARAMS COUNT: 2
IS VOID METHOD: true
FRAGMENT LENGTH: 386
FRAGMENT LINE AVG SIZE: 35.09090909090909
DEPTHS:
1 2 3 3 3 3 3 2 2 1 1 
AREA: 24
AVG DEPTH: 2.1818181818181817
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private buildFromJsonFields(builder PartitionSpec.Builder, json JsonNode) : void extracted from public fromJson(schema Schema, json JsonNode) : PartitionSpec in class com.netflix.iceberg.PartitionSpecParser
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/fd8a162e4d39aaeaca8da61a96ed62d4f391dfb9/core/src/main/java/com/netflix/iceberg/PartitionSpecParser.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/fd8a162e4d39aaeaca8da61a96ed62d4f391dfb9/core/src/main/java/com/netflix/iceberg/PartitionSpecParser.java#L141
DIRECTLY EXTRACTED OPERATION:
    Preconditions.checkArgument(json.isArray(),
        "Cannot parse partition spec fields, not an array: %s", json);

    Iterator<JsonNode> elements = json.elements();
    while (elements.hasNext()) {
      JsonNode element = elements.next();
      Preconditions.checkArgument(element.isObject(),
          "Cannot parse partition field, not an object: %s", element);

      String name = JsonUtil.getString(NAME, element);
      String transform = JsonUtil.getString(TRANSFORM, element);
      int sourceId = JsonUtil.getInt(SOURCE_ID, element);

      builder.add(sourceId, name, transform);
    }
  }
}
PARAMS COUNT: 2
IS VOID METHOD: true
FRAGMENT LENGTH: 609
FRAGMENT LINE AVG SIZE: 35.8235294117647
DEPTHS:
1 2 2 2 2 3 3 3 3 3 3 3 3 3 2 1 0 
AREA: 39
AVG DEPTH: 2.2941176470588234
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private freshSpec(specId int, schema Schema, partitionSpec PartitionSpec) : PartitionSpec extracted from public buildReplacement(schema Schema, partitionSpec PartitionSpec, properties Map<String,String>) : TableMetadata in class com.netflix.iceberg.TableMetadata
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/fd8a162e4d39aaeaca8da61a96ed62d4f391dfb9/core/src/main/java/com/netflix/iceberg/TableMetadata.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/fd8a162e4d39aaeaca8da61a96ed62d4f391dfb9/core/src/main/java/com/netflix/iceberg/TableMetadata.java#L423
DIRECTLY EXTRACTED OPERATION:
    PartitionSpec.Builder specBuilder = PartitionSpec.builderFor(schema)
        .withSpecId(specId);

    for (PartitionField field : partitionSpec.fields()) {
      // look up the name of the source field in the old schema to get the new schema's id
      String sourceName = partitionSpec.schema().findColumnName(field.sourceId());
      specBuilder.add(
          schema.findField(sourceName).fieldId(),
          field.name(),
          field.transform().toString());
    }

    return specBuilder.build();
  }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 517
FRAGMENT LINE AVG SIZE: 34.46666666666667
DEPTHS:
1 2 2 2 3 3 3 3 3 3 2 2 2 1 1 
AREA: 33
AVG DEPTH: 2.2
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private indexSnapshots(snapshots List<Snapshot>) : Map<Long,Snapshot> extracted from package TableMetadata(ops TableOperations, file InputFile, location String, lastUpdatedMillis long, lastColumnId int, schema Schema, spec PartitionSpec, properties Map<String,String>, currentSnapshotId long, snapshots List<Snapshot>, snapshotLog List<SnapshotLogEntry>) in class com.netflix.iceberg.TableMetadata
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/fd8a162e4d39aaeaca8da61a96ed62d4f391dfb9/core/src/main/java/com/netflix/iceberg/TableMetadata.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/fd8a162e4d39aaeaca8da61a96ed62d4f391dfb9/core/src/main/java/com/netflix/iceberg/TableMetadata.java#L439
DIRECTLY EXTRACTED OPERATION:
    ImmutableMap.Builder<Long, Snapshot> builder = ImmutableMap.builder();
    for (Snapshot version : snapshots) {
      builder.put(version.snapshotId(), version);
    }
    return builder.build();
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 205
FRAGMENT LINE AVG SIZE: 29.285714285714285
DEPTHS:
1 2 3 2 2 1 1 
AREA: 12
AVG DEPTH: 1.7142857142857142
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 880613e0682ad5de6dc3857ab254bb3f7060cc9d
URL: https://github.com/apache/incubator-iceberg/commit/880613e0682ad5de6dc3857ab254bb3f7060cc9d
DESCRIPTION: Extract Method	private addTimestampFilter(filter UnboundPredicate<Long>) : void extracted from public after(timestampMillis long) : Builder in class com.netflix.iceberg.ScanSummary.Builder
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/880613e0682ad5de6dc3857ab254bb3f7060cc9d/core/src/main/java/com/netflix/iceberg/ScanSummary.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/880613e0682ad5de6dc3857ab254bb3f7060cc9d/core/src/main/java/com/netflix/iceberg/ScanSummary.java#L88
DIRECTLY EXTRACTED OPERATION:
      throwIfLimited(); // ensure all partitions can be returned
      timeFilters.add(filter);
    }

PARAMS COUNT: 1
IS VOID METHOD: true
FRAGMENT LENGTH: 103
FRAGMENT LINE AVG SIZE: 25.75
DEPTHS:
2 3 2 2 
AREA: 9
AVG DEPTH: 2.25
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private addTimestampFilter(filter UnboundPredicate<Long>) : void extracted from public before(timestampMillis long) : Builder in class com.netflix.iceberg.ScanSummary.Builder
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/880613e0682ad5de6dc3857ab254bb3f7060cc9d/core/src/main/java/com/netflix/iceberg/ScanSummary.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/880613e0682ad5de6dc3857ab254bb3f7060cc9d/core/src/main/java/com/netflix/iceberg/ScanSummary.java#L88
DIRECTLY EXTRACTED OPERATION:
      throwIfLimited(); // ensure all partitions can be returned
      timeFilters.add(filter);
    }

PARAMS COUNT: 1
IS VOID METHOD: true
FRAGMENT LENGTH: 103
FRAGMENT LINE AVG SIZE: 25.75
DEPTHS:
2 3 2 2 
AREA: 9
AVG DEPTH: 2.25
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 1cafa4b58409b0ca3284fd33345e7914388c2c07
URL: https://github.com/apache/incubator-iceberg/commit/1cafa4b58409b0ca3284fd33345e7914388c2c07
DESCRIPTION: Extract Method	protected getFS(path Path, conf Configuration) : FileSystem extracted from public refresh() : TableMetadata in class com.netflix.iceberg.hadoop.HadoopTableOperations
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/1cafa4b58409b0ca3284fd33345e7914388c2c07/core/src/main/java/com/netflix/iceberg/hadoop/HadoopTableOperations.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/1cafa4b58409b0ca3284fd33345e7914388c2c07/core/src/main/java/com/netflix/iceberg/hadoop/HadoopTableOperations.java#L211
DIRECTLY EXTRACTED OPERATION:
    return Util.getFS(path, conf);
  }
}
PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 41
FRAGMENT LINE AVG SIZE: 13.666666666666666
DEPTHS:
1 1 0 
AREA: 2
AVG DEPTH: 0.6666666666666666
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	protected getFS(path Path, conf Configuration) : FileSystem extracted from public commit(base TableMetadata, metadata TableMetadata) : void in class com.netflix.iceberg.hadoop.HadoopTableOperations
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/1cafa4b58409b0ca3284fd33345e7914388c2c07/core/src/main/java/com/netflix/iceberg/hadoop/HadoopTableOperations.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/1cafa4b58409b0ca3284fd33345e7914388c2c07/core/src/main/java/com/netflix/iceberg/hadoop/HadoopTableOperations.java#L211
DIRECTLY EXTRACTED OPERATION:
    return Util.getFS(path, conf);
  }
}
PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 41
FRAGMENT LINE AVG SIZE: 13.666666666666666
DEPTHS:
1 1 0 
AREA: 2
AVG DEPTH: 0.6666666666666666
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	protected getFS(path Path, conf Configuration) : FileSystem extracted from public deleteFile(path String) : void in class com.netflix.iceberg.hadoop.HadoopTableOperations
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/1cafa4b58409b0ca3284fd33345e7914388c2c07/core/src/main/java/com/netflix/iceberg/hadoop/HadoopTableOperations.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/1cafa4b58409b0ca3284fd33345e7914388c2c07/core/src/main/java/com/netflix/iceberg/hadoop/HadoopTableOperations.java#L211
DIRECTLY EXTRACTED OPERATION:
    return Util.getFS(path, conf);
  }
}
PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 41
FRAGMENT LINE AVG SIZE: 13.666666666666666
DEPTHS:
1 1 0 
AREA: 2
AVG DEPTH: 0.6666666666666666
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	protected getFS(path Path, conf Configuration) : FileSystem extracted from private writeVersionHint(version int) : void in class com.netflix.iceberg.hadoop.HadoopTableOperations
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/1cafa4b58409b0ca3284fd33345e7914388c2c07/core/src/main/java/com/netflix/iceberg/hadoop/HadoopTableOperations.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/1cafa4b58409b0ca3284fd33345e7914388c2c07/core/src/main/java/com/netflix/iceberg/hadoop/HadoopTableOperations.java#L211
DIRECTLY EXTRACTED OPERATION:
    return Util.getFS(path, conf);
  }
}
PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 41
FRAGMENT LINE AVG SIZE: 13.666666666666666
DEPTHS:
1 1 0 
AREA: 2
AVG DEPTH: 0.6666666666666666
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 9349a259092a154c7b96c8423df03f6de6805520
URL: https://github.com/apache/incubator-iceberg/commit/9349a259092a154c7b96c8423df03f6de6805520
DESCRIPTION: Extract Method	protected refreshFromMetadataLocation(newLocation String, numRetries int) : void extracted from protected refreshFromMetadataLocation(newLocation String) : void in class com.netflix.iceberg.BaseMetastoreTableOperations
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/9349a259092a154c7b96c8423df03f6de6805520/core/src/main/java/com/netflix/iceberg/BaseMetastoreTableOperations.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/9349a259092a154c7b96c8423df03f6de6805520/core/src/main/java/com/netflix/iceberg/BaseMetastoreTableOperations.java#L110
DIRECTLY EXTRACTED OPERATION:
    // use null-safe equality check because new tables have a null metadata location
    if (!Objects.equal(currentMetadataLocation, newLocation)) {
      LOG.info("Refreshing table metadata from new version: " + newLocation);

      Tasks.foreach(newLocation)
          .retry(numRetries).exponentialBackoff(100, 5000, 600000, 4.0 /* 100, 400, 1600, ... */ )
          .suppressFailureWhenFinished()
          .run(location -> {
            this.currentMetadata = read(this, fromLocation(location, conf));
            this.currentMetadataLocation = location;
            this.baseLocation = currentMetadata.location();
            this.version = parseVersion(location);
          });
    }
    this.shouldRefresh = false;
  }

PARAMS COUNT: 2
IS VOID METHOD: true
FRAGMENT LENGTH: 728
FRAGMENT LINE AVG SIZE: 42.8235294117647
DEPTHS:
1 2 3 3 3 3 3 3 4 4 4 4 3 2 2 1 1 
AREA: 46
AVG DEPTH: 2.7058823529411766
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: affaff73bb581cbf143e86fa10bcd7bb2a686d67
URL: https://github.com/apache/incubator-iceberg/commit/affaff73bb581cbf143e86fa10bcd7bb2a686d67
DESCRIPTION: Extract Method	private nothingToFilter() : boolean extracted from private filterManifest(deleteExpression Expression, metricsEvaluator StrictMetricsEvaluator, reader ManifestReader) : ManifestReader in class com.netflix.iceberg.MergingSnapshotUpdate
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/affaff73bb581cbf143e86fa10bcd7bb2a686d67/core/src/main/java/com/netflix/iceberg/MergingSnapshotUpdate.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/affaff73bb581cbf143e86fa10bcd7bb2a686d67/core/src/main/java/com/netflix/iceberg/MergingSnapshotUpdate.java#L267
DIRECTLY EXTRACTED OPERATION:
    return (deleteExpression == null || deleteExpression == Expressions.alwaysFalse()) &&
        deletePaths.isEmpty() && dropPartitions.isEmpty();
  }

IS VOID METHOD: false
FRAGMENT LENGTH: 154
FRAGMENT LINE AVG SIZE: 38.5
DEPTHS:
1 2 1 1 
AREA: 5
AVG DEPTH: 1.25
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 84f30def3bde3f51c62ff75e4c48a35ee296863d
URL: https://github.com/apache/incubator-iceberg/commit/84f30def3bde3f51c62ff75e4c48a35ee296863d
DESCRIPTION: Extract Method	private lazyStat() : FileStatus extracted from public getStat() : FileStatus in class com.netflix.iceberg.hadoop.HadoopInputFile
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/84f30def3bde3f51c62ff75e4c48a35ee296863d/core/src/main/java/com/netflix/iceberg/hadoop/HadoopInputFile.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/84f30def3bde3f51c62ff75e4c48a35ee296863d/core/src/main/java/com/netflix/iceberg/hadoop/HadoopInputFile.java#L100
DIRECTLY EXTRACTED OPERATION:
    if (stat == null) {
      try {
        this.stat = fs.getFileStatus(path);
      } catch (IOException e) {
        throw new RuntimeIOException(e, "Failed to get status for file: %s", path);
      }
    }
    return stat;
  }

IS VOID METHOD: false
FRAGMENT LENGTH: 232
FRAGMENT LINE AVG SIZE: 23.2
DEPTHS:
2 3 4 4 4 3 2 2 1 1 
AREA: 26
AVG DEPTH: 2.6
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: e08caf072c0dfbf45c8c4ddf1a8ec292bfe47163
URL: https://github.com/apache/incubator-iceberg/commit/e08caf072c0dfbf45c8c4ddf1a8ec292bfe47163
DESCRIPTION: Extract Method	private cleanUncommittedMerges(committed Set<String>) : void extracted from protected cleanUncommitted(committed Set<String>) : void in class com.netflix.iceberg.MergingSnapshotUpdate
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e08caf072c0dfbf45c8c4ddf1a8ec292bfe47163/core/src/main/java/com/netflix/iceberg/MergingSnapshotUpdate.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e08caf072c0dfbf45c8c4ddf1a8ec292bfe47163/core/src/main/java/com/netflix/iceberg/MergingSnapshotUpdate.java#L201
DIRECTLY EXTRACTED OPERATION:
    List<Map.Entry<List<String>, String>> entries = Lists.newArrayList(mergeManifests.entrySet());
    for (Map.Entry<List<String>, String> entry : entries) {
      // delete any new merged manifests that aren't in the committed list
      String merged = entry.getValue();
      if (!committed.contains(merged)) {
        deleteFile(merged);
        // remove the deleted file from the cache
        mergeManifests.remove(entry.getKey());
      }
    }
  }

PARAMS COUNT: 1
IS VOID METHOD: true
FRAGMENT LENGTH: 459
FRAGMENT LINE AVG SIZE: 38.25
DEPTHS:
1 2 3 3 3 4 4 4 3 2 1 1 
AREA: 31
AVG DEPTH: 2.5833333333333335
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private cleanUncommittedMerges(committed Set<String>) : void extracted from protected cleanUncommitted(committed Set<String>) : void in class com.netflix.iceberg.MergingSnapshotUpdate
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e08caf072c0dfbf45c8c4ddf1a8ec292bfe47163/core/src/main/java/com/netflix/iceberg/MergingSnapshotUpdate.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e08caf072c0dfbf45c8c4ddf1a8ec292bfe47163/core/src/main/java/com/netflix/iceberg/MergingSnapshotUpdate.java#L201
DIRECTLY EXTRACTED OPERATION:
    List<Map.Entry<List<String>, String>> entries = Lists.newArrayList(mergeManifests.entrySet());
    for (Map.Entry<List<String>, String> entry : entries) {
      // delete any new merged manifests that aren't in the committed list
      String merged = entry.getValue();
      if (!committed.contains(merged)) {
        deleteFile(merged);
        // remove the deleted file from the cache
        mergeManifests.remove(entry.getKey());
      }
    }
  }

PARAMS COUNT: 1
IS VOID METHOD: true
FRAGMENT LENGTH: 459
FRAGMENT LINE AVG SIZE: 38.25
DEPTHS:
1 2 3 3 3 4 4 4 3 2 1 1 
AREA: 31
AVG DEPTH: 2.5833333333333335
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private cacheKey(group List<ManifestReader>) : List<String> extracted from public apply(base TableMetadata) : List<String> in class com.netflix.iceberg.MergingSnapshotUpdate
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e08caf072c0dfbf45c8c4ddf1a8ec292bfe47163/core/src/main/java/com/netflix/iceberg/MergingSnapshotUpdate.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e08caf072c0dfbf45c8c4ddf1a8ec292bfe47163/core/src/main/java/com/netflix/iceberg/MergingSnapshotUpdate.java#L432
DIRECTLY EXTRACTED OPERATION:
    List<String> key = Lists.newArrayList();

    for (ManifestReader reader : group) {
      if (reader.file() != null) {
        key.add(reader.file().location());
      } else {
        // if the file is null, this is an in-memory reader
        // use the size to avoid collisions if retries have added files
        key.add("append-" + newFiles.size() + "-files");
      }
    }

    return key;
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 406
FRAGMENT LINE AVG SIZE: 27.066666666666666
DEPTHS:
1 2 2 3 4 4 4 4 4 3 2 2 2 1 1 
AREA: 39
AVG DEPTH: 2.6
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private mergeGroup(groupSpec PartitionSpec, group List<ManifestReader>) : Iterable<String> extracted from public apply(base TableMetadata) : List<String> in class com.netflix.iceberg.MergingSnapshotUpdate
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e08caf072c0dfbf45c8c4ddf1a8ec292bfe47163/core/src/main/java/com/netflix/iceberg/MergingSnapshotUpdate.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e08caf072c0dfbf45c8c4ddf1a8ec292bfe47163/core/src/main/java/com/netflix/iceberg/MergingSnapshotUpdate.java#L323
DIRECTLY EXTRACTED OPERATION:
  private Iterable<String> mergeGroup(PartitionSpec groupSpec, List<ManifestReader> group) {
    // use a lookback of 1 to avoid reordering the manifests. using 1 also means this should pack
    // from the end so that the manifest that gets under-filled is the first one, which will be
    // merged the next time.
    long newFilesSize = newFiles.size() * SIZE_PER_FILE;
    ListPacker<ManifestReader> packer = new ListPacker<>(manifestTargetSizeBytes, 1);
    List<List<ManifestReader>> bins = packer.packEnd(group,
        reader -> reader.file() != null ? reader.file().getLength() : newFilesSize);

    // process bins in parallel, but put results in the order of the bins into an array to preserve
    // the order of manifests and contents. preserving the order helps avoid random deletes when
    // data files are eventually aged off.
    List<String>[] binResults = (List<String>[]) Array.newInstance(List.class, bins.size());
    Tasks.range(bins.size())
        .stopOnFailure().throwFailureWhenFinished()
        .executeWith(getWorkerPool())
        .run(index -> {
          List<ManifestReader> bin = bins.get(index);
          List<String> outputManifests = Lists.newArrayList();
          binResults[index] = outputManifests;

          if (bin.size() == 1 && bin.get(0).file() != null) {
            // no need to rewrite
            outputManifests.add(bin.get(0).file().location());
            return;
          }

          boolean hasInMemoryManifest = false;
          for (ManifestReader reader : bin) {
            if (reader.file() == null) {
              hasInMemoryManifest = true;
            }
          }

          // if the bin has an in-memory manifest (the new data) then only merge it if the number of
          // manifests is above the minimum count. this is applied only to bins with an in-memory
          // manifest so that large manifests don't prevent merging older groups.
          if (hasInMemoryManifest && bin.size() < minManifestsCountToMerge) {
            for (ManifestReader reader : bin) {
              if (reader.file() != null) {
                outputManifests.add(reader.file().location());
              } else {
                // write the in-memory manifest
                outputManifests.add(createManifest(groupSpec, Collections.singletonList(reader)));
              }
            }
          } else {
            outputManifests.add(createManifest(groupSpec, bin));
          }
        });

    return Iterables.concat(binResults);
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 2509
FRAGMENT LINE AVG SIZE: 46.46296296296296
DEPTHS:
1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 4 4 4 3 3 3 3 4 5 4 3 3 3 3 3 3 4 5 6 6 6 6 5 4 4 4 3 2 2 2 1 1 
AREA: 164
AVG DEPTH: 3.037037037037037
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private cleanUncommittedFilters(committed Set<String>) : void extracted from protected cleanUncommitted(committed Set<String>) : void in class com.netflix.iceberg.MergingSnapshotUpdate
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e08caf072c0dfbf45c8c4ddf1a8ec292bfe47163/core/src/main/java/com/netflix/iceberg/MergingSnapshotUpdate.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e08caf072c0dfbf45c8c4ddf1a8ec292bfe47163/core/src/main/java/com/netflix/iceberg/MergingSnapshotUpdate.java#L214
DIRECTLY EXTRACTED OPERATION:
    List<Map.Entry<String, String>> filterEntries = Lists.newArrayList(filteredManifests.entrySet());
    for (Map.Entry<String, String> entry : filterEntries) {
      // remove any new filtered manifests that aren't in the committed list
      String manifest = entry.getKey();
      String filtered = entry.getValue();
      if (filtered != null && !manifest.equals(filtered) && !committed.contains(filtered)) {
        deleteFile(filtered);
        filteredManifests.remove(entry.getKey());
      }
    }
  }

PARAMS COUNT: 1
IS VOID METHOD: true
FRAGMENT LENGTH: 513
FRAGMENT LINE AVG SIZE: 42.75
DEPTHS:
1 2 3 3 3 3 4 4 3 2 1 1 
AREA: 30
AVG DEPTH: 2.5
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: b0001d63c0c848558c366a50b56d3a7a98c6adcc
URL: https://github.com/apache/incubator-iceberg/commit/b0001d63c0c848558c366a50b56d3a7a98c6adcc
DESCRIPTION: Extract Method	public create(schema Schema, spec PartitionSpec, properties Map<String,String>, database String, table String) : Table extracted from public create(schema Schema, spec PartitionSpec, database String, table String) : Table in class com.netflix.iceberg.BaseMetastoreTables
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/b0001d63c0c848558c366a50b56d3a7a98c6adcc/core/src/main/java/com/netflix/iceberg/BaseMetastoreTables.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/b0001d63c0c848558c366a50b56d3a7a98c6adcc/core/src/main/java/com/netflix/iceberg/BaseMetastoreTables.java#L55
DIRECTLY EXTRACTED OPERATION:
                      String database, String table) {
    TableOperations ops = newTableOps(conf, database, table);
    if (ops.current() != null) {
      throw new AlreadyExistsException("Table already exists: " + database + "." + table);
    }

    String location = defaultWarehouseLocation(conf, database, table);
    TableMetadata metadata = newTableMetadata(ops, schema, spec, location, properties);
    ops.commit(null, metadata);

    return new BaseTable(ops, database + "." + table);
  }

PARAMS COUNT: 5
IS VOID METHOD: false
FRAGMENT LENGTH: 500
FRAGMENT LINE AVG SIZE: 38.46153846153846
DEPTHS:
1 2 2 3 2 2 2 2 2 2 2 1 1 
AREA: 24
AVG DEPTH: 1.8461538461538463
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public beginCreate(schema Schema, spec PartitionSpec, properties Map<String,String>, database String, table String) : Transaction extracted from public beginCreate(schema Schema, spec PartitionSpec, database String, table String) : Transaction in class com.netflix.iceberg.BaseMetastoreTables
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/b0001d63c0c848558c366a50b56d3a7a98c6adcc/core/src/main/java/com/netflix/iceberg/BaseMetastoreTables.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/b0001d63c0c848558c366a50b56d3a7a98c6adcc/core/src/main/java/com/netflix/iceberg/BaseMetastoreTables.java#L73
DIRECTLY EXTRACTED OPERATION:
                                 String database, String table) {
    TableOperations ops = newTableOps(conf, database, table);
    if (ops.current() != null) {
      throw new AlreadyExistsException("Table already exists: " + database + "." + table);
    }

    String location = defaultWarehouseLocation(conf, database, table);
    TableMetadata metadata = newTableMetadata(ops, schema, spec, location, properties);

    return BaseTransaction.createTableTransaction(ops, metadata);
  }

PARAMS COUNT: 5
IS VOID METHOD: false
FRAGMENT LENGTH: 490
FRAGMENT LINE AVG SIZE: 40.833333333333336
DEPTHS:
1 2 2 3 2 2 2 2 2 2 1 1 
AREA: 22
AVG DEPTH: 1.8333333333333333
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	public newTableMetadata(ops TableOperations, schema Schema, spec PartitionSpec, location String, properties Map<String,String>) : TableMetadata extracted from public newTableMetadata(ops TableOperations, schema Schema, spec PartitionSpec, location String) : TableMetadata in class com.netflix.iceberg.TableMetadata
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/b0001d63c0c848558c366a50b56d3a7a98c6adcc/core/src/main/java/com/netflix/iceberg/TableMetadata.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/b0001d63c0c848558c366a50b56d3a7a98c6adcc/core/src/main/java/com/netflix/iceberg/TableMetadata.java#L48
DIRECTLY EXTRACTED OPERATION:
                                               Schema schema,
                                               PartitionSpec spec,
                                               String location,
                                               Map<String, String> properties) {
    // reassign all column ids to ensure consistency
    AtomicInteger lastColumnId = new AtomicInteger(0);
    Schema freshSchema = TypeUtil.assignFreshIds(schema, lastColumnId::incrementAndGet);

    // rebuild the partition spec using the new column ids
    PartitionSpec.Builder specBuilder = PartitionSpec.builderFor(freshSchema);
    for (PartitionField field : spec.fields()) {
      // look up the name of the source field in the old schema to get the new schema's id
      String sourceName = schema.findColumnName(field.sourceId());
      specBuilder.add(
          freshSchema.findField(sourceName).fieldId(),
          field.name(),
          field.transform().toString());
    }
    PartitionSpec freshSpec = specBuilder.build();

    return new TableMetadata(ops, null, location,
        System.currentTimeMillis(),
        lastColumnId.get(), freshSchema, freshSpec,
        ImmutableMap.copyOf(properties), -1, ImmutableList.of(), ImmutableList.of());
  }

PARAMS COUNT: 5
IS VOID METHOD: false
FRAGMENT LENGTH: 1247
FRAGMENT LINE AVG SIZE: 47.96153846153846
DEPTHS:
0 1 1 1 2 2 2 2 2 2 2 3 3 3 3 3 3 2 2 2 2 2 2 2 1 1 
AREA: 51
AVG DEPTH: 1.9615384615384615
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: de5407e0c7dab4f34b1f1b9b43c040ace8819987
URL: https://github.com/apache/incubator-iceberg/commit/de5407e0c7dab4f34b1f1b9b43c040ace8819987
DESCRIPTION: Extract Method	private convertFields(fields List<Types.NestedField>) : ResourceFieldSchema[] extracted from public convert(icebergSchema Schema) : ResourceSchema in class com.netflix.iceberg.pig.SchemaUtil
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/de5407e0c7dab4f34b1f1b9b43c040ace8819987/pig/src/main/java/com/netflix/iceberg/pig/SchemaUtil.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/de5407e0c7dab4f34b1f1b9b43c040ace8819987/pig/src/main/java/com/netflix/iceberg/pig/SchemaUtil.java#L45
DIRECTLY EXTRACTED OPERATION:
    List<ResourceFieldSchema> result = Lists.newArrayList();

    for (Types.NestedField nf : fields) {
      result.add(convert(nf));
    }

    return result.toArray(new ResourceFieldSchema[0]);
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 202
FRAGMENT LINE AVG SIZE: 22.444444444444443
DEPTHS:
1 2 2 3 2 2 2 1 1 
AREA: 16
AVG DEPTH: 1.7777777777777777
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 035f24638fa952fb107719228b15faf815a34ea8
URL: https://github.com/apache/incubator-iceberg/commit/035f24638fa952fb107719228b15faf815a34ea8
DESCRIPTION: Extract Method	package listMetadataFiles(tableDir File, ext String) : List<File> extracted from package listMetadataFiles(ext String) : List<File> in class com.netflix.iceberg.TableTestBase
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/035f24638fa952fb107719228b15faf815a34ea8/core/src/test/java/com/netflix/iceberg/TableTestBase.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/035f24638fa952fb107719228b15faf815a34ea8/core/src/test/java/com/netflix/iceberg/TableTestBase.java#L98
DIRECTLY EXTRACTED OPERATION:
    return Lists.newArrayList(new File(tableDir, "metadata").listFiles(
        (dir, name) -> Files.getFileExtension(name).equalsIgnoreCase(ext)));
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 154
FRAGMENT LINE AVG SIZE: 38.5
DEPTHS:
1 2 1 1 
AREA: 5
AVG DEPTH: 1.25
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: e3d16a74ad3da72f014ba756752e1190942120ed
URL: https://github.com/apache/incubator-iceberg/commit/e3d16a74ad3da72f014ba756752e1190942120ed
DESCRIPTION: Extract Method	private open(task FileScanTask, readSchema Schema, conf Configuration) : Iterator<InternalRow> extracted from private open(task FileScanTask) : Iterator<UnsafeRow> in class com.netflix.iceberg.spark.source.Reader.TaskDataReader
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/e3d16a74ad3da72f014ba756752e1190942120ed/spark/src/main/java/com/netflix/iceberg/spark/source/Reader.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/e3d16a74ad3da72f014ba756752e1190942120ed/spark/src/main/java/com/netflix/iceberg/spark/source/Reader.java#L418
DIRECTLY EXTRACTED OPERATION:
                                       Configuration conf) {
      InputFile location = HadoopInputFile.fromLocation(task.file().path(), conf);
      CloseableIterable<InternalRow> iter;
      switch (task.file().format()) {
        case ORC:
          SparkOrcReader reader = new SparkOrcReader(location, task, readSchema);
          this.currentCloseable = reader;
          return reader;

        case PARQUET:
          iter = newParquetIterable(location, task, readSchema);
          break;

        case AVRO:
          iter = newAvroIterable(location, task, readSchema);
          break;

        default:
          throw new UnsupportedOperationException(
              "Cannot read unknown format: " + task.file().format());
      }

      this.currentCloseable = iter;

      return iter.iterator();
    }

PARAMS COUNT: 3
IS VOID METHOD: false
FRAGMENT LENGTH: 818
FRAGMENT LINE AVG SIZE: 30.296296296296298
DEPTHS:
2 3 3 3 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 3 3 3 3 3 2 2 
AREA: 94
AVG DEPTH: 3.4814814814814814
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 15ed870d2797ff3e83acc38de5ab56e45c65894c
URL: https://github.com/apache/incubator-iceberg/commit/15ed870d2797ff3e83acc38de5ab56e45c65894c
DESCRIPTION: Extract Method	private createManifest(binSpec PartitionSpec, bin List<ManifestReader>) : String extracted from private mergeGroup(spec PartitionSpec, group List<ManifestReader>) : List<String> in class com.netflix.iceberg.MergeAppend
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/15ed870d2797ff3e83acc38de5ab56e45c65894c/core/src/main/java/com/netflix/iceberg/MergeAppend.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/15ed870d2797ff3e83acc38de5ab56e45c65894c/core/src/main/java/com/netflix/iceberg/MergeAppend.java#L182
DIRECTLY EXTRACTED OPERATION:
    List<String> key = cacheKey(bin);
    // if this merge was already rewritten, use the existing file.
    // if the new files are in this merge, the key is based on the number of new files so files
    // added after the last merge will cause a cache miss.
    if (newManifests.containsKey(key)) {
      return newManifests.get(key);
    }

    OutputFile out = manifestPath(newManifests.size());

    try (ManifestWriter writer = new ManifestWriter(binSpec, out, snapshotId())) {

      for (ManifestReader reader : bin) {
        if (reader.file() != null) {
          writer.addExisting(reader.entries());
        } else {
          writer.addEntries(reader.entries());
        }
      }

    } catch (IOException e) {
      throw new RuntimeIOException(e, "Failed to write manifest: %s", out);
    }

    // update the cache
    newManifests.put(key, out.location());

    return out.location();
  }

PARAMS COUNT: 2
IS VOID METHOD: false
FRAGMENT LENGTH: 908
FRAGMENT LINE AVG SIZE: 30.266666666666666
DEPTHS:
1 2 2 2 2 3 2 2 2 2 2 3 3 4 5 5 5 4 3 3 3 3 2 2 2 2 2 2 1 1 
AREA: 77
AVG DEPTH: 2.566666666666667
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 64c11ad5f550c3e3c173a38cf51927cb3afb23ac
URL: https://github.com/apache/incubator-iceberg/commit/64c11ad5f550c3e3c173a38cf51927cb3afb23ac
DESCRIPTION: Extract Method	private lazyFieldList() : List<NestedField> extracted from public fields() : List<NestedField> in class com.netflix.iceberg.types.Types.ListType
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/64c11ad5f550c3e3c173a38cf51927cb3afb23ac/api/src/main/java/com/netflix/iceberg/types/Types.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/64c11ad5f550c3e3c173a38cf51927cb3afb23ac/api/src/main/java/com/netflix/iceberg/types/Types.java#L726
DIRECTLY EXTRACTED OPERATION:
      if (fields == null) {
        this.fields = ImmutableList.of(elementField);
      }
      return fields;
    }
  }
IS VOID METHOD: false
FRAGMENT LENGTH: 121
FRAGMENT LINE AVG SIZE: 20.166666666666668
DEPTHS:
3 4 3 3 2 1 
AREA: 16
AVG DEPTH: 2.6666666666666665
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	private lazyFieldList() : List<NestedField> extracted from public fields() : List<NestedField> in class com.netflix.iceberg.types.Types.MapType
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/64c11ad5f550c3e3c173a38cf51927cb3afb23ac/api/src/main/java/com/netflix/iceberg/types/Types.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/64c11ad5f550c3e3c173a38cf51927cb3afb23ac/api/src/main/java/com/netflix/iceberg/types/Types.java#L848
DIRECTLY EXTRACTED OPERATION:
      if (fields == null) {
        this.fields = ImmutableList.of(keyField, valueField);
      }
      return fields;
    }
  }
IS VOID METHOD: false
FRAGMENT LENGTH: 129
FRAGMENT LINE AVG SIZE: 21.5
DEPTHS:
3 4 3 3 2 1 
AREA: 16
AVG DEPTH: 2.6666666666666665
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 731d12d3e70e1e01c640c8c84cbd5fba76c5c0eb
URL: https://github.com/apache/incubator-iceberg/commit/731d12d3e70e1e01c640c8c84cbd5fba76c5c0eb
DESCRIPTION: Extract Method	private lazyType() : StructType extracted from public readSchema() : StructType in class com.netflix.iceberg.spark.source.Reader
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/731d12d3e70e1e01c640c8c84cbd5fba76c5c0eb/spark/src/main/java/com/netflix/iceberg/spark/source/Reader.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/731d12d3e70e1e01c640c8c84cbd5fba76c5c0eb/spark/src/main/java/com/netflix/iceberg/spark/source/Reader.java#L103
DIRECTLY EXTRACTED OPERATION:
    if (type == null) {
      this.type = convert(lazySchema());
    }
    return type;
  }

IS VOID METHOD: false
FRAGMENT LENGTH: 93
FRAGMENT LINE AVG SIZE: 15.5
DEPTHS:
2 3 2 2 1 1 
AREA: 11
AVG DEPTH: 1.8333333333333333
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: db8718866746fbf3f306df85d7efd06c4915849f
URL: https://github.com/apache/incubator-iceberg/commit/db8718866746fbf3f306df85d7efd06c4915849f
DESCRIPTION: Extract Method	protected findTable(options DataSourceV2Options) : Table extracted from public createReader(options DataSourceV2Options) : DataSourceV2Reader in class com.netflix.iceberg.spark.source.IcebergSource
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/db8718866746fbf3f306df85d7efd06c4915849f/spark/src/main/java/com/netflix/iceberg/spark/source/IcebergSource.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/db8718866746fbf3f306df85d7efd06c4915849f/spark/src/main/java/com/netflix/iceberg/spark/source/IcebergSource.java#L86
DIRECTLY EXTRACTED OPERATION:
    Optional<String> location = options.get("iceberg.table.location");
    Preconditions.checkArgument(location.isPresent(),
        "Cannot open table without a location: iceberg.table.location is not set");

    HadoopTables tables = new HadoopTables(lazyConf());

    return tables.load(location.get());
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 312
FRAGMENT LINE AVG SIZE: 34.666666666666664
DEPTHS:
1 2 2 2 2 2 2 1 1 
AREA: 15
AVG DEPTH: 1.6666666666666667
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	protected findTable(options DataSourceV2Options) : Table extracted from public createWriter(jobId String, dfStruct StructType, mode SaveMode, options DataSourceV2Options) : Optional<DataSourceV2Writer> in class com.netflix.iceberg.spark.source.IcebergSource
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/db8718866746fbf3f306df85d7efd06c4915849f/spark/src/main/java/com/netflix/iceberg/spark/source/IcebergSource.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/db8718866746fbf3f306df85d7efd06c4915849f/spark/src/main/java/com/netflix/iceberg/spark/source/IcebergSource.java#L86
DIRECTLY EXTRACTED OPERATION:
    Optional<String> location = options.get("iceberg.table.location");
    Preconditions.checkArgument(location.isPresent(),
        "Cannot open table without a location: iceberg.table.location is not set");

    HadoopTables tables = new HadoopTables(lazyConf());

    return tables.load(location.get());
  }

PARAMS COUNT: 1
IS VOID METHOD: false
FRAGMENT LENGTH: 312
FRAGMENT LINE AVG SIZE: 34.666666666666664
DEPTHS:
1 2 2 2 2 2 2 1 1 
AREA: 15
AVG DEPTH: 1.6666666666666667
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	protected lazyConf() : Configuration extracted from public createReader(options DataSourceV2Options) : DataSourceV2Reader in class com.netflix.iceberg.spark.source.IcebergSource
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/db8718866746fbf3f306df85d7efd06c4915849f/spark/src/main/java/com/netflix/iceberg/spark/source/IcebergSource.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/db8718866746fbf3f306df85d7efd06c4915849f/spark/src/main/java/com/netflix/iceberg/spark/source/IcebergSource.java#L96
DIRECTLY EXTRACTED OPERATION:
    if (lazyConf == null) {
      SparkSession session = SparkSession.builder().getOrCreate();
      this.lazyConf = session.sparkContext().hadoopConfiguration();
    }
    return lazyConf;
  }
}
IS VOID METHOD: false
FRAGMENT LENGTH: 196
FRAGMENT LINE AVG SIZE: 28.0
DEPTHS:
2 3 3 2 2 1 0 
AREA: 13
AVG DEPTH: 1.8571428571428572
---REFACTORING_FINISH---
DESCRIPTION: Extract Method	protected lazyConf() : Configuration extracted from public createWriter(jobId String, dfStruct StructType, mode SaveMode, options DataSourceV2Options) : Optional<DataSourceV2Writer> in class com.netflix.iceberg.spark.source.IcebergSource
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/db8718866746fbf3f306df85d7efd06c4915849f/spark/src/main/java/com/netflix/iceberg/spark/source/IcebergSource.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/db8718866746fbf3f306df85d7efd06c4915849f/spark/src/main/java/com/netflix/iceberg/spark/source/IcebergSource.java#L96
DIRECTLY EXTRACTED OPERATION:
    if (lazyConf == null) {
      SparkSession session = SparkSession.builder().getOrCreate();
      this.lazyConf = session.sparkContext().hadoopConfiguration();
    }
    return lazyConf;
  }
}
IS VOID METHOD: false
FRAGMENT LENGTH: 196
FRAGMENT LINE AVG SIZE: 28.0
DEPTHS:
2 3 3 2 2 1 0 
AREA: 13
AVG DEPTH: 1.8571428571428572
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
COMMIT ID: 4000069ce50c525159c362b13355eb9887b30f2a
URL: https://github.com/apache/incubator-iceberg/commit/4000069ce50c525159c362b13355eb9887b30f2a
DESCRIPTION: Extract Method	private lazySchema() : Schema extracted from private projection() : Schema in class com.netflix.iceberg.spark.source.IcebergSource.ScanTask
REFACTORING FILE DIFF URL: https://github.com/apache/incubator-iceberg/commit/4000069ce50c525159c362b13355eb9887b30f2a/spark/src/main/java/com/netflix/iceberg/spark/source/IcebergSource.java
REFACTORING URL: https://github.com/apache/incubator-iceberg/blob/4000069ce50c525159c362b13355eb9887b30f2a/spark/src/main/java/com/netflix/iceberg/spark/source/IcebergSource.java#L279
DIRECTLY EXTRACTED OPERATION:
      if (schema == null) {
        this.schema = SchemaParser.fromJson(schemaString);
      }
      return schema;
    }
  }
IS VOID METHOD: false
FRAGMENT LENGTH: 126
FRAGMENT LINE AVG SIZE: 21.0
DEPTHS:
3 4 3 3 2 1 
AREA: 16
AVG DEPTH: 2.6666666666666665
---REFACTORING_FINISH---
-----REFACTORINGS_END-----
0
